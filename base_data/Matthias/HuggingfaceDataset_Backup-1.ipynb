{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758a94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances:\n",
      "140\n",
      "\n",
      "all keys:\n",
      "['text', 'meta', '_input_hash', '_task_hash', 'spans', 'tokens', '_view_id', 'answer', '_timestamp']\n",
      "\n",
      "important keys:\n",
      "['text', 'spans', 'tokens']\n",
      "\n",
      "example text:\n",
      "DORNBIRN In der Schulgasse in Dornbirn hat eine 71,93 Quadratmeter große Wohnung für einen Quadratmeterpreis von 5533,71 Euro den Besitzer gewechselt. Dieser beinhaltet auch einen Pkw-Abstellplatz. Käufer der Wohnung mit 9,86 Quadratmetern Terrasse ist die ValLiLean Beteiligungs- und Immobilienverwaltungs GmbH. Beim Verkäufer handelt es sich um die Karrenblick Projekt GmbH.  Der Kaufpreis liegt bei 398.040 Euro. Unterzeichnet wurde der Kaufvertrag am 18. September. Die Verbücherung datiert mit Oktober 2020.\n",
      "\n",
      "5 example spans:\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'pattern': 2069086582, 'token_start': 0, 'token_end': 0, 'label': 'ORT', 'noWords': 1}\n",
      "{'start': 16, 'end': 26, 'token_start': 3, 'token_end': 3, 'label': 'STRASSE', 'noWords': 1}\n",
      "{'text': 'Dornbirn', 'start': 30, 'end': 38, 'pattern': 2069086582, 'token_start': 5, 'token_end': 5, 'label': 'ORT', 'noWords': 1}\n",
      "{'start': 48, 'end': 53, 'token_start': 8, 'token_end': 8, 'label': 'FLAECHE', 'noWords': 1}\n",
      "{'start': 73, 'end': 80, 'token_start': 11, 'token_end': 11, 'label': 'IMMO_TYP', 'noWords': 1}\n",
      "\n",
      "5 example tokens:\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'id': 0, 'ws': True, 'label': 'ORT'}\n",
      "{'text': 'In', 'start': 9, 'end': 11, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'der', 'start': 12, 'end': 15, 'id': 2, 'ws': True, 'label': 'O'}\n",
      "{'text': 'Schulgasse', 'start': 16, 'end': 26, 'id': 3, 'ws': True, 'label': 'STRASSE'}\n",
      "{'text': 'in', 'start': 27, 'end': 29, 'id': 4, 'ws': True, 'label': 'O'}\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "#import sklearn\n",
    "#import sklearn_crfsuite\n",
    "#from sklearn_crfsuite import scorers\n",
    "#from sklearn.utils import shuffle\n",
    "#from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "#from sklearn_crfsuite import metrics\n",
    "#from sklearn.metrics import make_scorer\n",
    "#import metrics as mtx\n",
    "#import scipy.stats\n",
    "#import random\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(\"./annotations2.jsonl\") as jsonl_file: # . instead of ..\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]\n",
    "print(\"instances:\\n{}\".format(len(annot)))\n",
    "keys = [key for key in annot[0].keys()]\n",
    "print(\"\\nall keys:\\n{}\".format(keys))\n",
    "key_keys = [\"text\", \"spans\", \"tokens\"]\n",
    "print(\"\\nimportant keys:\\n{}\".format(key_keys))\n",
    "print(\"\\nexample text:\\n{}\".format(annot[0][\"text\"]))\n",
    "n_examples = 5\n",
    "print(\"\\n{} example spans:\".format(n_examples))\n",
    "for span in annot[0][\"spans\"][:n_examples]:\n",
    "    print(\"{}\".format(span))\n",
    "print(\"\\n{} example tokens:\".format(n_examples))\n",
    "for token in annot[0][\"tokens\"][:n_examples]:\n",
    "    print(\"{}\".format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc53d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token dictionaries for the last 3 words of instance 0\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'id': 0, 'ws': True, 'label': 'ORT'}\n",
      "{'text': 'In', 'start': 9, 'end': 11, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'der', 'start': 12, 'end': 15, 'id': 2, 'ws': True, 'label': 'O'}\n",
      "Token dictionaries for the last 3 words of instance 1\n",
      "{'text': 'FELDKIRCH', 'start': 0, 'end': 9, 'id': 0, 'ws': True, 'label': 'ORT'}\n",
      "{'text': 'Im', 'start': 10, 'end': 12, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'Altenreuteweg', 'start': 13, 'end': 26, 'id': 2, 'ws': True, 'label': 'STRASSE'}\n"
     ]
    }
   ],
   "source": [
    "def getLabel(tokenDictList, idx):\n",
    "    result = \"O\"\n",
    "    for dict_i in tokenDictList:\n",
    "        idx_0, idx_1 = dict_i[\"start\"], dict_i[\"end\"]\n",
    "        if (idx_0<=idx) and (idx<=idx_1):\n",
    "            result = dict_i[\"label\"]\n",
    "    return result \n",
    "\n",
    "for j in range(len(annot)): # loop over instances\n",
    "    a = annot[j]            # instance j\n",
    "    spans = a[\"spans\"]      # list of annotation dicts\n",
    "    toks = a[\"tokens\"]      # list of token dicts\n",
    "    for i in range(len(toks)):                                 # loop over token dicts\n",
    "        toks[i][\"label\"] = getLabel(spans, toks[i][\"start\"])   # assign label from span (if exists, otherwise \"O\")\n",
    "    annot[j][\"tokens\"] = toks\n",
    "\n",
    "words_n = 3\n",
    "for i in range(2):\n",
    "    print(\"Token dictionaries for the last {} words of instance {}\".format(words_n, i))\n",
    "    ann = annot[i]\n",
    "    for tok in ann[\"tokens\"][:words_n]:\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e587b7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('DORNBIRN', 'ORT'),\n",
       "  ('In', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Schulgasse', 'STRASSE'),\n",
       "  ('in', 'O'),\n",
       "  ('Dornbirn', 'ORT'),\n",
       "  ('hat', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('71,93', 'FLAECHE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('große', 'O'),\n",
       "  ('Wohnung', 'IMMO_TYP'),\n",
       "  ('für', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Quadratmeterpreis', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('5533,71', 'QMPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('den', 'O'),\n",
       "  ('Besitzer', 'O'),\n",
       "  ('gewechselt', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Dieser', 'O'),\n",
       "  ('beinhaltet', 'O'),\n",
       "  ('auch', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Pkw-Abstellplatz', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Käufer', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Wohnung', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('9,86', 'TERRASSENGROESSE'),\n",
       "  ('Quadratmetern', 'O'),\n",
       "  ('Terrasse', 'O'),\n",
       "  ('ist', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('ValLiLean', 'KAEUFER'),\n",
       "  ('Beteiligungs-', 'KAEUFER'),\n",
       "  ('und', 'KAEUFER'),\n",
       "  ('Immobilienverwaltungs', 'KAEUFER'),\n",
       "  ('GmbH', 'KAEUFER'),\n",
       "  ('.', 'KAEUFER'),\n",
       "  ('Beim', 'O'),\n",
       "  ('Verkäufer', 'O'),\n",
       "  ('handelt', 'O'),\n",
       "  ('es', 'O'),\n",
       "  ('sich', 'O'),\n",
       "  ('um', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('Karrenblick', 'VERKAEUFER'),\n",
       "  ('Projekt', 'VERKAEUFER'),\n",
       "  ('GmbH', 'VERKAEUFER'),\n",
       "  ('.', 'VERKAEUFER'),\n",
       "  (' ', 'O'),\n",
       "  ('Der', 'O'),\n",
       "  ('Kaufpreis', 'O'),\n",
       "  ('liegt', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('398.040', 'GESAMTPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Unterzeichnet', 'O'),\n",
       "  ('wurde', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Kaufvertrag', 'O'),\n",
       "  ('am', 'O'),\n",
       "  ('18.', 'DATUM_VERTRAG'),\n",
       "  ('September', 'DATUM_VERTRAG'),\n",
       "  ('.', 'DATUM_VERTRAG'),\n",
       "  ('Die', 'O'),\n",
       "  ('Verbücherung', 'O'),\n",
       "  ('datiert', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('Oktober', 'DATUM_VERBUECHERUNG'),\n",
       "  ('2020', 'DATUM_VERBUECHERUNG'),\n",
       "  ('.', 'DATUM_VERBUECHERUNG')],\n",
       " [('FELDKIRCH', 'ORT'),\n",
       "  ('Im', 'O'),\n",
       "  ('Altenreuteweg', 'STRASSE'),\n",
       "  ('in', 'O'),\n",
       "  ('Feldkirch', 'ORT'),\n",
       "  ('hat', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('100,67', 'FLAECHE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('große', 'O'),\n",
       "  ('Wohnung', 'IMMO_TYP'),\n",
       "  ('für', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Quadratmeterpreis', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('6168,67', 'QMPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('den', 'O'),\n",
       "  ('Besitzer', 'O'),\n",
       "  ('gewechselt', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Käufer', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Wohnung', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('einer', 'O'),\n",
       "  ('137,49', 'TERRASSENGROESSE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('großen', 'O'),\n",
       "  ('Terrasse', 'O'),\n",
       "  ('ist', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('Privatperson', 'KAEUFER'),\n",
       "  ('.', 'KAEUFER'),\n",
       "  ('Beim', 'O'),\n",
       "  ('Verkäufer', 'O'),\n",
       "  ('handelt', 'O'),\n",
       "  ('es', 'O'),\n",
       "  ('sich', 'O'),\n",
       "  ('um', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('Rüscher', 'VERKAEUFER'),\n",
       "  ('und', 'VERKAEUFER'),\n",
       "  ('Söhne', 'VERKAEUFER'),\n",
       "  ('Bau', 'VERKAEUFER'),\n",
       "  ('GmbH', 'VERKAEUFER'),\n",
       "  ('&', 'VERKAEUFER'),\n",
       "  ('Co', 'VERKAEUFER'),\n",
       "  ('KG', 'VERKAEUFER'),\n",
       "  ('.', 'VERKAEUFER'),\n",
       "  (' ', 'O'),\n",
       "  ('Der', 'O'),\n",
       "  ('Kaufpreis', 'O'),\n",
       "  ('liegt', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('621.000', 'GESAMTPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Unterzeichnet', 'O'),\n",
       "  ('wurde', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Kaufvertrag', 'O'),\n",
       "  ('am', 'O'),\n",
       "  ('11.', 'DATUM_VERTRAG'),\n",
       "  ('August', 'DATUM_VERTRAG'),\n",
       "  ('.', 'DATUM_VERTRAG'),\n",
       "  ('Die', 'O'),\n",
       "  ('Verbücherung', 'O'),\n",
       "  ('datiert', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('September', 'DATUM_VERBUECHERUNG'),\n",
       "  ('2020', 'DATUM_VERBUECHERUNG'),\n",
       "  ('.', 'DATUM_VERBUECHERUNG')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents=[] \n",
    "for annot_i in annot:                  # loop over instances\n",
    "    toks = annot_i['tokens']           # get tokens list for instance i\n",
    "    train_sentence = []\n",
    "    for tok in toks:                   # loop over token dicts\n",
    "        if 'label' in tok:             # only if the current token has been labelled, ...\n",
    "            token_element = (tok['text'], tok['label']) # ... create a \"text\", \"label\" pair for this token ...\n",
    "            train_sentence.append(token_element)        # ... and append it to the list\n",
    "    sents.append(train_sentence) # append the list for that instances to the list for all instances / sentences\n",
    "\n",
    "# list of lists of pairs (sets): outer list contains instances and inner list contains (token, label) pairs\n",
    "sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2593d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels'],\n",
       "    num_rows: 140\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. build tokens = list of lists of tokens\n",
    "# 2. build labels = list of lists of labels\n",
    "tokens = []\n",
    "labels = []\n",
    "for sent_i in sents:\n",
    "    tokens_i = []\n",
    "    labels_i = []\n",
    "    for word_label in sent_i:\n",
    "        tokens_i.append(word_label[0])\n",
    "        labels_i.append(word_label[1])\n",
    "    tokens.append(tokens_i)\n",
    "    labels.append(labels_i)\n",
    "\n",
    "dataset = Dataset.from_dict({\"tokens\": tokens, \"labels\": labels})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8287f437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATUM_VERBUECHERUNG',\n",
       " 'DATUM_VERTRAG',\n",
       " 'FLAECHE',\n",
       " 'GESAMTPREIS',\n",
       " 'IMMO_TYP',\n",
       " 'KAEUFER',\n",
       " 'O',\n",
       " 'ORT',\n",
       " 'QMPREIS',\n",
       " 'STRASSE',\n",
       " 'TERRASSENGROESSE',\n",
       " 'VERKAEUFER']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set of labels\n",
    "label_names = sorted(list(set([label_ij for labels_i in labels for label_ij in labels_i])))\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b91604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0326e130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature = dataset.features[\"tokens\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b02670f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=12, names=['DATUM_VERBUECHERUNG', 'DATUM_VERTRAG', 'FLAECHE', 'GESAMTPREIS', 'IMMO_TYP', 'KAEUFER', 'O', 'ORT', 'QMPREIS', 'STRASSE', 'TERRASSENGROESSE', 'VERKAEUFER'], id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset.features[\"tokens\"].feature.names = label_names # ClassLabel(names=label_names)\n",
    "from datasets import ClassLabel\n",
    "dataset.features[\"tokens\"].feature.names = ClassLabel(names=label_names)\n",
    "dataset.features[\"tokens\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100ee7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATUM_VERBUECHERUNG',\n",
       " 'DATUM_VERTRAG',\n",
       " 'FLAECHE',\n",
       " 'GESAMTPREIS',\n",
       " 'IMMO_TYP',\n",
       " 'KAEUFER',\n",
       " 'O',\n",
       " 'ORT',\n",
       " 'QMPREIS',\n",
       " 'STRASSE',\n",
       " 'TERRASSENGROESSE',\n",
       " 'VERKAEUFER']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features[\"tokens\"].feature.names.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11feed58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and adjust labels\n",
    "checkpoint = \"flair/ner-german\"  # https://huggingface.co/flair/ner-german\n",
    "checkpoint = \"fhswf/bert_de_ner\" # https://huggingface.co/fhswf/bert_de_ner\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982f1106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels'],\n",
       "    num_rows: 140\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dfa0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "#inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cdffac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 51,\n",
       " 51,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 53,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 71,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(dataset[0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d4629a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'dor',\n",
       " '##nb',\n",
       " '##irn',\n",
       " 'in',\n",
       " 'der',\n",
       " 'schul',\n",
       " '##gasse',\n",
       " 'in',\n",
       " 'dor',\n",
       " '##nb',\n",
       " '##irn',\n",
       " 'hat',\n",
       " 'eine',\n",
       " '71',\n",
       " ',',\n",
       " '93',\n",
       " 'quadrat',\n",
       " '##meter',\n",
       " 'große',\n",
       " 'wohn',\n",
       " '##ung',\n",
       " 'fur',\n",
       " 'einen',\n",
       " 'quadrat',\n",
       " '##meter',\n",
       " '##preis',\n",
       " 'von',\n",
       " '55',\n",
       " '##33',\n",
       " ',',\n",
       " '71',\n",
       " 'eur',\n",
       " '##o',\n",
       " 'den',\n",
       " 'bes',\n",
       " '##itzer',\n",
       " 'gewechselt',\n",
       " '.',\n",
       " 'dieser',\n",
       " 'beinhaltet',\n",
       " 'auch',\n",
       " 'einen',\n",
       " 'p',\n",
       " '##kw',\n",
       " '-',\n",
       " 'abs',\n",
       " '##tell',\n",
       " '##platz',\n",
       " '.',\n",
       " 'kauf',\n",
       " '##er',\n",
       " 'der',\n",
       " 'wohn',\n",
       " '##ung',\n",
       " 'mit',\n",
       " '9',\n",
       " ',',\n",
       " '86',\n",
       " 'quadrat',\n",
       " '##metern',\n",
       " 'ter',\n",
       " '##rasse',\n",
       " 'ist',\n",
       " 'die',\n",
       " 'val',\n",
       " '##li',\n",
       " '##le',\n",
       " '##an',\n",
       " 'beteil',\n",
       " '##igungs',\n",
       " '-',\n",
       " 'und',\n",
       " 'imm',\n",
       " '##obilien',\n",
       " '##verwaltungs',\n",
       " 'g',\n",
       " '##mb',\n",
       " '##h',\n",
       " '.',\n",
       " 'beim',\n",
       " 'verkauf',\n",
       " '##er',\n",
       " 'handelt',\n",
       " 'es',\n",
       " 'sich',\n",
       " 'um',\n",
       " 'die',\n",
       " 'kar',\n",
       " '##ren',\n",
       " '##blick',\n",
       " 'pro',\n",
       " '##jekt',\n",
       " 'g',\n",
       " '##mb',\n",
       " '##h',\n",
       " '.',\n",
       " 'der',\n",
       " 'kauf',\n",
       " '##preis',\n",
       " 'liegt',\n",
       " 'bei',\n",
       " '39',\n",
       " '##8',\n",
       " '.',\n",
       " '04',\n",
       " '##0',\n",
       " 'eur',\n",
       " '##o',\n",
       " '.',\n",
       " 'unterzeichnet',\n",
       " 'wurde',\n",
       " 'der',\n",
       " 'kauf',\n",
       " '##vertrag',\n",
       " 'am',\n",
       " '18',\n",
       " '.',\n",
       " 'se',\n",
       " '##pt',\n",
       " '##ember',\n",
       " '.',\n",
       " 'die',\n",
       " 'verb',\n",
       " '##ucher',\n",
       " '##ung',\n",
       " 'datiert',\n",
       " 'mit',\n",
       " 'ok',\n",
       " '##to',\n",
       " '##ber',\n",
       " '2020',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb34bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 134)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.word_ids()), len(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1516c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            label = -100\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "        new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed05db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 'ORT', 'ORT', 'ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'ORT', 'ORT', 'O', 'O', 'FLAECHE', 'FLAECHE', 'FLAECHE', 'O', 'O', 'O', 'IMMO_TYP', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'QMPREIS', 'QMPREIS', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'TERRASSENGROESSE', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'GESAMTPREIS', 'GESAMTPREIS', 'GESAMTPREIS', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', -100]\n",
      "134\n",
      "['[CLS]', 'dor', '##nb', '##irn', 'in', 'der', 'schul', '##gasse', 'in', 'dor', '##nb', '##irn', 'hat', 'eine', '71', ',', '93', 'quadrat', '##meter', 'große', 'wohn', '##ung', 'fur', 'einen', 'quadrat', '##meter', '##preis', 'von', '55', '##33', ',', '71', 'eur', '##o', 'den', 'bes', '##itzer', 'gewechselt', '.', 'dieser', 'beinhaltet', 'auch', 'einen', 'p', '##kw', '-', 'abs', '##tell', '##platz', '.', 'kauf', '##er', 'der', 'wohn', '##ung', 'mit', '9', ',', '86', 'quadrat', '##metern', 'ter', '##rasse', 'ist', 'die', 'val', '##li', '##le', '##an', 'beteil', '##igungs', '-', 'und', 'imm', '##obilien', '##verwaltungs', 'g', '##mb', '##h', '.', 'beim', 'verkauf', '##er', 'handelt', 'es', 'sich', 'um', 'die', 'kar', '##ren', '##blick', 'pro', '##jekt', 'g', '##mb', '##h', '.', 'der', 'kauf', '##preis', 'liegt', 'bei', '39', '##8', '.', '04', '##0', 'eur', '##o', '.', 'unterzeichnet', 'wurde', 'der', 'kauf', '##vertrag', 'am', '18', '.', 'se', '##pt', '##ember', '.', 'die', 'verb', '##ucher', '##ung', 'datiert', 'mit', 'ok', '##to', '##ber', '2020', '.', '[SEP]']\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[0][\"labels\"]\n",
    "word_ids = inputs.word_ids()\n",
    "aligned_labels = align_labels_with_tokens(labels, word_ids)\n",
    "print(aligned_labels)\n",
    "print(len(aligned_labels))\n",
    "print(inputs.tokens())\n",
    "print(len(inputs.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d51b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.17.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2b3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"labels\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872269bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f98a3eeceea45ccb07ea0fc1535dcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not convert 'ORT' with type str: tried to convert to int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_1d_for_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;31m# use smaller integer precisions if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Could not convert 'ORT' with type str: tried to convert to int64",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8624e2766908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tokenized_dataset = dataset.map(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         }\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2352\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2354\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2355\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mcol_try_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtry_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtry_features\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mtyped_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_try_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    208\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mlist_of_np_array_to_pyarrow_listarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_1d_for_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowInvalid\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"overflow\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Could not convert 'ORT' with type str: tried to convert to int64"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca31ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
