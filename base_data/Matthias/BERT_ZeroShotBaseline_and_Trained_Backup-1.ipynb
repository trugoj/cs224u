{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1524a41",
   "metadata": {},
   "source": [
    "# BERT for *gNER*: Zero-Shot Baseline and Trained Model\n",
    "- Prepare data\n",
    "- 7.2\n",
    "- Tokenize\n",
    "- Make datasetdict\n",
    "- Build model for gNER labels\n",
    "- 0-shot baseline\n",
    "- training and loading of best model\n",
    "- evaluation on the test set\n",
    "- list of links (maybe incorporate in markdown)\n",
    "\n",
    "## Building a *gNER* `DatasetDict` for Hugging Face models\n",
    "Eventually, we want to train a Hugging Face transformer moder on the NER-annotated data from the `annotations2.jsonl`. To this end, we need to bring that data into the correct format, namely a `dataset` or a `DatasetDict`. Here, we want to save all relevant dataset splits (training, validation, and test splits) in a single `DatasetDict`.\n",
    "\n",
    "### Loading and inspecting the data\n",
    "Start by loading the data. This and many other steps regarding the data are very similar to the steps in the notebook [CRF-Model_MajorityBaseline_and_HyperparameterSearch](./CRF-Model_MajorityBaseline_and_HyperparameterSearch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848b05ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances:\n",
      "140\n",
      "\n",
      "all keys:\n",
      "['text', 'meta', '_input_hash', '_task_hash', 'spans', 'tokens', '_view_id', 'answer', '_timestamp']\n",
      "\n",
      "important keys:\n",
      "['text', 'spans', 'tokens']\n",
      "\n",
      "example text:\n",
      "DORNBIRN In der Schulgasse in Dornbirn hat eine 71,93 Quadratmeter große Wohnung für einen Quadratmeterpreis von 5533,71 Euro den Besitzer gewechselt. Dieser beinhaltet auch einen Pkw-Abstellplatz. Käufer der Wohnung mit 9,86 Quadratmetern Terrasse ist die ValLiLean Beteiligungs- und Immobilienverwaltungs GmbH. Beim Verkäufer handelt es sich um die Karrenblick Projekt GmbH.  Der Kaufpreis liegt bei 398.040 Euro. Unterzeichnet wurde der Kaufvertrag am 18. September. Die Verbücherung datiert mit Oktober 2020.\n",
      "\n",
      "5 example spans:\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'pattern': 2069086582, 'token_start': 0, 'token_end': 0, 'label': 'ORT', 'noWords': 1}\n",
      "{'start': 16, 'end': 26, 'token_start': 3, 'token_end': 3, 'label': 'STRASSE', 'noWords': 1}\n",
      "{'text': 'Dornbirn', 'start': 30, 'end': 38, 'pattern': 2069086582, 'token_start': 5, 'token_end': 5, 'label': 'ORT', 'noWords': 1}\n",
      "{'start': 48, 'end': 53, 'token_start': 8, 'token_end': 8, 'label': 'FLAECHE', 'noWords': 1}\n",
      "{'start': 73, 'end': 80, 'token_start': 11, 'token_end': 11, 'label': 'IMMO_TYP', 'noWords': 1}\n",
      "\n",
      "5 example tokens:\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'id': 0, 'ws': True, 'label': 'ORT'}\n",
      "{'text': 'In', 'start': 9, 'end': 11, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'der', 'start': 12, 'end': 15, 'id': 2, 'ws': True, 'label': 'O'}\n",
      "{'text': 'Schulgasse', 'start': 16, 'end': 26, 'id': 3, 'ws': True, 'label': 'STRASSE'}\n",
      "{'text': 'in', 'start': 27, 'end': 29, 'id': 4, 'ws': True, 'label': 'O'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "with open(\"./annotations2.jsonl\") as jsonl_file: # . instead of ..\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]\n",
    "print(\"instances:\\n{}\".format(len(annot)))\n",
    "keys = [key for key in annot[0].keys()]\n",
    "print(\"\\nall keys:\\n{}\".format(keys))\n",
    "key_keys = [\"text\", \"spans\", \"tokens\"]\n",
    "print(\"\\nimportant keys:\\n{}\".format(key_keys))\n",
    "print(\"\\nexample text:\\n{}\".format(annot[0][\"text\"]))\n",
    "n_examples = 5\n",
    "print(\"\\n{} example spans:\".format(n_examples))\n",
    "for span in annot[0][\"spans\"][:n_examples]:\n",
    "    print(\"{}\".format(span))\n",
    "print(\"\\n{} example tokens:\".format(n_examples))\n",
    "for token in annot[0][\"tokens\"][:n_examples]:\n",
    "    print(\"{}\".format(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8eee87",
   "metadata": {},
   "source": [
    "Given a list of span dictionaries (key `\"spans\"`) and an index referring to the text position within the real estate offering text, return the `\"label\"` obtained via [prodigy](https://prodi.gy/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f9b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token dictionaries for the last 3 words of instance 0\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'id': 0, 'ws': True, 'label': 'ORT'}\n",
      "{'text': 'In', 'start': 9, 'end': 11, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'der', 'start': 12, 'end': 15, 'id': 2, 'ws': True, 'label': 'O'}\n",
      "Token dictionaries for the last 3 words of instance 1\n",
      "{'text': 'FELDKIRCH', 'start': 0, 'end': 9, 'id': 0, 'ws': True, 'label': 'ORT'}\n",
      "{'text': 'Im', 'start': 10, 'end': 12, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'Altenreuteweg', 'start': 13, 'end': 26, 'id': 2, 'ws': True, 'label': 'STRASSE'}\n"
     ]
    }
   ],
   "source": [
    "def getLabel(tokenDictList, idx):\n",
    "    result = \"O\"\n",
    "    for dict_i in tokenDictList:\n",
    "        idx_0, idx_1 = dict_i[\"start\"], dict_i[\"end\"]\n",
    "        if (idx_0<=idx) and (idx<=idx_1):\n",
    "            result = dict_i[\"label\"]\n",
    "    return result \n",
    "\n",
    "for j in range(len(annot)): # loop over instances\n",
    "    a = annot[j]            # instance j\n",
    "    spans = a[\"spans\"]      # list of annotation dicts\n",
    "    toks = a[\"tokens\"]      # list of token dicts\n",
    "    for i in range(len(toks)):                                 # loop over token dicts\n",
    "        toks[i][\"label\"] = getLabel(spans, toks[i][\"start\"])   # assign label from span (if exists, otherwise \"O\")\n",
    "    annot[j][\"tokens\"] = toks\n",
    "\n",
    "words_n = 3\n",
    "for i in range(2):\n",
    "    print(\"Token dictionaries for the last {} words of instance {}\".format(words_n, i))\n",
    "    ann = annot[i]\n",
    "    for tok in ann[\"tokens\"][:words_n]:\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cafe86",
   "metadata": {},
   "source": [
    "### Retrieve the relevant data\n",
    "Enhance the `\"tokens\"` dictionaries by labels. The labels are stored in the `spans` dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d059dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('DORNBIRN', 'ORT'),\n",
       "  ('In', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Schulgasse', 'STRASSE'),\n",
       "  ('in', 'O'),\n",
       "  ('Dornbirn', 'ORT'),\n",
       "  ('hat', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('71,93', 'FLAECHE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('große', 'O'),\n",
       "  ('Wohnung', 'IMMO_TYP'),\n",
       "  ('für', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Quadratmeterpreis', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('5533,71', 'QMPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('den', 'O'),\n",
       "  ('Besitzer', 'O'),\n",
       "  ('gewechselt', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Dieser', 'O'),\n",
       "  ('beinhaltet', 'O'),\n",
       "  ('auch', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Pkw-Abstellplatz', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Käufer', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Wohnung', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('9,86', 'TERRASSENGROESSE'),\n",
       "  ('Quadratmetern', 'O'),\n",
       "  ('Terrasse', 'O'),\n",
       "  ('ist', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('ValLiLean', 'KAEUFER'),\n",
       "  ('Beteiligungs-', 'KAEUFER'),\n",
       "  ('und', 'KAEUFER'),\n",
       "  ('Immobilienverwaltungs', 'KAEUFER'),\n",
       "  ('GmbH', 'KAEUFER'),\n",
       "  ('.', 'KAEUFER'),\n",
       "  ('Beim', 'O'),\n",
       "  ('Verkäufer', 'O'),\n",
       "  ('handelt', 'O'),\n",
       "  ('es', 'O'),\n",
       "  ('sich', 'O'),\n",
       "  ('um', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('Karrenblick', 'VERKAEUFER'),\n",
       "  ('Projekt', 'VERKAEUFER'),\n",
       "  ('GmbH', 'VERKAEUFER'),\n",
       "  ('.', 'VERKAEUFER'),\n",
       "  (' ', 'O'),\n",
       "  ('Der', 'O'),\n",
       "  ('Kaufpreis', 'O'),\n",
       "  ('liegt', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('398.040', 'GESAMTPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Unterzeichnet', 'O'),\n",
       "  ('wurde', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Kaufvertrag', 'O'),\n",
       "  ('am', 'O'),\n",
       "  ('18.', 'DATUM_VERTRAG'),\n",
       "  ('September', 'DATUM_VERTRAG'),\n",
       "  ('.', 'DATUM_VERTRAG'),\n",
       "  ('Die', 'O'),\n",
       "  ('Verbücherung', 'O'),\n",
       "  ('datiert', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('Oktober', 'DATUM_VERBUECHERUNG'),\n",
       "  ('2020', 'DATUM_VERBUECHERUNG'),\n",
       "  ('.', 'DATUM_VERBUECHERUNG')],\n",
       " [('FELDKIRCH', 'ORT'),\n",
       "  ('Im', 'O'),\n",
       "  ('Altenreuteweg', 'STRASSE'),\n",
       "  ('in', 'O'),\n",
       "  ('Feldkirch', 'ORT'),\n",
       "  ('hat', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('100,67', 'FLAECHE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('große', 'O'),\n",
       "  ('Wohnung', 'IMMO_TYP'),\n",
       "  ('für', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Quadratmeterpreis', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('6168,67', 'QMPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('den', 'O'),\n",
       "  ('Besitzer', 'O'),\n",
       "  ('gewechselt', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Käufer', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Wohnung', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('einer', 'O'),\n",
       "  ('137,49', 'TERRASSENGROESSE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('großen', 'O'),\n",
       "  ('Terrasse', 'O'),\n",
       "  ('ist', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('Privatperson', 'KAEUFER'),\n",
       "  ('.', 'KAEUFER'),\n",
       "  ('Beim', 'O'),\n",
       "  ('Verkäufer', 'O'),\n",
       "  ('handelt', 'O'),\n",
       "  ('es', 'O'),\n",
       "  ('sich', 'O'),\n",
       "  ('um', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('Rüscher', 'VERKAEUFER'),\n",
       "  ('und', 'VERKAEUFER'),\n",
       "  ('Söhne', 'VERKAEUFER'),\n",
       "  ('Bau', 'VERKAEUFER'),\n",
       "  ('GmbH', 'VERKAEUFER'),\n",
       "  ('&', 'VERKAEUFER'),\n",
       "  ('Co', 'VERKAEUFER'),\n",
       "  ('KG', 'VERKAEUFER'),\n",
       "  ('.', 'VERKAEUFER'),\n",
       "  (' ', 'O'),\n",
       "  ('Der', 'O'),\n",
       "  ('Kaufpreis', 'O'),\n",
       "  ('liegt', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('621.000', 'GESAMTPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Unterzeichnet', 'O'),\n",
       "  ('wurde', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Kaufvertrag', 'O'),\n",
       "  ('am', 'O'),\n",
       "  ('11.', 'DATUM_VERTRAG'),\n",
       "  ('August', 'DATUM_VERTRAG'),\n",
       "  ('.', 'DATUM_VERTRAG'),\n",
       "  ('Die', 'O'),\n",
       "  ('Verbücherung', 'O'),\n",
       "  ('datiert', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('September', 'DATUM_VERBUECHERUNG'),\n",
       "  ('2020', 'DATUM_VERBUECHERUNG'),\n",
       "  ('.', 'DATUM_VERBUECHERUNG')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents=[] \n",
    "for annot_i in annot:                  # loop over instances\n",
    "    toks = annot_i[\"tokens\"]           # get tokens list for instance i\n",
    "    train_sentence = []\n",
    "    for tok in toks:                   # loop over token dicts\n",
    "        if \"label\" in tok:             # only if the current token has been labelled, ...\n",
    "            token_element = (tok[\"text\"], tok[\"label\"]) # ... create a \"text\", \"label\" pair for this token ...\n",
    "            train_sentence.append(token_element)        # ... and append it to the list\n",
    "    sents.append(train_sentence) # append the list for that instances to the list for all instances / sentences\n",
    "\n",
    "# list of lists of pairs (sets): outer list contains instances and inner list contains (token, label) pairs\n",
    "sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec55c60",
   "metadata": {},
   "source": [
    "From `sents`, which is a list of instances where each instance is a list word-and-label pairs, we create two equally long lists of instances where list 1 contains lists of words (representing the text for an instance) and where list 2 contains lists of labels (representing the NER tags listed in list 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bb2f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 140)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. build words / tokens = list of lists of tokens\n",
    "# 2. build ner_tags / labels = list of lists of labels\n",
    "words = []\n",
    "ner_tags = []\n",
    "for sent_i in sents:\n",
    "    words_i = []\n",
    "    ner_tags_i = []\n",
    "    for item in sent_i:\n",
    "        words_i.append(item[0])\n",
    "        ner_tags_i.append(item[1])\n",
    "    words.append(words_i)\n",
    "    ner_tags.append(ner_tags_i)\n",
    "\n",
    "len(words), len(ner_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29cde6",
   "metadata": {},
   "source": [
    "### Data splits\n",
    "We use 80% of the data for development the remaining 20% for final evaluation. The 80% for development are further split into 64% for training and 16% for validation.\n",
    "\n",
    "Start with the first 80%-20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01db16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[45, 59, 7, 50, 92]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(104, 104, 36, 36)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = 0.75\n",
    "train_test_split = round(0.75*len(words) - 0.5) # -0.5 => floor\n",
    "idx = [i for i in range(len(words))]\n",
    "print(idx[:5])\n",
    "idx_shuffle = shuffle(idx, random_state=0)\n",
    "print(idx_shuffle[:5])\n",
    "words_shuffle, ner_tags_shuffle = [words[idx_i] for idx_i in idx_shuffle], [ner_tags[idx_i] for idx_i in idx_shuffle]\n",
    "words_train, words_test = words_shuffle[:train_test_split], words_shuffle[train_test_split:]\n",
    "ner_tags_train, ner_tags_test = ner_tags_shuffle[:train_test_split], ner_tags_shuffle[train_test_split:]\n",
    "len(words_train), len(ner_tags_train), len(words_test), len(ner_tags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1cf5e",
   "metadata": {},
   "source": [
    "Split the 80% part (104 instances) further into training and validation splits. Then, merge all three splits into a `DatasetDict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b71b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'ner_tags'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['words', 'ner_tags'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'ner_tags'],\n",
       "        num_rows: 36\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict({\"words\": words_train, \"ner_tags\": ner_tags_train})\n",
    "test_dataset = Dataset.from_dict({\"words\": words_test, \"ner_tags\": ner_tags_test})\n",
    "train_valid_split = train_dataset.train_test_split(shuffle=True, seed=42, test_size=0.25)\n",
    "untokenizedDatasetDict = DatasetDict({\n",
    "    \"train\": train_valid_split[\"train\"],\n",
    "    \"valid\": train_valid_split[\"test\"],\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "untokenizedDatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb468c",
   "metadata": {},
   "source": [
    "In order to get a list of all `ner_tags` without duplicates, we flatten the equally named list, transform it into a set ($\\Rightarrow$ no duplicates) and transform it back into a list. Then, we also sort that list by the leading characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50868c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATUM_VERBUECHERUNG',\n",
       " 'DATUM_VERTRAG',\n",
       " 'FLAECHE',\n",
       " 'GESAMTPREIS',\n",
       " 'IMMO_TYP',\n",
       " 'KAEUFER',\n",
       " 'O',\n",
       " 'ORT',\n",
       " 'QMPREIS',\n",
       " 'STRASSE',\n",
       " 'TERRASSENGROESSE',\n",
       " 'VERKAEUFER']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set of labels (as sorted list)\n",
    "ner_tag_names = sorted(list(set([ner_tag_ij for ner_tags_i in ner_tags for ner_tag_ij in ner_tags_i])))\n",
    "ner_tag_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926b4f3",
   "metadata": {},
   "source": [
    "### Tokenize data and build `DatasetDict`\n",
    "Select a promising checkpoint from the [model hub](https://huggingface.co/models), instantiate the according tokenizer, and check whether it is a *fast* tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2f3f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and adjust labels\n",
    "checkpoint = \"flair/ner-german\"  # https://huggingface.co/flair/ner-german (1.41GB)\n",
    "checkpoint = \"fhswf/bert_de_ner\" # https://huggingface.co/fhswf/bert_de_ner (419MB)\n",
    "checkpoint = \"bert-base-cased\"   # https://huggingface.co/bert-base-cased (416MB)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc19f20",
   "metadata": {},
   "source": [
    "Tokenization turns words into tokens, thus often leading to longer sequences. However, the labels / `\"ner_tags\"` are not automatically adapted to a modified sequence length. The code below handles this by repeating the NER tag for a word that has been split into multiple tokens as often as is necessary to match the length. Below is only a demonstration with according output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd487a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\ttoken: [CLS]\tword_id: None\taligned label: -100\ttext label: SPECIAL TOKEN\n",
      "index: 1\ttoken: L\tword_id: 0\taligned label: 7\ttext label: ORT\n",
      "index: 2\ttoken: ##US\tword_id: 0\taligned label: 7\ttext label: ORT\n",
      "index: 3\ttoken: ##TE\tword_id: 0\taligned label: 7\ttext label: ORT\n",
      "index: 4\ttoken: ##NA\tword_id: 0\taligned label: 7\ttext label: ORT\n",
      "index: 5\ttoken: ##U\tword_id: 0\taligned label: 7\ttext label: ORT\n",
      "index: 6\ttoken: In\tword_id: 1\taligned label: 6\ttext label: O\n",
      "index: 7\ttoken: der\tword_id: 2\taligned label: 6\ttext label: O\n",
      "index: 8\ttoken: Stein\tword_id: 3\taligned label: 9\ttext label: STRASSE\n",
      "index: 9\ttoken: ##ack\tword_id: 3\taligned label: 9\ttext label: STRASSE\n",
      "index: 10\ttoken: ##ers\tword_id: 3\taligned label: 9\ttext label: STRASSE\n",
      "index: 11\ttoken: ##tra\tword_id: 3\taligned label: 9\ttext label: STRASSE\n",
      "index: 12\ttoken: ##ße\tword_id: 3\taligned label: 9\ttext label: STRASSE\n",
      "index: 13\ttoken: 26\tword_id: 4\taligned label: 9\ttext label: STRASSE\n",
      "index: 14\ttoken: in\tword_id: 5\taligned label: 6\ttext label: O\n",
      "index: 15\ttoken: Lu\tword_id: 6\taligned label: 7\ttext label: ORT\n",
      "index: 16\ttoken: ##sten\tword_id: 6\taligned label: 7\ttext label: ORT\n",
      "index: 17\ttoken: ##au\tword_id: 6\taligned label: 7\ttext label: ORT\n",
      "index: 18\ttoken: hat\tword_id: 7\taligned label: 6\ttext label: O\n",
      "index: 19\ttoken: e\tword_id: 8\taligned label: 6\ttext label: O\n",
      "index: 20\ttoken: ##ine\tword_id: 8\taligned label: 6\ttext label: O\n",
      "index: 21\ttoken: 93\tword_id: 9\taligned label: 2\ttext label: FLAECHE\n",
      "index: 22\ttoken: ,\tword_id: 9\taligned label: 2\ttext label: FLAECHE\n",
      "index: 23\ttoken: 8\tword_id: 9\taligned label: 2\ttext label: FLAECHE\n",
      "index: 24\ttoken: Q\tword_id: 10\taligned label: 6\ttext label: O\n",
      "index: 25\ttoken: ##uad\tword_id: 10\taligned label: 6\ttext label: O\n",
      "index: 26\ttoken: ##rat\tword_id: 10\taligned label: 6\ttext label: O\n",
      "index: 27\ttoken: ##meter\tword_id: 10\taligned label: 6\ttext label: O\n",
      "index: 28\ttoken: g\tword_id: 11\taligned label: 6\ttext label: O\n",
      "index: 29\ttoken: ##ro\tword_id: 11\taligned label: 6\ttext label: O\n",
      "index: 30\ttoken: ##ße\tword_id: 11\taligned label: 6\ttext label: O\n",
      "index: 31\ttoken: W\tword_id: 12\taligned label: 4\ttext label: IMMO_TYP\n",
      "index: 32\ttoken: ##oh\tword_id: 12\taligned label: 4\ttext label: IMMO_TYP\n",
      "index: 33\ttoken: ##nu\tword_id: 12\taligned label: 4\ttext label: IMMO_TYP\n",
      "index: 34\ttoken: ##ng\tword_id: 12\taligned label: 4\ttext label: IMMO_TYP\n",
      "index: 35\ttoken: für\tword_id: 13\taligned label: 6\ttext label: O\n",
      "index: 36\ttoken: e\tword_id: 14\taligned label: 6\ttext label: O\n",
      "index: 37\ttoken: ##inen\tword_id: 14\taligned label: 6\ttext label: O\n",
      "index: 38\ttoken: Q\tword_id: 15\taligned label: 6\ttext label: O\n",
      "index: 39\ttoken: ##uad\tword_id: 15\taligned label: 6\ttext label: O\n",
      "index: 40\ttoken: ##rat\tword_id: 15\taligned label: 6\ttext label: O\n",
      "index: 41\ttoken: ##meter\tword_id: 15\taligned label: 6\ttext label: O\n",
      "index: 42\ttoken: ##p\tword_id: 15\taligned label: 6\ttext label: O\n",
      "index: 43\ttoken: ##reis\tword_id: 15\taligned label: 6\ttext label: O\n",
      "index: 44\ttoken: von\tword_id: 16\taligned label: 6\ttext label: O\n",
      "index: 45\ttoken: 45\tword_id: 17\taligned label: 8\ttext label: QMPREIS\n",
      "index: 46\ttoken: ##8\tword_id: 17\taligned label: 8\ttext label: QMPREIS\n",
      "index: 47\ttoken: ##4\tword_id: 17\taligned label: 8\ttext label: QMPREIS\n",
      "index: 48\ttoken: ,\tword_id: 17\taligned label: 8\ttext label: QMPREIS\n",
      "index: 49\ttoken: 22\tword_id: 17\taligned label: 8\ttext label: QMPREIS\n",
      "index: 50\ttoken: Euro\tword_id: 18\taligned label: 6\ttext label: O\n",
      "index: 51\ttoken: den\tword_id: 19\taligned label: 6\ttext label: O\n",
      "index: 52\ttoken: Be\tword_id: 20\taligned label: 6\ttext label: O\n",
      "index: 53\ttoken: ##si\tword_id: 20\taligned label: 6\ttext label: O\n",
      "index: 54\ttoken: ##tz\tword_id: 20\taligned label: 6\ttext label: O\n",
      "index: 55\ttoken: ##er\tword_id: 20\taligned label: 6\ttext label: O\n",
      "index: 56\ttoken: g\tword_id: 21\taligned label: 6\ttext label: O\n",
      "index: 57\ttoken: ##ew\tword_id: 21\taligned label: 6\ttext label: O\n",
      "index: 58\ttoken: ##ech\tword_id: 21\taligned label: 6\ttext label: O\n",
      "index: 59\ttoken: ##sel\tword_id: 21\taligned label: 6\ttext label: O\n",
      "index: 60\ttoken: ##t\tword_id: 21\taligned label: 6\ttext label: O\n",
      "index: 61\ttoken: .\tword_id: 22\taligned label: 6\ttext label: O\n",
      "index: 62\ttoken: Die\tword_id: 23\taligned label: 6\ttext label: O\n",
      "index: 63\ttoken: ##ser\tword_id: 23\taligned label: 6\ttext label: O\n",
      "index: 64\ttoken: be\tword_id: 24\taligned label: 6\ttext label: O\n",
      "index: 65\ttoken: ##in\tword_id: 24\taligned label: 6\ttext label: O\n",
      "index: 66\ttoken: ##halt\tword_id: 24\taligned label: 6\ttext label: O\n",
      "index: 67\ttoken: ##et\tword_id: 24\taligned label: 6\ttext label: O\n",
      "index: 68\ttoken: au\tword_id: 25\taligned label: 6\ttext label: O\n",
      "index: 69\ttoken: ##ch\tword_id: 25\taligned label: 6\ttext label: O\n",
      "index: 70\ttoken: e\tword_id: 26\taligned label: 6\ttext label: O\n",
      "index: 71\ttoken: ##inen\tword_id: 26\taligned label: 6\ttext label: O\n",
      "index: 72\ttoken: P\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 73\ttoken: ##k\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 74\ttoken: ##w\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 75\ttoken: -\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 76\ttoken: A\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 77\ttoken: ##bs\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 78\ttoken: ##tel\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 79\ttoken: ##l\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 80\ttoken: ##p\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 81\ttoken: ##latz\tword_id: 27\taligned label: 6\ttext label: O\n",
      "index: 82\ttoken: .\tword_id: 28\taligned label: 6\ttext label: O\n",
      "index: 83\ttoken: Be\tword_id: 29\taligned label: 6\ttext label: O\n",
      "index: 84\ttoken: ##i\tword_id: 29\taligned label: 6\ttext label: O\n",
      "index: 85\ttoken: K\tword_id: 30\taligned label: 6\ttext label: O\n",
      "index: 86\ttoken: ##ä\tword_id: 30\taligned label: 6\ttext label: O\n",
      "index: 87\ttoken: ##uf\tword_id: 30\taligned label: 6\ttext label: O\n",
      "index: 88\ttoken: ##er\tword_id: 30\taligned label: 6\ttext label: O\n",
      "index: 89\ttoken: und\tword_id: 31\taligned label: 6\ttext label: O\n",
      "index: 90\ttoken: V\tword_id: 32\taligned label: 6\ttext label: O\n",
      "index: 91\ttoken: ##er\tword_id: 32\taligned label: 6\ttext label: O\n",
      "index: 92\ttoken: ##k\tword_id: 32\taligned label: 6\ttext label: O\n",
      "index: 93\ttoken: ##ä\tword_id: 32\taligned label: 6\ttext label: O\n",
      "index: 94\ttoken: ##uf\tword_id: 32\taligned label: 6\ttext label: O\n",
      "index: 95\ttoken: ##er\tword_id: 32\taligned label: 6\ttext label: O\n",
      "index: 96\ttoken: der\tword_id: 33\taligned label: 6\ttext label: O\n",
      "index: 97\ttoken: W\tword_id: 34\taligned label: 6\ttext label: O\n",
      "index: 98\ttoken: ##oh\tword_id: 34\taligned label: 6\ttext label: O\n",
      "index: 99\ttoken: ##nu\tword_id: 34\taligned label: 6\ttext label: O\n",
      "index: 100\ttoken: ##ng\tword_id: 34\taligned label: 6\ttext label: O\n",
      "index: 101\ttoken: mit\tword_id: 35\taligned label: 6\ttext label: O\n",
      "index: 102\ttoken: 22\tword_id: 36\taligned label: 10\ttext label: TERRASSENGROESSE\n",
      "index: 103\ttoken: ,\tword_id: 36\taligned label: 10\ttext label: TERRASSENGROESSE\n",
      "index: 104\ttoken: 26\tword_id: 36\taligned label: 10\ttext label: TERRASSENGROESSE\n",
      "index: 105\ttoken: Q\tword_id: 37\taligned label: 6\ttext label: O\n",
      "index: 106\ttoken: ##uad\tword_id: 37\taligned label: 6\ttext label: O\n",
      "index: 107\ttoken: ##rat\tword_id: 37\taligned label: 6\ttext label: O\n",
      "index: 108\ttoken: ##meter\tword_id: 37\taligned label: 6\ttext label: O\n",
      "index: 109\ttoken: ##n\tword_id: 37\taligned label: 6\ttext label: O\n",
      "index: 110\ttoken: Terra\tword_id: 38\taligned label: 6\ttext label: O\n",
      "index: 111\ttoken: ##sse\tword_id: 38\taligned label: 6\ttext label: O\n",
      "index: 112\ttoken: hand\tword_id: 39\taligned label: 6\ttext label: O\n",
      "index: 113\ttoken: ##el\tword_id: 39\taligned label: 6\ttext label: O\n",
      "index: 114\ttoken: ##t\tword_id: 39\taligned label: 6\ttext label: O\n",
      "index: 115\ttoken: es\tword_id: 40\taligned label: 6\ttext label: O\n",
      "index: 116\ttoken: sic\tword_id: 41\taligned label: 6\ttext label: O\n",
      "index: 117\ttoken: ##h\tword_id: 41\taligned label: 6\ttext label: O\n",
      "index: 118\ttoken: um\tword_id: 42\taligned label: 6\ttext label: O\n",
      "index: 119\ttoken: P\tword_id: 43\taligned label: 11\ttext label: VERKAEUFER\n",
      "index: 120\ttoken: ##ri\tword_id: 43\taligned label: 11\ttext label: VERKAEUFER\n",
      "index: 121\ttoken: ##vat\tword_id: 43\taligned label: 11\ttext label: VERKAEUFER\n",
      "index: 122\ttoken: ##person\tword_id: 43\taligned label: 11\ttext label: VERKAEUFER\n",
      "index: 123\ttoken: ##en\tword_id: 43\taligned label: 11\ttext label: VERKAEUFER\n",
      "index: 124\ttoken: .\tword_id: 44\taligned label: 11\ttext label: VERKAEUFER\n",
      "index: 125\ttoken: Der\tword_id: 45\taligned label: 6\ttext label: O\n",
      "index: 126\ttoken: Ka\tword_id: 46\taligned label: 6\ttext label: O\n",
      "index: 127\ttoken: ##uf\tword_id: 46\taligned label: 6\ttext label: O\n",
      "index: 128\ttoken: ##p\tword_id: 46\taligned label: 6\ttext label: O\n",
      "index: 129\ttoken: ##reis\tword_id: 46\taligned label: 6\ttext label: O\n",
      "index: 130\ttoken: lie\tword_id: 47\taligned label: 6\ttext label: O\n",
      "index: 131\ttoken: ##gt\tword_id: 47\taligned label: 6\ttext label: O\n",
      "index: 132\ttoken: be\tword_id: 48\taligned label: 6\ttext label: O\n",
      "index: 133\ttoken: ##i\tword_id: 48\taligned label: 6\ttext label: O\n",
      "index: 134\ttoken: 430\tword_id: 49\taligned label: 3\ttext label: GESAMTPREIS\n",
      "index: 135\ttoken: .\tword_id: 49\taligned label: 3\ttext label: GESAMTPREIS\n",
      "index: 136\ttoken: 000\tword_id: 49\taligned label: 3\ttext label: GESAMTPREIS\n",
      "index: 137\ttoken: Euro\tword_id: 50\taligned label: 6\ttext label: O\n",
      "index: 138\ttoken: .\tword_id: 51\taligned label: 6\ttext label: O\n",
      "index: 139\ttoken: Un\tword_id: 52\taligned label: 6\ttext label: O\n",
      "index: 140\ttoken: ##ter\tword_id: 52\taligned label: 6\ttext label: O\n",
      "index: 141\ttoken: ##ze\tword_id: 52\taligned label: 6\ttext label: O\n",
      "index: 142\ttoken: ##ich\tword_id: 52\taligned label: 6\ttext label: O\n",
      "index: 143\ttoken: ##net\tword_id: 52\taligned label: 6\ttext label: O\n",
      "index: 144\ttoken: w\tword_id: 53\taligned label: 6\ttext label: O\n",
      "index: 145\ttoken: ##ur\tword_id: 53\taligned label: 6\ttext label: O\n",
      "index: 146\ttoken: ##de\tword_id: 53\taligned label: 6\ttext label: O\n",
      "index: 147\ttoken: der\tword_id: 54\taligned label: 6\ttext label: O\n",
      "index: 148\ttoken: Ka\tword_id: 55\taligned label: 6\ttext label: O\n",
      "index: 149\ttoken: ##uf\tword_id: 55\taligned label: 6\ttext label: O\n",
      "index: 150\ttoken: ##vert\tword_id: 55\taligned label: 6\ttext label: O\n",
      "index: 151\ttoken: ##rag\tword_id: 55\taligned label: 6\ttext label: O\n",
      "index: 152\ttoken: am\tword_id: 56\taligned label: 6\ttext label: O\n",
      "index: 153\ttoken: 6\tword_id: 57\taligned label: 1\ttext label: DATUM_VERTRAG\n",
      "index: 154\ttoken: .\tword_id: 57\taligned label: 1\ttext label: DATUM_VERTRAG\n",
      "index: 155\ttoken: Jul\tword_id: 58\taligned label: 1\ttext label: DATUM_VERTRAG\n",
      "index: 156\ttoken: ##i\tword_id: 58\taligned label: 1\ttext label: DATUM_VERTRAG\n",
      "index: 157\ttoken: .\tword_id: 59\taligned label: 1\ttext label: DATUM_VERTRAG\n",
      "index: 158\ttoken: Die\tword_id: 60\taligned label: 6\ttext label: O\n",
      "index: 159\ttoken: V\tword_id: 61\taligned label: 6\ttext label: O\n",
      "index: 160\ttoken: ##er\tword_id: 61\taligned label: 6\ttext label: O\n",
      "index: 161\ttoken: ##b\tword_id: 61\taligned label: 6\ttext label: O\n",
      "index: 162\ttoken: ##ü\tword_id: 61\taligned label: 6\ttext label: O\n",
      "index: 163\ttoken: ##cher\tword_id: 61\taligned label: 6\ttext label: O\n",
      "index: 164\ttoken: ##ung\tword_id: 61\taligned label: 6\ttext label: O\n",
      "index: 165\ttoken: da\tword_id: 62\taligned label: 6\ttext label: O\n",
      "index: 166\ttoken: ##tier\tword_id: 62\taligned label: 6\ttext label: O\n",
      "index: 167\ttoken: ##t\tword_id: 62\taligned label: 6\ttext label: O\n",
      "index: 168\ttoken: mit\tword_id: 63\taligned label: 6\ttext label: O\n",
      "index: 169\ttoken: J\tword_id: 64\taligned label: 0\ttext label: DATUM_VERBUECHERUNG\n",
      "index: 170\ttoken: ##ä\tword_id: 64\taligned label: 0\ttext label: DATUM_VERBUECHERUNG\n",
      "index: 171\ttoken: ##nner\tword_id: 64\taligned label: 0\ttext label: DATUM_VERBUECHERUNG\n",
      "index: 172\ttoken: 202\tword_id: 65\taligned label: 0\ttext label: DATUM_VERBUECHERUNG\n",
      "index: 173\ttoken: ##1\tword_id: 65\taligned label: 0\ttext label: DATUM_VERBUECHERUNG\n",
      "index: 174\ttoken: .\tword_id: 66\taligned label: 0\ttext label: DATUM_VERBUECHERUNG\n",
      "index: 175\ttoken: [SEP]\tword_id: None\taligned label: -100\ttext label: SPECIAL TOKEN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(176, 176, 176)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def align_labels_with_tokens(ner_tags, word_ids):\n",
    "    #print(word_ids)\n",
    "    new_labels = []\n",
    "    previous_label = None\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        # handle word_id==None\n",
    "        if word_id==None:\n",
    "            label = -100\n",
    "        # handle word_id==previous_word_id\n",
    "        elif word_id==previous_word_id:\n",
    "            label = previous_label\n",
    "        # handle word_id!=previous_word_id and word_id!=None\n",
    "        else:\n",
    "            text_label = ner_tags[word_id]\n",
    "            label = ner_tag_names.index(text_label)\n",
    "        new_labels.append(label)\n",
    "        previous_label = label\n",
    "        previous_word_id = word_id\n",
    "    return new_labels\n",
    "\n",
    "instance = 0\n",
    "ner_tags = untokenizedDatasetDict[\"train\"][instance][\"ner_tags\"]\n",
    "word_ids = tokenizer(untokenizedDatasetDict[\"train\"][instance][\"words\"], is_split_into_words=True).word_ids()\n",
    "aligned_labels = align_labels_with_tokens(ner_tags, word_ids)\n",
    "inputs = tokenizer(untokenizedDatasetDict[\"train\"][instance][\"words\"], is_split_into_words=True)\n",
    "for i, token in enumerate(inputs.tokens()):\n",
    "    alabel = aligned_labels[i]\n",
    "    tlabel = ner_tag_names[alabel] if alabel>=0 else \"SPECIAL TOKEN\"\n",
    "    print(f\"index: {i}\\ttoken: {token}\\tword_id: {word_ids[i]}\\taligned label: {alabel}\\ttext label: {tlabel}\")\n",
    "len(inputs.tokens()), len(aligned_labels), len(word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0e1ba",
   "metadata": {},
   "source": [
    "Now, we apply this function to all splits of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7350c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a3a410e70c4714bdf7f034560c4d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700939beafc49d0a2e7998f2312d8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47989d02f28a44679512f84bfd48424f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 36\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "#\n",
    "gNerDatasetDict = untokenizedDatasetDict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=untokenizedDatasetDict[\"train\"].column_names\n",
    ")\n",
    "gNerDatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f2f97",
   "metadata": {},
   "source": [
    "Add the `ner_tag_names` via the `ClassLabel` class (just as an exercise [&#128521;](https://www.w3schools.com/charsets/ref_emoji_smileys.asp))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d29c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(num_classes=12, names=['DATUM_VERBUECHERUNG', 'DATUM_VERTRAG', 'FLAECHE', 'GESAMTPREIS', 'IMMO_TYP', 'KAEUFER', 'O', 'ORT', 'QMPREIS', 'STRASSE', 'TERRASSENGROESSE', 'VERKAEUFER'], id=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DATUM_VERBUECHERUNG',\n",
       " 'DATUM_VERTRAG',\n",
       " 'FLAECHE',\n",
       " 'GESAMTPREIS',\n",
       " 'IMMO_TYP',\n",
       " 'KAEUFER',\n",
       " 'O',\n",
       " 'ORT',\n",
       " 'QMPREIS',\n",
       " 'STRASSE',\n",
       " 'TERRASSENGROESSE',\n",
       " 'VERKAEUFER']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gNerDatasetDict[\"train\"].features[\"labels\"].feature.names = ClassLabel(names=ner_tag_names)\n",
    "print(gNerDatasetDict[\"train\"].features[\"labels\"].feature.names)\n",
    "gNerDatasetDict[\"train\"].features[\"labels\"].feature.names.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c998c",
   "metadata": {},
   "source": [
    "Save `gNerDatasetDict` to the local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e42fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gNerDatasetDict.save_to_disk(\"gNERdataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667a266",
   "metadata": {},
   "source": [
    "Reload the dataset dictionary from disk! However, the `ClassLabel` feature list with the names of the NER tags is not available, anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e75a04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 36\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gNerDataset = load_from_disk(\"gNERdataset\")\n",
    "gNerDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a7315",
   "metadata": {},
   "source": [
    "Trying to access the `ClassLabel` feature would lead to the following error.\n",
    "```python\n",
    "gNerDataset[\"train\"].features[\"labels\"].feature.names.names\n",
    "\n",
    "AttributeError: 'Value' object has no attribute 'names'\n",
    "```\n",
    "## Using a [BERT](https://arxiv.org/abs/1810.04805) model from  Hugging Face for evaluation on the *gNER* `DatasetDict`\n",
    "In order to train a model on the *gNER* `DatasetDict` that we have just built, we need to instantiate a `DataCollatorForTokenClassification`, a model for [token classification](https://huggingface.co/course/chapter7/2?fw=pt), `TrainingArguments`, and a `Trainer`. We also want to avoid using a version of the model that has been overfitted on the training set. But before we get to training, we establish a zero-shot baseline that uses the working model without it having ever seen a single instance of our dataset.\n",
    "\n",
    "### Building the model\n",
    "Create `id2label` and `label2id` dictionaries as we will build the model following the Hugging Face course section on [token classification](https://huggingface.co/course/chapter7/2?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf1e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': 'DATUM_VERBUECHERUNG',\n",
       "  '1': 'DATUM_VERTRAG',\n",
       "  '2': 'FLAECHE',\n",
       "  '3': 'GESAMTPREIS',\n",
       "  '4': 'IMMO_TYP',\n",
       "  '5': 'KAEUFER',\n",
       "  '6': 'O',\n",
       "  '7': 'ORT',\n",
       "  '8': 'QMPREIS',\n",
       "  '9': 'STRASSE',\n",
       "  '10': 'TERRASSENGROESSE',\n",
       "  '11': 'VERKAEUFER'},\n",
       " {'DATUM_VERBUECHERUNG': '0',\n",
       "  'DATUM_VERTRAG': '1',\n",
       "  'FLAECHE': '2',\n",
       "  'GESAMTPREIS': '3',\n",
       "  'IMMO_TYP': '4',\n",
       "  'KAEUFER': '5',\n",
       "  'O': '6',\n",
       "  'ORT': '7',\n",
       "  'QMPREIS': '8',\n",
       "  'STRASSE': '9',\n",
       "  'TERRASSENGROESSE': '10',\n",
       "  'VERKAEUFER': '11'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(ner_tag_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2681f3",
   "metadata": {},
   "source": [
    "Create the model via `AutoModelForTokenClassification.from_pretrained()`, using the same checkpoint as for the tokenizer above, and confirm that the number of labels - implied by `id2label` and `label2id` - matches the number of different labels that occur in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb84a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caa2ff4f3f04b179a32562f5ee21881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4dad93",
   "metadata": {},
   "source": [
    "### `DataCollatorForTokenClassification` and macro-averaged $F_1$-scoring\n",
    "In order to feed batches of square data, i.e. instances of equal length, to our model, we instantiate a `DataCollatorForTokenClassification` that uses our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b6488e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    7,    7,    7,    7,    7,    6,    6,    9,    9,    9,    9,\n",
       "            9,    9,    6,    7,    7,    7,    6,    6,    6,    2,    2,    2,\n",
       "            6,    6,    6,    6,    6,    6,    6,    4,    4,    4,    4,    6,\n",
       "            6,    6,    6,    6,    6,    6,    6,    6,    6,    8,    8,    8,\n",
       "            8,    8,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    6,    6,    6,    6,    6,   10,   10,   10,    6,    6,    6,\n",
       "            6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,   11,\n",
       "           11,   11,   11,   11,   11,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    6,    3,    3,    3,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    6,    6,    6,    6,    6,    6,    6,    6,    1,    1,    1,\n",
       "            1,    1,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    0,    0,    0,    0,    0,    0, -100],\n",
       "        [-100,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,    6,    6,    6, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([gNerDataset[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68b9cd",
   "metadata": {},
   "source": [
    "As for the other models that we have applied to *gNER*, we evaluate the performance of BERT via *macro-averaged $F_1$ scoring*. The `Trainer` API will expect a function that receives predictions and returns a dictionary where each key holds a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53b32a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.compute_metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    label_names = ner_tag_names\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels] # list of lists\n",
    "    true_predictions = [                                                             # list of lists\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    flat_true_labels = [label for true_labels_i in true_labels for label in true_labels_i]                   # list\n",
    "    flat_true_predictions = [pred for true_predictions_i in true_predictions for pred in true_predictions_i] # list\n",
    "    macroF1 = f1_score(flat_true_labels, flat_true_predictions, average=\"macro\")\n",
    "    return {\"F1 macro\": macroF1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c2ab8",
   "metadata": {},
   "source": [
    "### Zero-shot baseline\n",
    "While we have not trained the model, yet, we do already have everything ready to evaluate the model. So let's do that and establish a *zero-shot baseline* by evaluating the model on the validation set without it having ever seen an instance from the training set. [This](https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378) forum post clarifies how inference is performed when using the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "879da000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 26\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZERO-SHOT BASELINE\n",
      "macro averaged F_1 score:\t0.05318610762219017\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/v4.12.5/main_classes/trainer.html#trainingarguments\n",
    "output_dir = f\"gNER_{checkpoint}\"\n",
    "# using the model without further training\n",
    "# arguments for Trainer\n",
    "test_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    do_train=False,\n",
    "    do_predict=True,\n",
    "    dataloader_drop_last=False\n",
    ")\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=test_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_results = trainer.predict(gNerDataset[\"valid\"])\n",
    "print(f\"ZERO-SHOT BASELINE\\nmacro averaged F_1 score:\\t{test_results.metrics['test_F1 macro']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f12d8",
   "metadata": {},
   "source": [
    "This score is ***really*** low! But that's not too surprising as the model has never before seen any data from our dataset. So let's do some training on *gNER*!\n",
    "\n",
    "## Training\n",
    "We will use the `Trainer` API to train the model. That means that we will create `TrainingArguments` that specify certain parameters for training, define a `Trainer` instance, and run `Trainer.train()` to launch the training. The following resources have been helpful for getting the training running and loading the best model (based on validation loss) once training has completed:\n",
    "- Use a trained model for inference:\n",
    "  - https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378.\n",
    "  - https://discuss.huggingface.co/t/model-inference-on-tokenized-dataset/14820.\n",
    "- Save regularly and reload the best model when training is done:\n",
    "  - https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442.\n",
    "  - https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.load_best_model_at_end.\n",
    "- Hyperparameter search has not been implemented, here. However, the following links might provide some illumination on that topic:\n",
    "  - https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10.\n",
    "  - https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.hyperparameter_search.\n",
    "\n",
    "Instantiate specific `TrainingArguments`, a specific `Trainer`, and run `Trainer.train()`. While hyperparameter search was not performed - at least not in the fashion of [`RandomizedSearchCV`](https://scikit-learn.org/0.23/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV) or [`GridSearchCV`](https://scikit-learn.org/0.23/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) - at least a small number of hyperparameters have been tried out manually (see comments below `trainer.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c04b18de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/matthias/opt/anaconda3/envs/hf/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 78\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 03:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.943132</td>\n",
       "      <td>0.086022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to gNER_bert-base-cased/checkpoint-10\n",
      "Configuration saved in gNER_bert-base-cased/checkpoint-10/config.json\n",
      "Model weights saved in gNER_bert-base-cased/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in gNER_bert-base-cased/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in gNER_bert-base-cased/checkpoint-10/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from gNER_bert-base-cased/checkpoint-10 (score: 0.943131685256958).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.3088194847106933, metrics={'train_runtime': 252.3619, 'train_samples_per_second': 0.309, 'train_steps_per_second': 0.04, 'total_flos': 8669507776848.0, 'train_loss': 1.3088194847106933, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training arguments\n",
    "train_args = TrainingArguments(\n",
    "    output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, # loads checkpoint with lowest loss on validation set\n",
    "    learning_rate=5e-5,\n",
    "    # https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L356\n",
    "    # \"linear\" \"cosine\" \"cosine_with_restarts\" \"polynomial\", \"constant\", \"constant_with_warmup\"\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1, # 15\n",
    "    weight_decay=0.03,\n",
    "    push_to_hub=False\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=gNerDataset[\"train\"], # \"train\"\n",
    "    eval_dataset=gNerDataset[\"valid\"],  # \"valid\"\n",
    "    # https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.compute_metrics\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()\n",
    "# learning rate, weight decay, lr_scheduler_type,   valid F1  valid loss\n",
    "# 5e-5           0.001         linear               0.868     0.207\n",
    "# 5e-5           0.001         cosine               0.901     0.192\n",
    "# 5e-5           0.001         cosine_with_restarts 0.895     0.239\n",
    "# 5e-5           0.001         polynomial           0.885     0.212\n",
    "# 5e-5           0.001         constant             0.873     0.195\n",
    "# 5e-5           0.001         constant_with_warmup 0.875     0.193\n",
    "\n",
    "# 5e-5           0.005         cosine               0.885     0.208\n",
    "# 5e-5           0.01          cosine               0.890     0.208\n",
    "# 5e-5           0.03          cosine               0.894     0.189"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a232c",
   "metadata": {},
   "source": [
    "The model has been trained over several epochs. The best model version, namely the one with the lowest loss on the validation set, has been loaded from its checkpoint saved during training. Now, we can evaluate the trained model on the `\"test\"` split of our *gNER* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4447a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 26\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.08602175575639598"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378/7\n",
    "test_results = trainer.predict(gNerDataset[\"valid\"]) # use \"test\" instead of \"valid\" when done with development\n",
    "test_results.metrics[\"test_F1 macro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fc322",
   "metadata": {},
   "source": [
    "Not surprisingly, this is much better than the *zero-shot baseline* further above and roughly on par with the results for [Conditional Random Fields (*CRF*) models](https://stanford.app.box.com/s/5zeuqnkx739rtb103wpdxiedvr63mtrp/file/948454891762).\n",
    "$\\checkmark$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1481a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 199, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc1155e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 199)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_results.predictions, axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c938066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0,\n",
       "        0],\n",
       "       [6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0,\n",
       "        0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_results.predictions, axis=-1)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baa32526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 199)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.label_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822cc54",
   "metadata": {},
   "source": [
    "See `metrics.py`. => Do the following for predictions:\n",
    "- flatten\n",
    "- transform to text label via id2label dict (or the other way round)\n",
    "- remove the labels that ought to be ignored (-100 kind of thing)\n",
    "- produce matching labels (could be \"ner_tag_names\" or something else, maybe via \"id2label\")\n",
    "\n",
    "Do just the same for .label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f035ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
