{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "46bc54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from torch_model_base import TorchModelBase\n",
    "#from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNDataset, TorchRNNClassifier, TorchRNNModel\n",
    "import utils\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "df4daa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh torch rnn classifier:\n",
    "import importlib\n",
    "import torch_rnn_classifier\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_rnn_classifier import TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c519f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc5fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations2.jsonl') as jsonl_file:\n",
    "    # note: after running data-preprocessing.ipynb this file already has token-level labels\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fbb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get data into format that TorchRNN expects:\n",
    "X=[] \n",
    "y=[]\n",
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]['tokens']\n",
    "    auxX = []\n",
    "    auxy = []\n",
    "    if annot[j]['spans']!=[]: # are there annot for this example?\n",
    "        for i in range(0,len(a)):\n",
    "            #token_element = (a[i]['text'],a[i]['label'])\n",
    "            auxX.append(a[i]['text'])\n",
    "            auxy.append(a[i]['label'])\n",
    "        X.append(auxX)\n",
    "        y.append(auxy)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X[:120], X[120:], y[:120], y[120:]\n",
    "vocab = sorted({w for seq in X_train for w in seq}) + [\"$UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b8b5c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a33711db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSequenceLabeler(TorchRNNClassifier):\n",
    "\n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "        print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "        print(\"here02\")\n",
    "        model = TorchSequenceLabeler( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_)\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        # The base class does the heavy lifting:\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "        # Use `softmax`; the model doesn't do this because the loss\n",
    "        # function does it internally.\n",
    "        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X): #out: list of lists with text labels of predictions\n",
    "        probs = self.predict_proba(X)\n",
    "        return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        flat_preds = [x for seq in preds for x in seq]\n",
    "        flat_y = [x for seq in y for x in seq]\n",
    "        return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class TorchSequenceLabeler(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim):\n",
    "        print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # Out are logits - probs of each token for each class; logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        # this is the forward method of self.model\n",
    "        print(\"here2\")\n",
    "        print(type(X))\n",
    "        print(type(seq_lengths))\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "       # print(\"out1\")\n",
    "        print(outputs.data.shape)\n",
    "        #print(state[0].data.shape)\n",
    "        #print(state[1].data.shape)\n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "       # print(\"out2\")\n",
    "        print(outputs.data.shape)\n",
    "        #print(seq_length)\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "        # During training, we need to swap the dimensions of logits\n",
    "        # to accommodate `nn.CrossEntropyLoss`:\n",
    "        print(logits)\n",
    "        if self.training:\n",
    "            return logits.transpose(1, 2) # transpose dimensions 1 and 2 w/ each other (3d array) # outputs (108,12,117) or (1,5,11)\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4010d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"the wall street journal reported today that apple corporation made money\".split(),\"georgia tech is a university in georgia\".split()]\n",
    "y_train = [\"B I I I O O O B I O O\".split(),\"B I O O O O B\".split()]\n",
    "vocab = sorted({w for seq in X_train for w in seq}) + [\"$UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fe4012cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mod = TorchRNNSequenceLabeler(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "727e7b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "TorchSequenceLabeler(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "graph0 = seq_mod.build_graph()\n",
    "print(graph0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f8f210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 2 of 1000; error is 2.4679622650146484"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here00\n",
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1555,  0.1656,  0.1929,  ..., -0.0701,  0.0507,  0.0170],\n",
      "         [ 0.2475,  0.0425,  0.1348,  ..., -0.0693, -0.0505, -0.0182],\n",
      "         [ 0.2581,  0.2185,  0.0750,  ..., -0.0397,  0.0245,  0.0540],\n",
      "         ...,\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135]],\n",
      "\n",
      "        [[ 0.0744,  0.0264,  0.1459,  ..., -0.0602,  0.0489, -0.0761],\n",
      "         [ 0.0421, -0.1507,  0.1405,  ..., -0.0403,  0.0051, -0.0264],\n",
      "         [ 0.0792, -0.0927,  0.1270,  ..., -0.0051,  0.0446,  0.0004],\n",
      "         ...,\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135]],\n",
      "\n",
      "        [[ 0.1351,  0.1097,  0.1038,  ..., -0.1411,  0.0392, -0.0959],\n",
      "         [ 0.0715, -0.0845,  0.1497,  ..., -0.1020,  0.0245, -0.0186],\n",
      "         [ 0.0609, -0.0131,  0.1152,  ..., -0.1668,  0.0617, -0.1330],\n",
      "         ...,\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1117, -0.0300,  0.1277,  ..., -0.0950,  0.1382, -0.0812],\n",
      "         [ 0.0552, -0.0654,  0.0184,  ..., -0.1395,  0.0451, -0.1435],\n",
      "         [ 0.1783,  0.0348,  0.0043,  ..., -0.0413,  0.0376, -0.0245],\n",
      "         ...,\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135]],\n",
      "\n",
      "        [[ 0.1396,  0.0533,  0.1153,  ..., -0.0556,  0.1507,  0.0887],\n",
      "         [ 0.0257,  0.0881,  0.1748,  ..., -0.1068,  0.3355, -0.0180],\n",
      "         [ 0.0690, -0.0066,  0.1057,  ..., -0.0118,  0.4364, -0.0346],\n",
      "         ...,\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135]],\n",
      "\n",
      "        [[ 0.1117,  0.0129,  0.0506,  ..., -0.0335,  0.1907, -0.0075],\n",
      "         [-0.0032,  0.0541,  0.1450,  ..., -0.0961,  0.3380, -0.0651],\n",
      "         [ 0.0548, -0.0186,  0.1025,  ..., -0.0091,  0.4306, -0.0579],\n",
      "         ...,\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135],\n",
      "         [ 0.1307,  0.0677,  0.1158,  ..., -0.0671,  0.1205, -0.0135]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0704, -0.0476, -0.0070,  ..., -0.1339,  0.0374, -0.1262],\n",
      "         [ 0.1097, -0.0078, -0.0413,  ..., -0.1457, -0.0428, -0.1147],\n",
      "         [ 0.1752,  0.0553, -0.0070,  ..., -0.0604,  0.0324, -0.2501],\n",
      "         ...,\n",
      "         [ 0.0897,  0.0452,  0.1506,  ..., -0.0774,  0.2518,  0.0891],\n",
      "         [ 0.1975,  0.0053,  0.0541,  ...,  0.0397,  0.1757,  0.0935],\n",
      "         [ 0.1766,  0.0594,  0.0980,  ...,  0.0049,  0.0758,  0.0211]],\n",
      "\n",
      "        [[ 0.1852,  0.1532,  0.0839,  ..., -0.1149,  0.1022, -0.0645],\n",
      "         [ 0.0433,  0.0817,  0.1493,  ..., -0.1316,  0.2760, -0.0670],\n",
      "         [ 0.0489,  0.1323,  0.1084,  ..., -0.2199,  0.2550, -0.1124],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        [[-0.0557,  0.0542,  0.0135,  ..., -0.1332,  0.1378, -0.1512],\n",
      "         [ 0.0117, -0.1001,  0.1028,  ..., -0.0812,  0.0668, -0.0716],\n",
      "         [ 0.0290, -0.0355,  0.0793,  ..., -0.1615,  0.0770, -0.1510],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0568,  0.0165,  0.1094,  ..., -0.0508,  0.1452,  0.0296],\n",
      "         [ 0.1390,  0.0674,  0.2982,  ..., -0.0101,  0.2333,  0.0471],\n",
      "         [ 0.0897, -0.0052,  0.1909,  ..., -0.0976,  0.1684, -0.1238],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        [[ 0.0704, -0.0476, -0.0070,  ..., -0.1339,  0.0374, -0.1262],\n",
      "         [ 0.1274, -0.0957, -0.0811,  ..., -0.1227, -0.0295,  0.0041],\n",
      "         [ 0.1870, -0.0649, -0.0618,  ..., -0.0765,  0.1386, -0.0486],\n",
      "         ...,\n",
      "         [ 0.0825,  0.0624,  0.1211,  ..., -0.0976,  0.2369,  0.0766],\n",
      "         [ 0.1161,  0.0602,  0.0610,  ..., -0.0583,  0.1178, -0.0222],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        [[-0.0557,  0.0542,  0.0135,  ..., -0.1332,  0.1378, -0.1512],\n",
      "         [ 0.0117, -0.1001,  0.1028,  ..., -0.0812,  0.0668, -0.0716],\n",
      "         [ 0.0290, -0.0355,  0.0793,  ..., -0.1615,  0.0770, -0.1510],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1153,  0.0678,  0.0900,  ..., -0.0159,  0.1563,  0.0204],\n",
      "         [ 0.0811, -0.1028,  0.1202,  ..., -0.0033,  0.1178,  0.0682],\n",
      "         [ 0.0605, -0.0329,  0.1060,  ..., -0.1268,  0.0941, -0.1014],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        [[ 0.1153,  0.0678,  0.0900,  ..., -0.0159,  0.1563,  0.0204],\n",
      "         [ 0.0811, -0.1028,  0.1202,  ..., -0.0033,  0.1178,  0.0682],\n",
      "         [ 0.0605, -0.0329,  0.1060,  ..., -0.1268,  0.0941, -0.1014],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        [[ 0.0927, -0.0153,  0.0365,  ..., -0.1322,  0.1601,  0.0264],\n",
      "         [-0.0030,  0.0470,  0.1414,  ..., -0.1640,  0.3554, -0.0698],\n",
      "         [ 0.0377,  0.0307,  0.0682,  ..., -0.1230,  0.2779, -0.1088],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2124, -0.0584,  0.1572,  ..., -0.0595,  0.1564,  0.0485],\n",
      "         [ 0.1734,  0.0376,  0.2447,  ..., -0.0103,  0.2224,  0.0413],\n",
      "         [ 0.1989,  0.0639,  0.2021,  ..., -0.0248,  0.1254,  0.0209],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        [[ 0.1105, -0.0317,  0.1258,  ..., -0.0972,  0.1342, -0.0823],\n",
      "         [ 0.0519, -0.0681,  0.0123,  ..., -0.1417,  0.0397, -0.1442],\n",
      "         [ 0.1748,  0.0339, -0.0028,  ..., -0.0466,  0.0326, -0.0263],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]],\n",
      "\n",
      "        [[ 0.1452,  0.0831,  0.1099,  ..., -0.0214,  0.1868, -0.0116],\n",
      "         [ 0.0921, -0.0910,  0.1446,  ..., -0.0224,  0.0897,  0.0148],\n",
      "         [ 0.0592, -0.0221,  0.1167,  ..., -0.1338,  0.0959, -0.1236],\n",
      "         ...,\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145],\n",
      "         [ 0.1317,  0.0667,  0.1148,  ..., -0.0681,  0.1195, -0.0145]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0688, -0.0499, -0.0125,  ..., -0.1350,  0.0338, -0.1273],\n",
      "         [ 0.1056, -0.0081, -0.0486,  ..., -0.1499, -0.0449, -0.1187],\n",
      "         [ 0.1704,  0.0531, -0.0179,  ..., -0.0659,  0.0254, -0.2554],\n",
      "         ...,\n",
      "         [ 0.0913,  0.0462,  0.1491,  ..., -0.0767,  0.2489,  0.0913],\n",
      "         [ 0.1973,  0.0061,  0.0531,  ...,  0.0381,  0.1746,  0.0936],\n",
      "         [ 0.1601,  0.0580,  0.0880,  ..., -0.0097,  0.0696,  0.0153]],\n",
      "\n",
      "        [[ 0.1845,  0.1514,  0.0816,  ..., -0.1167,  0.1001, -0.0651],\n",
      "         [ 0.0409,  0.0782,  0.1451,  ..., -0.1346,  0.2724, -0.0718],\n",
      "         [ 0.0486,  0.1312,  0.1048,  ..., -0.2210,  0.2519, -0.1157],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        [[-0.0598,  0.0528,  0.0076,  ..., -0.1381,  0.1347, -0.1543],\n",
      "         [ 0.0133, -0.1002,  0.1029,  ..., -0.0821,  0.0666, -0.0712],\n",
      "         [ 0.0267, -0.0368,  0.0763,  ..., -0.1626,  0.0736, -0.1523],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0561,  0.0152,  0.1078,  ..., -0.0536,  0.1437,  0.0278],\n",
      "         [ 0.1384,  0.0655,  0.2969,  ..., -0.0116,  0.2315,  0.0452],\n",
      "         [ 0.0844, -0.0080,  0.1869,  ..., -0.1000,  0.1645, -0.1252],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        [[ 0.0688, -0.0499, -0.0125,  ..., -0.1350,  0.0338, -0.1273],\n",
      "         [ 0.1231, -0.0987, -0.0887,  ..., -0.1269, -0.0340,  0.0012],\n",
      "         [ 0.1850, -0.0672, -0.0671,  ..., -0.0797,  0.1344, -0.0509],\n",
      "         ...,\n",
      "         [ 0.0807,  0.0603,  0.1127,  ..., -0.1006,  0.2348,  0.0728],\n",
      "         [ 0.0963,  0.0530,  0.0463,  ..., -0.0738,  0.1097, -0.0269],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        [[-0.0598,  0.0528,  0.0076,  ..., -0.1381,  0.1347, -0.1543],\n",
      "         [ 0.0133, -0.1002,  0.1029,  ..., -0.0821,  0.0666, -0.0712],\n",
      "         [ 0.0267, -0.0368,  0.0763,  ..., -0.1626,  0.0736, -0.1523],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 4 of 1000; error is 2.4458308219909672"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.2124,  0.0701,  0.1721,  ...,  0.0012,  0.1832,  0.0602],\n",
      "         [ 0.0900, -0.1026,  0.1534,  ..., -0.0363,  0.0799,  0.0096],\n",
      "         [ 0.0618, -0.0185,  0.1198,  ..., -0.1365,  0.0970, -0.1287],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        [[ 0.0958,  0.0801,  0.0630,  ..., -0.0561,  0.0423, -0.0704],\n",
      "         [ 0.0240,  0.0920,  0.1473,  ..., -0.0960,  0.2591, -0.1204],\n",
      "         [-0.0210,  0.0704,  0.1762,  ..., -0.1205,  0.3610, -0.1504],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        [[ 0.1630,  0.0188,  0.0730,  ..., -0.0727,  0.1521,  0.0723],\n",
      "         [ 0.1067, -0.1362,  0.1534,  ..., -0.0490,  0.0512,  0.0856],\n",
      "         [ 0.0694, -0.0264,  0.1157,  ..., -0.1393,  0.0800, -0.1018],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1320, -0.0835,  0.0478,  ..., -0.0517,  0.0248,  0.1192],\n",
      "         [ 0.1494,  0.0684,  0.0371,  ..., -0.0035,  0.1087,  0.0746],\n",
      "         [ 0.0962,  0.0842,  0.1272,  ..., -0.1494,  0.2231,  0.0675],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        [[ 0.2436,  0.1007,  0.2425,  ..., -0.0477,  0.0566,  0.0472],\n",
      "         [ 0.2182,  0.1120,  0.1127,  ...,  0.0116,  0.1290,  0.0480],\n",
      "         [ 0.1574,  0.1885,  0.2731,  ..., -0.0009, -0.0574, -0.1393],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]],\n",
      "\n",
      "        [[ 0.1215,  0.0635,  0.1552,  ..., -0.0486,  0.0747,  0.0169],\n",
      "         [ 0.0755, -0.0989,  0.1689,  ..., -0.0483,  0.0606,  0.0006],\n",
      "         [ 0.0420, -0.0365,  0.1302,  ..., -0.1523,  0.0783, -0.1285],\n",
      "         ...,\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155],\n",
      "         [ 0.1327,  0.0657,  0.1138,  ..., -0.0691,  0.1185, -0.0155]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0672, -0.0520, -0.0179,  ..., -0.1362,  0.0304, -0.1286],\n",
      "         [ 0.1013, -0.0087, -0.0563,  ..., -0.1541, -0.0472, -0.1228],\n",
      "         [ 0.1659,  0.0509, -0.0289,  ..., -0.0713,  0.0183, -0.2608],\n",
      "         ...,\n",
      "         [ 0.0927,  0.0475,  0.1475,  ..., -0.0762,  0.2460,  0.0936],\n",
      "         [ 0.1968,  0.0068,  0.0522,  ...,  0.0365,  0.1738,  0.0936],\n",
      "         [ 0.1437,  0.0566,  0.0776,  ..., -0.0239,  0.0635,  0.0096]],\n",
      "\n",
      "        [[ 0.1841,  0.1497,  0.0797,  ..., -0.1182,  0.0982, -0.0658],\n",
      "         [ 0.0386,  0.0746,  0.1408,  ..., -0.1377,  0.2689, -0.0767],\n",
      "         [ 0.0484,  0.1300,  0.1013,  ..., -0.2223,  0.2487, -0.1188],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        [[-0.0638,  0.0514,  0.0016,  ..., -0.1430,  0.1317, -0.1575],\n",
      "         [ 0.0148, -0.1002,  0.1026,  ..., -0.0830,  0.0663, -0.0709],\n",
      "         [ 0.0243, -0.0382,  0.0734,  ..., -0.1635,  0.0704, -0.1539],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0556,  0.0138,  0.1063,  ..., -0.0562,  0.1422,  0.0261],\n",
      "         [ 0.1378,  0.0637,  0.2957,  ..., -0.0131,  0.2297,  0.0435],\n",
      "         [ 0.0793, -0.0107,  0.1833,  ..., -0.1022,  0.1608, -0.1267],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        [[ 0.0672, -0.0520, -0.0179,  ..., -0.1362,  0.0304, -0.1286],\n",
      "         [ 0.1190, -0.1016, -0.0962,  ..., -0.1308, -0.0383, -0.0020],\n",
      "         [ 0.1830, -0.0693, -0.0726,  ..., -0.0827,  0.1303, -0.0533],\n",
      "         ...,\n",
      "         [ 0.0783,  0.0583,  0.1041,  ..., -0.1038,  0.2330,  0.0689],\n",
      "         [ 0.0767,  0.0461,  0.0314,  ..., -0.0890,  0.1018, -0.0317],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        [[-0.0638,  0.0514,  0.0016,  ..., -0.1430,  0.1317, -0.1575],\n",
      "         [ 0.0148, -0.1002,  0.1026,  ..., -0.0830,  0.0663, -0.0709],\n",
      "         [ 0.0243, -0.0382,  0.0734,  ..., -0.1635,  0.0704, -0.1539],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1170,  0.0678,  0.0855,  ..., -0.0192,  0.1539,  0.0176],\n",
      "         [ 0.0897, -0.1019,  0.1241,  ..., -0.0011,  0.1203,  0.0692],\n",
      "         [ 0.0577, -0.0353,  0.1028,  ..., -0.1272,  0.0902, -0.1028],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        [[ 0.0814,  0.0656,  0.1187,  ..., -0.0881,  0.0618, -0.0560],\n",
      "         [ 0.1552,  0.1060,  0.0753,  ..., -0.0135,  0.1291,  0.0007],\n",
      "         [ 0.1246,  0.1220,  0.1077,  ..., -0.0739,  0.2243, -0.1321],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        [[ 0.0372,  0.0368,  0.1120,  ..., -0.1563,  0.2085,  0.0275],\n",
      "         [ 0.0812,  0.0390,  0.0662,  ..., -0.1193,  0.2750,  0.0551],\n",
      "         [ 0.1928,  0.0907,  0.1635,  ..., -0.0013,  0.2270,  0.0026],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1170,  0.0678,  0.0855,  ..., -0.0192,  0.1539,  0.0176],\n",
      "         [ 0.0897, -0.1019,  0.1241,  ..., -0.0011,  0.1203,  0.0692],\n",
      "         [ 0.0577, -0.0353,  0.1028,  ..., -0.1272,  0.0902, -0.1028],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        [[ 0.1170,  0.0678,  0.0855,  ..., -0.0192,  0.1539,  0.0176],\n",
      "         [ 0.0089,  0.0822,  0.1515,  ..., -0.0728,  0.3224, -0.0341],\n",
      "         [ 0.0088, -0.0134,  0.0926,  ..., -0.0185,  0.4168, -0.0463],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]],\n",
      "\n",
      "        [[ 0.1841,  0.1497,  0.0797,  ..., -0.1182,  0.0982, -0.0658],\n",
      "         [ 0.1102, -0.0760,  0.1293,  ..., -0.0802,  0.0544,  0.0083],\n",
      "         [ 0.0632, -0.0290,  0.0964,  ..., -0.1623,  0.0732, -0.1194],\n",
      "         ...,\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165],\n",
      "         [ 0.1337,  0.0647,  0.1128,  ..., -0.0701,  0.1175, -0.0165]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0657, -0.0542, -0.0233,  ..., -0.1374,  0.0270, -0.1301],\n",
      "         [ 0.0967, -0.0096, -0.0645,  ..., -0.1584, -0.0497, -0.1270],\n",
      "         [ 0.1615,  0.0487, -0.0400,  ..., -0.0767,  0.0110, -0.2662],\n",
      "         ...,\n",
      "         [ 0.0940,  0.0491,  0.1459,  ..., -0.0759,  0.2430,  0.0959],\n",
      "         [ 0.1961,  0.0076,  0.0511,  ...,  0.0347,  0.1729,  0.0935],\n",
      "         [ 0.1271,  0.0550,  0.0669,  ..., -0.0378,  0.0575,  0.0038]],\n",
      "\n",
      "        [[ 0.1841,  0.1478,  0.0779,  ..., -0.1195,  0.0964, -0.0666],\n",
      "         [ 0.0364,  0.0709,  0.1362,  ..., -0.1410,  0.2653, -0.0815],\n",
      "         [ 0.0478,  0.1286,  0.0976,  ..., -0.2238,  0.2452, -0.1218],\n",
      "         ...,\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175]],\n",
      "\n",
      "        [[-0.0678,  0.0498, -0.0045,  ..., -0.1479,  0.1288, -0.1607],\n",
      "         [ 0.0162, -0.1003,  0.1020,  ..., -0.0839,  0.0659, -0.0707],\n",
      "         [ 0.0219, -0.0398,  0.0705,  ..., -0.1643,  0.0673, -0.1558],\n",
      "         ...,\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0551,  0.0124,  0.1048,  ..., -0.0587,  0.1407,  0.0245],\n",
      "         [ 0.1372,  0.0620,  0.2946,  ..., -0.0146,  0.2280,  0.0420],\n",
      "         [ 0.0744, -0.0134,  0.1798,  ..., -0.1042,  0.1574, -0.1282],\n",
      "         ...,\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175]],\n",
      "\n",
      "        [[ 0.0657, -0.0542, -0.0233,  ..., -0.1374,  0.0270, -0.1301],\n",
      "         [ 0.1152, -0.1045, -0.1040,  ..., -0.1348, -0.0426, -0.0054],\n",
      "         [ 0.1811, -0.0712, -0.0783,  ..., -0.0857,  0.1261, -0.0559],\n",
      "         ...,\n",
      "         [ 0.0757,  0.0564,  0.0953,  ..., -0.1071,  0.2313,  0.0648],\n",
      "         [ 0.0570,  0.0393,  0.0160,  ..., -0.1040,  0.0939, -0.0366],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175]],\n",
      "\n",
      "        [[-0.0678,  0.0498, -0.0045,  ..., -0.1479,  0.1288, -0.1607],\n",
      "         [ 0.0162, -0.1003,  0.1020,  ..., -0.0839,  0.0659, -0.0707],\n",
      "         [ 0.0219, -0.0398,  0.0705,  ..., -0.1643,  0.0673, -0.1558],\n",
      "         ...,\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175],\n",
      "         [ 0.1347,  0.0637,  0.1118,  ..., -0.0711,  0.1165, -0.0175]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 2.2419e-01,  1.3990e-01,  9.8365e-02,  ..., -9.3350e-02,\n",
      "           2.4363e-02, -6.9360e-02],\n",
      "         [ 6.3735e-02,  1.0815e-01,  1.4791e-01,  ..., -1.1218e-01,\n",
      "           2.5739e-01, -9.5706e-02],\n",
      "         [ 1.0269e-01,  5.9250e-02,  4.1186e-02,  ..., -7.8775e-02,\n",
      "           5.9198e-02, -1.4889e-01],\n",
      "         ...,\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02]],\n",
      "\n",
      "        [[ 9.0070e-02, -2.5496e-02,  3.1322e-02,  ..., -1.4218e-01,\n",
      "           1.4647e-01,  1.5935e-02],\n",
      "         [-1.2064e-02,  3.5474e-02,  1.2825e-01,  ..., -1.7543e-01,\n",
      "           3.3904e-01, -8.9230e-02],\n",
      "         [ 2.6023e-02,  2.2451e-02,  5.6700e-02,  ..., -1.3113e-01,\n",
      "           2.6372e-01, -1.2437e-01],\n",
      "         ...,\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02]],\n",
      "\n",
      "        [[ 1.9425e-01,  1.9808e-03,  4.8629e-02,  ..., -6.4425e-02,\n",
      "           1.7513e-01,  2.3688e-02],\n",
      "         [ 1.1059e-01, -1.3707e-01,  1.3691e-01,  ..., -2.3996e-02,\n",
      "           1.2404e-01,  6.0721e-02],\n",
      "         [ 5.8859e-02, -4.8953e-02,  1.0799e-01,  ..., -1.3812e-01,\n",
      "           8.8065e-02, -1.1240e-01],\n",
      "         ...,\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1738e-01,  6.7630e-02,  8.3008e-02,  ..., -2.0892e-02,\n",
      "           1.5274e-01,  1.6281e-02],\n",
      "         [ 9.3392e-02, -1.0138e-01,  1.2541e-01,  ..., -1.9427e-04,\n",
      "           1.2129e-01,  6.9477e-02],\n",
      "         [ 5.6219e-02, -3.6671e-02,  1.0104e-01,  ..., -1.2729e-01,\n",
      "           8.8292e-02, -1.0395e-01],\n",
      "         ...,\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02]],\n",
      "\n",
      "        [[ 1.6104e-01,  7.4428e-02,  1.4068e-01,  ..., -3.5300e-02,\n",
      "           1.0671e-01,  2.4353e-02],\n",
      "         [ 1.7178e-01,  1.2988e-01,  8.6647e-02,  ...,  6.7099e-03,\n",
      "           1.2831e-01,  3.4221e-02],\n",
      "         [ 1.0713e-01,  1.2956e-01,  1.8635e-01,  ..., -6.0867e-02,\n",
      "           1.3098e-01, -5.3459e-02],\n",
      "         ...,\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02]],\n",
      "\n",
      "        [[ 1.4616e-01,  1.4278e-01,  1.1796e-01,  ..., -1.2540e-01,\n",
      "           1.1124e-01, -8.5028e-02],\n",
      "         [ 8.8864e-02, -7.9949e-02,  1.3855e-01,  ..., -8.8590e-02,\n",
      "           7.1579e-02, -5.0169e-03],\n",
      "         [ 4.5190e-02, -3.3748e-02,  9.1215e-02,  ..., -1.6725e-01,\n",
      "           7.3316e-02, -1.2490e-01],\n",
      "         ...,\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02],\n",
      "         [ 1.3472e-01,  6.3735e-02,  1.1181e-01,  ..., -7.1118e-02,\n",
      "           1.1646e-01, -1.7464e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 6 of 1000; error is 2.4237730503082275"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 6.4161e-02, -5.6284e-02, -2.8749e-02,  ..., -1.3859e-01,\n",
      "           2.3736e-02, -1.3168e-01],\n",
      "         [ 9.1779e-02, -1.0776e-02, -7.3187e-02,  ..., -1.6277e-01,\n",
      "          -5.2684e-02, -1.3148e-01],\n",
      "         [ 1.5702e-01,  4.6549e-02, -5.1470e-02,  ..., -8.2120e-02,\n",
      "           3.4676e-03, -2.7178e-01],\n",
      "         ...,\n",
      "         [ 9.5034e-02,  5.0870e-02,  1.4430e-01,  ..., -7.5742e-02,\n",
      "           2.4008e-01,  9.8319e-02],\n",
      "         [ 1.9529e-01,  8.2651e-03,  4.9915e-02,  ...,  3.2913e-02,\n",
      "           1.7206e-01,  9.3258e-02],\n",
      "         [ 1.1054e-01,  5.3191e-02,  5.5372e-02,  ..., -5.1415e-02,\n",
      "           5.1349e-02, -2.3098e-03]],\n",
      "\n",
      "        [[ 1.8423e-01,  1.4590e-01,  7.6264e-02,  ..., -1.2069e-01,\n",
      "           9.4881e-02, -6.7345e-02],\n",
      "         [ 3.4063e-02,  6.7193e-02,  1.3158e-01,  ..., -1.4428e-01,\n",
      "           2.6157e-01, -8.6309e-02],\n",
      "         [ 4.6893e-02,  1.2708e-01,  9.3888e-02,  ..., -2.2537e-01,\n",
      "           2.4153e-01, -1.2482e-01],\n",
      "         ...,\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02]],\n",
      "\n",
      "        [[-7.1883e-02,  4.7964e-02, -1.0850e-02,  ..., -1.5278e-01,\n",
      "           1.2599e-01, -1.6419e-01],\n",
      "         [ 1.7418e-02, -1.0053e-01,  1.0112e-01,  ..., -8.4712e-02,\n",
      "           6.5501e-02, -7.0641e-02],\n",
      "         [ 1.9489e-02, -4.1547e-02,  6.7372e-02,  ..., -1.6495e-01,\n",
      "           6.4110e-02, -1.5802e-01],\n",
      "         ...,\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.4789e-02,  1.1074e-02,  1.0329e-01,  ..., -6.0986e-02,\n",
      "           1.3924e-01,  2.2774e-02],\n",
      "         [ 1.3667e-01,  6.0428e-02,  2.9346e-01,  ..., -1.6015e-02,\n",
      "           2.2632e-01,  4.0743e-02],\n",
      "         [ 6.9728e-02, -1.6048e-02,  1.7632e-01,  ..., -1.0607e-01,\n",
      "           1.5416e-01, -1.2986e-01],\n",
      "         ...,\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02]],\n",
      "\n",
      "        [[ 6.4161e-02, -5.6284e-02, -2.8749e-02,  ..., -1.3859e-01,\n",
      "           2.3736e-02, -1.3168e-01],\n",
      "         [ 1.1140e-01, -1.0746e-01, -1.1215e-01,  ..., -1.3876e-01,\n",
      "          -4.7044e-02, -9.0463e-03],\n",
      "         [ 1.7903e-01, -7.3032e-02, -8.4256e-02,  ..., -8.8510e-02,\n",
      "           1.2182e-01, -5.8512e-02],\n",
      "         ...,\n",
      "         [ 7.2654e-02,  5.4680e-02,  8.6045e-02,  ..., -1.1051e-01,\n",
      "           2.2950e-01,  6.0647e-02],\n",
      "         [ 3.7149e-02,  3.2246e-02, -5.0910e-05,  ..., -1.1875e-01,\n",
      "           8.5994e-02, -4.1927e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02]],\n",
      "\n",
      "        [[-7.1883e-02,  4.7964e-02, -1.0850e-02,  ..., -1.5278e-01,\n",
      "           1.2599e-01, -1.6419e-01],\n",
      "         [ 1.7418e-02, -1.0053e-01,  1.0112e-01,  ..., -8.4712e-02,\n",
      "           6.5501e-02, -7.0641e-02],\n",
      "         [ 1.9489e-02, -4.1547e-02,  6.7372e-02,  ..., -1.6495e-01,\n",
      "           6.4110e-02, -1.5802e-01],\n",
      "         ...,\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02],\n",
      "         [ 1.3572e-01,  6.2735e-02,  1.1081e-01,  ..., -7.2117e-02,\n",
      "           1.1546e-01, -1.8464e-02]]], device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1238,  0.0609,  0.1518,  ..., -0.0489,  0.0704,  0.0076],\n",
      "         [ 0.0839, -0.0962,  0.1764,  ..., -0.0458,  0.0625, -0.0014],\n",
      "         [ 0.0378, -0.0390,  0.1254,  ..., -0.1519,  0.0710, -0.1350],\n",
      "         ...,\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185]],\n",
      "\n",
      "        [[ 0.1238,  0.0609,  0.1518,  ..., -0.0489,  0.0704,  0.0076],\n",
      "         [ 0.0186,  0.0562,  0.1863,  ..., -0.1139,  0.2792, -0.1080],\n",
      "         [ 0.0109, -0.0489,  0.0961,  ..., -0.0300,  0.4107, -0.0705],\n",
      "         ...,\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185]],\n",
      "\n",
      "        [[ 0.1427,  0.0749,  0.1213,  ..., -0.0489,  0.1268, -0.0040],\n",
      "         [ 0.1496,  0.0747,  0.1061,  ..., -0.0270,  0.1797,  0.0131],\n",
      "         [ 0.0814, -0.0616,  0.2087,  ..., -0.0487,  0.0998,  0.0263],\n",
      "         ...,\n",
      "         [ 0.1691,  0.1275,  0.1228,  ..., -0.0154,  0.0588, -0.0315],\n",
      "         [ 0.1034,  0.0979,  0.1017,  ..., -0.0959,  0.0398,  0.0042],\n",
      "         [ 0.0802,  0.0867,  0.0810,  ..., -0.1128, -0.0133, -0.0741]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2095, -0.0644,  0.1519,  ..., -0.0628,  0.1478,  0.0426],\n",
      "         [ 0.1726,  0.0253,  0.2408,  ..., -0.0112,  0.2195,  0.0353],\n",
      "         [ 0.2057,  0.0546,  0.1984,  ..., -0.0242,  0.1195,  0.0232],\n",
      "         ...,\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185]],\n",
      "\n",
      "        [[ 0.1842,  0.1459,  0.0763,  ..., -0.1207,  0.0949, -0.0673],\n",
      "         [ 0.1153, -0.0764,  0.1333,  ..., -0.0776,  0.0574,  0.0105],\n",
      "         [ 0.0599, -0.0318,  0.0930,  ..., -0.1621,  0.0689, -0.1218],\n",
      "         ...,\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185]],\n",
      "\n",
      "        [[ 0.0617, -0.0583,  0.0851,  ..., -0.0312,  0.1309,  0.0357],\n",
      "         [-0.0138, -0.0166,  0.1419,  ..., -0.0905,  0.2704, -0.0810],\n",
      "         [ 0.0029, -0.0712,  0.0805,  ...,  0.0438,  0.2556, -0.0718],\n",
      "         ...,\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185],\n",
      "         [ 0.1357,  0.0627,  0.1108,  ..., -0.0721,  0.1155, -0.0185]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0625, -0.0584, -0.0341,  ..., -0.1398,  0.0204, -0.1334],\n",
      "         [ 0.0865, -0.0124, -0.0823,  ..., -0.1673, -0.0560, -0.1362],\n",
      "         [ 0.1524,  0.0442, -0.0632,  ..., -0.0875, -0.0043, -0.2776],\n",
      "         ...,\n",
      "         [ 0.0960,  0.0528,  0.1428,  ..., -0.0758,  0.2371,  0.1007],\n",
      "         [ 0.1943,  0.0088,  0.0485,  ...,  0.0310,  0.1711,  0.0928],\n",
      "         [ 0.0938,  0.0509,  0.0430,  ..., -0.0647,  0.0451, -0.0087]],\n",
      "\n",
      "        [[ 0.1844,  0.1439,  0.0747,  ..., -0.1218,  0.0935, -0.0681],\n",
      "         [ 0.0318,  0.0634,  0.1267,  ..., -0.1475,  0.2578, -0.0912],\n",
      "         [ 0.0457,  0.1254,  0.0900,  ..., -0.2270,  0.2377, -0.1279],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        [[-0.0760,  0.0458, -0.0175,  ..., -0.1575,  0.1233, -0.1679],\n",
      "         [ 0.0186, -0.1008,  0.1000,  ..., -0.0855,  0.0650, -0.0708],\n",
      "         [ 0.0171, -0.0436,  0.0640,  ..., -0.1656,  0.0608, -0.1607],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0546,  0.0097,  0.1018,  ..., -0.0632,  0.1379,  0.0210],\n",
      "         [ 0.1363,  0.0590,  0.2924,  ..., -0.0174,  0.2248,  0.0396],\n",
      "         [ 0.0652, -0.0188,  0.1728,  ..., -0.1077,  0.1510, -0.1318],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        [[ 0.0625, -0.0584, -0.0341,  ..., -0.1398,  0.0204, -0.1334],\n",
      "         [ 0.1076, -0.1105, -0.1208,  ..., -0.1428, -0.0517, -0.0129],\n",
      "         [ 0.1768, -0.0748, -0.0905,  ..., -0.0913,  0.1175, -0.0613],\n",
      "         ...,\n",
      "         [ 0.0694,  0.0530,  0.0764,  ..., -0.1139,  0.2276,  0.0563],\n",
      "         [ 0.0170,  0.0249, -0.0171,  ..., -0.1332,  0.0779, -0.0476],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        [[-0.0760,  0.0458, -0.0175,  ..., -0.1575,  0.1233, -0.1679],\n",
      "         [ 0.0186, -0.1008,  0.1000,  ..., -0.0855,  0.0650, -0.0708],\n",
      "         [ 0.0171, -0.0436,  0.0640,  ..., -0.1656,  0.0608, -0.1607],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1263, -0.0904,  0.0304,  ..., -0.0672,  0.0159,  0.1101],\n",
      "         [ 0.1547,  0.0594,  0.0293,  ..., -0.0136,  0.0968,  0.0651],\n",
      "         [ 0.0955,  0.0747,  0.1118,  ..., -0.1606,  0.2063,  0.0537],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        [[ 0.1403,  0.0848,  0.0864,  ..., -0.0838,  0.1190, -0.0915],\n",
      "         [ 0.0943, -0.1044,  0.1614,  ..., -0.0712,  0.0749,  0.0408],\n",
      "         [ 0.0457, -0.0348,  0.1037,  ..., -0.1530,  0.0782, -0.1228],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        [[ 0.1428,  0.0738,  0.1181,  ..., -0.0502,  0.1243, -0.0051],\n",
      "         [ 0.1507,  0.0742,  0.1048,  ..., -0.0268,  0.1782,  0.0130],\n",
      "         [ 0.0807, -0.0612,  0.2082,  ..., -0.0507,  0.0981,  0.0241],\n",
      "         ...,\n",
      "         [ 0.1659,  0.1276,  0.1182,  ..., -0.0181,  0.0567, -0.0316],\n",
      "         [ 0.1008,  0.0950,  0.0951,  ..., -0.0999,  0.0367,  0.0016],\n",
      "         [ 0.0605,  0.0809,  0.0649,  ..., -0.1279, -0.0227, -0.0805]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1196, -0.0095,  0.0152,  ..., -0.0758,  0.1422,  0.0278],\n",
      "         [ 0.0146,  0.0135,  0.0841,  ..., -0.1126,  0.2763, -0.0823],\n",
      "         [ 0.0160, -0.0665,  0.0385,  ..., -0.0311,  0.3872, -0.0718],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        [[ 0.1844,  0.1439,  0.0747,  ..., -0.1218,  0.0935, -0.0681],\n",
      "         [ 0.0318,  0.0634,  0.1267,  ..., -0.1475,  0.2578, -0.0912],\n",
      "         [ 0.0286, -0.0422,  0.0467,  ..., -0.0407,  0.3936, -0.0844],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]],\n",
      "\n",
      "        [[ 0.1934, -0.0107,  0.1879,  ..., -0.0956,  0.0963,  0.0958],\n",
      "         [ 0.0353,  0.0532,  0.1712,  ..., -0.1445,  0.3090, -0.0633],\n",
      "         [-0.0390,  0.0071,  0.0624,  ..., -0.1390,  0.2125, -0.2146],\n",
      "         ...,\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195],\n",
      "         [ 0.1367,  0.0617,  0.1098,  ..., -0.0731,  0.1145, -0.0195]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0607, -0.0607, -0.0396,  ..., -0.1411,  0.0171, -0.1352],\n",
      "         [ 0.0808, -0.0146, -0.0921,  ..., -0.1720, -0.0599, -0.1413],\n",
      "         [ 0.1475,  0.0415, -0.0753,  ..., -0.0928, -0.0125, -0.2839],\n",
      "         ...,\n",
      "         [ 0.0968,  0.0547,  0.1415,  ..., -0.0760,  0.2340,  0.1031],\n",
      "         [ 0.1931,  0.0093,  0.0470,  ...,  0.0289,  0.1700,  0.0923],\n",
      "         [ 0.0769,  0.0481,  0.0296,  ..., -0.0776,  0.0386, -0.0154]],\n",
      "\n",
      "        [[ 0.1845,  0.1418,  0.0731,  ..., -0.1228,  0.0924, -0.0688],\n",
      "         [ 0.0294,  0.0596,  0.1217,  ..., -0.1506,  0.2539, -0.0961],\n",
      "         [ 0.0443,  0.1234,  0.0858,  ..., -0.2286,  0.2338, -0.1311],\n",
      "         ...,\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205]],\n",
      "\n",
      "        [[-0.0803,  0.0432, -0.0245,  ..., -0.1621,  0.1206, -0.1718],\n",
      "         [ 0.0198, -0.1011,  0.0987,  ..., -0.0863,  0.0643, -0.0710],\n",
      "         [ 0.0146, -0.0460,  0.0604,  ..., -0.1661,  0.0573, -0.1638],\n",
      "         ...,\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0544,  0.0082,  0.1004,  ..., -0.0653,  0.1366,  0.0193],\n",
      "         [ 0.1361,  0.0578,  0.2914,  ..., -0.0187,  0.2233,  0.0385],\n",
      "         [ 0.0607, -0.0216,  0.1692,  ..., -0.1092,  0.1478, -0.1341],\n",
      "         ...,\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205]],\n",
      "\n",
      "        [[ 0.0607, -0.0607, -0.0396,  ..., -0.1411,  0.0171, -0.1352],\n",
      "         [ 0.1036, -0.1136, -0.1299,  ..., -0.1470, -0.0565, -0.0169],\n",
      "         [ 0.1745, -0.0766, -0.0971,  ..., -0.0940,  0.1130, -0.0642],\n",
      "         ...,\n",
      "         [ 0.0658,  0.0512,  0.0662,  ..., -0.1175,  0.2255,  0.0518],\n",
      "         [-0.0034,  0.0171, -0.0353,  ..., -0.1474,  0.0696, -0.0537],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205]],\n",
      "\n",
      "        [[-0.0803,  0.0432, -0.0245,  ..., -0.1621,  0.1206, -0.1718],\n",
      "         [ 0.0198, -0.1011,  0.0987,  ..., -0.0863,  0.0643, -0.0710],\n",
      "         [ 0.0146, -0.0460,  0.0604,  ..., -0.1661,  0.0573, -0.1638],\n",
      "         ...,\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205],\n",
      "         [ 0.1377,  0.0607,  0.1088,  ..., -0.0741,  0.1135, -0.0205]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 9 of 1000; error is 2.3894722461700445"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 1.8452e-01,  1.4182e-01,  7.3143e-02,  ..., -1.2275e-01,\n",
      "           9.2368e-02, -6.8833e-02],\n",
      "         [ 2.9445e-02,  5.9562e-02,  1.2166e-01,  ..., -1.5064e-01,\n",
      "           2.5387e-01, -9.6135e-02],\n",
      "         [ 5.7529e-02,  6.8543e-02,  9.6782e-02,  ..., -1.3153e-01,\n",
      "           9.9517e-02, -4.0871e-02],\n",
      "         ...,\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02]],\n",
      "\n",
      "        [[ 1.2868e-01,  1.0296e-01,  7.6515e-02,  ..., -1.6763e-01,\n",
      "           1.7179e-02, -1.1325e-01],\n",
      "         [ 3.7162e-02,  6.9748e-02,  1.0870e-01,  ..., -1.6555e-01,\n",
      "           2.2432e-01, -1.6905e-01],\n",
      "         [ 2.7673e-04, -4.1514e-02,  3.9254e-02,  ..., -7.0837e-02,\n",
      "           3.5578e-01, -1.2697e-01],\n",
      "         ...,\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02]],\n",
      "\n",
      "        [[ 2.0825e-01,  6.5243e-02,  1.7673e-01,  ..., -7.7640e-03,\n",
      "           1.8654e-01,  6.5264e-02],\n",
      "         [ 5.7745e-02,  6.7526e-02,  1.9362e-01,  ..., -9.6893e-02,\n",
      "           3.3119e-01, -6.9948e-02],\n",
      "         [ 7.2268e-02,  2.8398e-02,  1.4364e-01,  ...,  1.1110e-02,\n",
      "           2.0723e-01, -1.2872e-01],\n",
      "         ...,\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2488e-01,  5.9188e-02,  1.4931e-01,  ..., -4.9585e-02,\n",
      "           6.6741e-02,  1.0734e-03],\n",
      "         [ 8.8891e-02, -9.4262e-02,  1.7996e-01,  ..., -4.4641e-02,\n",
      "           6.2908e-02, -4.2359e-03],\n",
      "         [ 3.4980e-02, -4.1780e-02,  1.2111e-01,  ..., -1.5144e-01,\n",
      "           6.5286e-02, -1.4154e-01],\n",
      "         ...,\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02]],\n",
      "\n",
      "        [[ 1.2488e-01,  5.9188e-02,  1.4931e-01,  ..., -4.9585e-02,\n",
      "           6.6741e-02,  1.0734e-03],\n",
      "         [ 8.8891e-02, -9.4262e-02,  1.7996e-01,  ..., -4.4641e-02,\n",
      "           6.2908e-02, -4.2359e-03],\n",
      "         [ 3.4980e-02, -4.1780e-02,  1.2111e-01,  ..., -1.5144e-01,\n",
      "           6.5286e-02, -1.4154e-01],\n",
      "         ...,\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02]],\n",
      "\n",
      "        [[ 5.4415e-02,  8.2483e-03,  1.0041e-01,  ..., -6.5309e-02,\n",
      "           1.3656e-01,  1.9279e-02],\n",
      "         [-2.0245e-02,  2.3637e-02,  1.4601e-01,  ..., -1.0242e-01,\n",
      "           2.8568e-01, -7.3742e-02],\n",
      "         [-1.8230e-02, -5.2195e-02,  6.2695e-02,  ..., -3.7833e-02,\n",
      "           3.9141e-01, -8.2249e-02],\n",
      "         ...,\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02],\n",
      "         [ 1.3772e-01,  6.0736e-02,  1.0881e-01,  ..., -7.4115e-02,\n",
      "           1.1346e-01, -2.0464e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0587, -0.0631, -0.0452,  ..., -0.1424,  0.0136, -0.1373],\n",
      "         [ 0.0746, -0.0174, -0.1024,  ..., -0.1769, -0.0643, -0.1468],\n",
      "         [ 0.1422,  0.0385, -0.0879,  ..., -0.0980, -0.0210, -0.2906],\n",
      "         ...,\n",
      "         [ 0.0975,  0.0566,  0.1402,  ..., -0.0764,  0.2310,  0.1054],\n",
      "         [ 0.1918,  0.0096,  0.0454,  ...,  0.0268,  0.1687,  0.0917],\n",
      "         [ 0.0596,  0.0446,  0.0151,  ..., -0.0904,  0.0319, -0.0225]],\n",
      "\n",
      "        [[ 0.1846,  0.1397,  0.0716,  ..., -0.1237,  0.0914, -0.0695],\n",
      "         [ 0.0271,  0.0556,  0.1164,  ..., -0.1537,  0.2499, -0.1012],\n",
      "         [ 0.0428,  0.1212,  0.0814,  ..., -0.2303,  0.2296, -0.1345],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        [[-0.0848,  0.0402, -0.0318,  ..., -0.1665,  0.1179, -0.1760],\n",
      "         [ 0.0210, -0.1016,  0.0972,  ..., -0.0872,  0.0635, -0.0714],\n",
      "         [ 0.0120, -0.0488,  0.0564,  ..., -0.1666,  0.0534, -0.1673],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0543,  0.0068,  0.0989,  ..., -0.0674,  0.1353,  0.0175],\n",
      "         [ 0.1361,  0.0566,  0.2904,  ..., -0.0199,  0.2220,  0.0375],\n",
      "         [ 0.0563, -0.0247,  0.1654,  ..., -0.1104,  0.1445, -0.1368],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        [[ 0.0587, -0.0631, -0.0452,  ..., -0.1424,  0.0136, -0.1373],\n",
      "         [ 0.0994, -0.1171, -0.1395,  ..., -0.1511, -0.0616, -0.0212],\n",
      "         [ 0.1719, -0.0785, -0.1042,  ..., -0.0966,  0.1083, -0.0673],\n",
      "         ...,\n",
      "         [ 0.0619,  0.0493,  0.0556,  ..., -0.1212,  0.2232,  0.0471],\n",
      "         [-0.0244,  0.0085, -0.0549,  ..., -0.1616,  0.0609, -0.0605],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        [[-0.0848,  0.0402, -0.0318,  ..., -0.1665,  0.1179, -0.1760],\n",
      "         [ 0.0210, -0.1016,  0.0972,  ..., -0.0872,  0.0635, -0.0714],\n",
      "         [ 0.0120, -0.0488,  0.0564,  ..., -0.1666,  0.0534, -0.1673],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1840, -0.0113,  0.0333,  ..., -0.0774,  0.1630,  0.0143],\n",
      "         [ 0.1152, -0.1413,  0.1375,  ..., -0.0271,  0.1213,  0.0585],\n",
      "         [ 0.0488, -0.0579,  0.0974,  ..., -0.1397,  0.0764, -0.1221],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        [[ 0.1381,  0.1264,  0.0934,  ..., -0.1448,  0.0899, -0.1023],\n",
      "         [ 0.0928, -0.0858,  0.1352,  ..., -0.0911,  0.0683, -0.0078],\n",
      "         [ 0.0347, -0.0439,  0.0778,  ..., -0.1698,  0.0587, -0.1369],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        [[ 0.1381,  0.0810,  0.0851,  ..., -0.0883,  0.1150, -0.0908],\n",
      "         [ 0.0985, -0.1051,  0.1645,  ..., -0.0716,  0.0735,  0.0420],\n",
      "         [ 0.0411, -0.0398,  0.0992,  ..., -0.1534,  0.0722, -0.1280],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1428,  0.0715,  0.1113,  ..., -0.0529,  0.1194, -0.0076],\n",
      "         [ 0.1522,  0.0734,  0.1019,  ..., -0.0267,  0.1753,  0.0122],\n",
      "         [ 0.0789, -0.0597,  0.2071,  ..., -0.0549,  0.0951,  0.0198],\n",
      "         ...,\n",
      "         [ 0.1590,  0.1276,  0.1082,  ..., -0.0232,  0.0526, -0.0320],\n",
      "         [ 0.0950,  0.0887,  0.0812,  ..., -0.1078,  0.0304, -0.0037],\n",
      "         [ 0.0198,  0.0675,  0.0300,  ..., -0.1574, -0.0428, -0.0945]],\n",
      "\n",
      "        [[ 0.0624,  0.1053,  0.1553,  ..., -0.1293,  0.1813,  0.0021],\n",
      "         [ 0.0020,  0.1054,  0.1677,  ..., -0.1499,  0.3323, -0.0433],\n",
      "         [-0.0124, -0.0367,  0.0654,  ..., -0.0644,  0.3940, -0.0805],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]],\n",
      "\n",
      "        [[ 0.1381,  0.1264,  0.0934,  ..., -0.1448,  0.0899, -0.1023],\n",
      "         [ 0.0159,  0.0481,  0.1135,  ..., -0.1705,  0.2499, -0.1545],\n",
      "         [-0.0082, -0.0633,  0.0235,  ..., -0.0545,  0.3800, -0.1121],\n",
      "         ...,\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215],\n",
      "         [ 0.1387,  0.0597,  0.1078,  ..., -0.0751,  0.1125, -0.0215]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0565, -0.0658, -0.0510,  ..., -0.1437,  0.0099, -0.1394],\n",
      "         [ 0.0679, -0.0208, -0.1135,  ..., -0.1820, -0.0693, -0.1527],\n",
      "         [ 0.1365,  0.0350, -0.1010,  ..., -0.1032, -0.0301, -0.2979],\n",
      "         ...,\n",
      "         [ 0.0981,  0.0585,  0.1390,  ..., -0.0770,  0.2279,  0.1076],\n",
      "         [ 0.1905,  0.0099,  0.0435,  ...,  0.0245,  0.1673,  0.0910],\n",
      "         [ 0.0420,  0.0402, -0.0007,  ..., -0.1029,  0.0247, -0.0300]],\n",
      "\n",
      "        [[ 0.1846,  0.1376,  0.0701,  ..., -0.1246,  0.0905, -0.0702],\n",
      "         [ 0.0247,  0.0515,  0.1110,  ..., -0.1567,  0.2457, -0.1065],\n",
      "         [ 0.0409,  0.1188,  0.0768,  ..., -0.2321,  0.2252, -0.1383],\n",
      "         ...,\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225]],\n",
      "\n",
      "        [[-0.0895,  0.0368, -0.0396,  ..., -0.1710,  0.1151, -0.1806],\n",
      "         [ 0.0222, -0.1021,  0.0954,  ..., -0.0881,  0.0625, -0.0719],\n",
      "         [ 0.0092, -0.0521,  0.0521,  ..., -0.1671,  0.0493, -0.1713],\n",
      "         ...,\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0543,  0.0052,  0.0973,  ..., -0.0694,  0.1340,  0.0156],\n",
      "         [ 0.1362,  0.0556,  0.2895,  ..., -0.0211,  0.2207,  0.0364],\n",
      "         [ 0.0519, -0.0279,  0.1614,  ..., -0.1115,  0.1411, -0.1399],\n",
      "         ...,\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225]],\n",
      "\n",
      "        [[ 0.0565, -0.0658, -0.0510,  ..., -0.1437,  0.0099, -0.1394],\n",
      "         [ 0.0949, -0.1208, -0.1497,  ..., -0.1554, -0.0670, -0.0257],\n",
      "         [ 0.1692, -0.0807, -0.1117,  ..., -0.0992,  0.1034, -0.0705],\n",
      "         ...,\n",
      "         [ 0.0577,  0.0471,  0.0444,  ..., -0.1250,  0.2206,  0.0421],\n",
      "         [-0.0459, -0.0009, -0.0759,  ..., -0.1757,  0.0515, -0.0679],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225]],\n",
      "\n",
      "        [[-0.0895,  0.0368, -0.0396,  ..., -0.1710,  0.1151, -0.1806],\n",
      "         [ 0.0222, -0.1021,  0.0954,  ..., -0.0881,  0.0625, -0.0719],\n",
      "         [ 0.0092, -0.0521,  0.0521,  ..., -0.1671,  0.0493, -0.1713],\n",
      "         ...,\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225],\n",
      "         [ 0.1397,  0.0587,  0.1068,  ..., -0.0761,  0.1115, -0.0225]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 5.1268e-02,  3.0893e-02,  1.4649e-01,  ..., -1.2078e-01,\n",
      "           6.5448e-02, -1.1671e-01],\n",
      "         [-2.2309e-02,  4.1353e-02,  1.4970e-01,  ..., -1.2369e-01,\n",
      "           2.5725e-01, -1.7352e-01],\n",
      "         [-5.6226e-02, -6.6457e-02,  4.9578e-02,  ..., -5.2211e-02,\n",
      "           3.7222e-01, -1.2192e-01],\n",
      "         ...,\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02]],\n",
      "\n",
      "        [[ 1.6888e-01,  7.8443e-02,  7.6376e-02,  ..., -8.7440e-04,\n",
      "           6.7589e-02,  3.9074e-02],\n",
      "         [ 1.6120e-01,  6.6473e-02,  4.6133e-02,  ..., -7.3400e-02,\n",
      "           1.8770e-01,  4.0680e-02],\n",
      "         [ 1.8918e-01,  2.3610e-01,  2.2874e-02,  ..., -2.3390e-02,\n",
      "           3.4900e-02, -8.2938e-02],\n",
      "         ...,\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02]],\n",
      "\n",
      "        [[ 5.9709e-02,  1.0400e-01,  1.5088e-01,  ..., -1.3176e-01,\n",
      "           1.7821e-01, -1.0306e-03],\n",
      "         [-1.9531e-03,  1.0253e-01,  1.6142e-01,  ..., -1.5341e-01,\n",
      "           3.2776e-01, -4.9609e-02],\n",
      "         [-2.4527e-02, -4.1468e-02,  5.5034e-02,  ..., -7.0480e-02,\n",
      "           3.8615e-01, -8.7883e-02],\n",
      "         ...,\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6336e-01,  5.9379e-02,  1.0381e-01,  ...,  1.9368e-02,\n",
      "           1.1354e-01,  1.3062e-02],\n",
      "         [ 1.9033e-01,  1.4151e-01,  7.2321e-02,  ...,  2.1563e-02,\n",
      "           1.4384e-01,  4.5183e-02],\n",
      "         [ 1.4962e-01, -1.5083e-02, -7.6579e-02,  ...,  5.4926e-05,\n",
      "           9.4489e-02,  7.8499e-02],\n",
      "         ...,\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02]],\n",
      "\n",
      "        [[ 1.1291e-01, -3.1496e-02, -7.9045e-02,  ..., -1.3863e-01,\n",
      "           3.3697e-03,  4.5154e-02],\n",
      "         [ 8.7348e-02, -1.4290e-01,  6.0741e-02,  ..., -6.2385e-02,\n",
      "           2.0624e-02,  4.4491e-02],\n",
      "         [ 1.7365e-01, -5.1219e-02, -7.8557e-02,  ..., -1.1252e-01,\n",
      "          -1.6834e-01,  3.6448e-02],\n",
      "         ...,\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02]],\n",
      "\n",
      "        [[ 8.6857e-02,  7.2610e-03,  1.3680e-01,  ..., -7.3487e-02,\n",
      "           3.1804e-02, -8.6007e-02],\n",
      "         [ 7.0502e-02, -1.5696e-01,  1.5841e-01,  ..., -3.6470e-02,\n",
      "           1.5831e-03, -2.7419e-02],\n",
      "         [ 1.1427e-01, -8.8152e-02,  1.3253e-01,  ..., -4.0401e-04,\n",
      "           3.8485e-02, -4.4542e-03],\n",
      "         ...,\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02],\n",
      "         [ 1.3972e-01,  5.8737e-02,  1.0681e-01,  ..., -7.6113e-02,\n",
      "           1.1146e-01, -2.2464e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 11 of 1000; error is 2.364762306213379"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0540, -0.0688, -0.0571,  ..., -0.1451,  0.0059, -0.1418],\n",
      "         [ 0.0607, -0.0248, -0.1253,  ..., -0.1874, -0.0750, -0.1591],\n",
      "         [ 0.1303,  0.0311, -0.1148,  ..., -0.1085, -0.0398, -0.3057],\n",
      "         ...,\n",
      "         [ 0.0986,  0.0603,  0.1378,  ..., -0.0777,  0.2248,  0.1097],\n",
      "         [ 0.1890,  0.0100,  0.0415,  ...,  0.0223,  0.1658,  0.0902],\n",
      "         [ 0.0238,  0.0349, -0.0178,  ..., -0.1155,  0.0170, -0.0380]],\n",
      "\n",
      "        [[ 0.1846,  0.1355,  0.0686,  ..., -0.1256,  0.0896, -0.0708],\n",
      "         [ 0.0221,  0.0472,  0.1054,  ..., -0.1597,  0.2414, -0.1118],\n",
      "         [ 0.0389,  0.1162,  0.0718,  ..., -0.2339,  0.2206, -0.1424],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        [[-0.0944,  0.0329, -0.0477,  ..., -0.1755,  0.1120, -0.1854],\n",
      "         [ 0.0235, -0.1026,  0.0935,  ..., -0.0890,  0.0614, -0.0726],\n",
      "         [ 0.0062, -0.0558,  0.0475,  ..., -0.1676,  0.0448, -0.1758],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0543,  0.0036,  0.0956,  ..., -0.0715,  0.1326,  0.0137],\n",
      "         [ 0.1366,  0.0546,  0.2885,  ..., -0.0221,  0.2194,  0.0354],\n",
      "         [ 0.0474, -0.0314,  0.1573,  ..., -0.1124,  0.1375, -0.1433],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        [[ 0.0540, -0.0688, -0.0571,  ..., -0.1451,  0.0059, -0.1418],\n",
      "         [ 0.0901, -0.1250, -0.1604,  ..., -0.1597, -0.0728, -0.0305],\n",
      "         [ 0.1662, -0.0832, -0.1199,  ..., -0.1018,  0.0983, -0.0739],\n",
      "         ...,\n",
      "         [ 0.0529,  0.0446,  0.0326,  ..., -0.1291,  0.2176,  0.0369],\n",
      "         [-0.0681, -0.0113, -0.0985,  ..., -0.1901,  0.0414, -0.0761],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        [[-0.0944,  0.0329, -0.0477,  ..., -0.1755,  0.1120, -0.1854],\n",
      "         [ 0.0235, -0.1026,  0.0935,  ..., -0.0890,  0.0614, -0.0726],\n",
      "         [ 0.0062, -0.0558,  0.0475,  ..., -0.1676,  0.0448, -0.1758],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1267,  0.0565,  0.1457,  ..., -0.0509,  0.0605, -0.0093],\n",
      "         [ 0.0049,  0.0385,  0.1585,  ..., -0.1275,  0.2540, -0.1417],\n",
      "         [-0.0435, -0.0717,  0.0476,  ..., -0.0576,  0.3715, -0.1091],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        [[ 0.1169,  0.0634,  0.0658,  ..., -0.0315,  0.1433,  0.0061],\n",
      "         [ 0.1115, -0.0979,  0.1296,  ...,  0.0009,  0.1227,  0.0658],\n",
      "         [ 0.0451, -0.0500,  0.0856,  ..., -0.1277,  0.0727, -0.1194],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        [[ 0.1169,  0.0634,  0.0658,  ..., -0.0315,  0.1433,  0.0061],\n",
      "         [-0.0099,  0.0632,  0.1116,  ..., -0.0939,  0.2939, -0.0719],\n",
      "         [ 0.0193,  0.0733,  0.0524,  ..., -0.1299,  0.2203, -0.0228],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1228,  0.0014,  0.0352,  ..., -0.0655,  0.2049,  0.0278],\n",
      "         [ 0.1135, -0.0245,  0.0291,  ...,  0.0145,  0.0729, -0.0945],\n",
      "         [ 0.0749,  0.0922,  0.0664,  ..., -0.1015, -0.0127, -0.0769],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        [[ 0.2263,  0.1386,  0.0776,  ..., -0.0970,  0.0203, -0.0673],\n",
      "         [ 0.1381, -0.0880,  0.1406,  ..., -0.0440,  0.0643,  0.0338],\n",
      "         [ 0.0502, -0.0503,  0.0919,  ..., -0.1544,  0.0439, -0.1352],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]],\n",
      "\n",
      "        [[ 0.2071,  0.0616,  0.1797,  ..., -0.0123,  0.1890,  0.0694],\n",
      "         [ 0.0507,  0.0578,  0.1823,  ..., -0.1044,  0.3225, -0.0811],\n",
      "         [ 0.0628,  0.0139,  0.1283,  ...,  0.0056,  0.1935, -0.1404],\n",
      "         ...,\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235],\n",
      "         [ 0.1407,  0.0577,  0.1058,  ..., -0.0771,  0.1105, -0.0235]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0512, -0.0721, -0.0635,  ..., -0.1466,  0.0016, -0.1444],\n",
      "         [ 0.0529, -0.0297, -0.1380,  ..., -0.1931, -0.0814, -0.1661],\n",
      "         [ 0.1235,  0.0264, -0.1294,  ..., -0.1138, -0.0502, -0.3141],\n",
      "         ...,\n",
      "         [ 0.0990,  0.0621,  0.1366,  ..., -0.0784,  0.2218,  0.1117],\n",
      "         [ 0.1875,  0.0100,  0.0392,  ...,  0.0201,  0.1642,  0.0893],\n",
      "         [ 0.0050,  0.0286, -0.0362,  ..., -0.1282,  0.0086, -0.0467]],\n",
      "\n",
      "        [[ 0.1846,  0.1334,  0.0671,  ..., -0.1265,  0.0889, -0.0714],\n",
      "         [ 0.0194,  0.0428,  0.0996,  ..., -0.1628,  0.2370, -0.1173],\n",
      "         [ 0.0365,  0.1133,  0.0666,  ..., -0.2359,  0.2158, -0.1469],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        [[-0.0996,  0.0285, -0.0562,  ..., -0.1802,  0.1087, -0.1906],\n",
      "         [ 0.0247, -0.1033,  0.0913,  ..., -0.0900,  0.0601, -0.0734],\n",
      "         [ 0.0030, -0.0600,  0.0423,  ..., -0.1681,  0.0400, -0.1807],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0543,  0.0020,  0.0938,  ..., -0.0736,  0.1312,  0.0116],\n",
      "         [ 0.1372,  0.0538,  0.2875,  ..., -0.0230,  0.2181,  0.0344],\n",
      "         [ 0.0429, -0.0351,  0.1528,  ..., -0.1133,  0.1336, -0.1471],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        [[ 0.0512, -0.0721, -0.0635,  ..., -0.1466,  0.0016, -0.1444],\n",
      "         [ 0.0850, -0.1296, -0.1718,  ..., -0.1642, -0.0789, -0.0356],\n",
      "         [ 0.1630, -0.0860, -0.1286,  ..., -0.1044,  0.0928, -0.0776],\n",
      "         ...,\n",
      "         [ 0.0476,  0.0417,  0.0203,  ..., -0.1333,  0.2143,  0.0313],\n",
      "         [-0.0913, -0.0230, -0.1228,  ..., -0.2047,  0.0304, -0.0854],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        [[-0.0996,  0.0285, -0.0562,  ..., -0.1802,  0.1087, -0.1906],\n",
      "         [ 0.0247, -0.1033,  0.0913,  ..., -0.0900,  0.0601, -0.0734],\n",
      "         [ 0.0030, -0.0600,  0.0423,  ..., -0.1681,  0.0400, -0.1807],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1164,  0.0622,  0.0624,  ..., -0.0334,  0.1411,  0.0040],\n",
      "         [ 0.1144, -0.0971,  0.1300,  ...,  0.0004,  0.1222,  0.0645],\n",
      "         [ 0.0427, -0.0535,  0.0819,  ..., -0.1279,  0.0691, -0.1235],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        [[ 0.1194,  0.0960,  0.0555,  ..., -0.1833,  0.0021, -0.1259],\n",
      "         [ 0.0201,  0.0555,  0.0776,  ..., -0.1814,  0.2019, -0.1978],\n",
      "         [-0.0519, -0.0642, -0.0070,  ..., -0.0990,  0.3186, -0.1622],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        [[ 0.1076,  0.0559,  0.0648,  ..., -0.0718,  0.0178, -0.0710],\n",
      "         [ 0.1130, -0.0643,  0.1310,  ..., -0.0352,  0.0425, -0.0041],\n",
      "         [ 0.0452, -0.0214,  0.0788,  ..., -0.1458,  0.0492, -0.1581],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1699,  0.0632,  0.1285,  ..., -0.0439,  0.0900,  0.0165],\n",
      "         [ 0.1855,  0.1148,  0.0821,  ...,  0.0032,  0.1103,  0.0291],\n",
      "         [ 0.0992,  0.1091,  0.1736,  ..., -0.0727,  0.1109, -0.0649],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        [[ 0.1995,  0.0255,  0.0933,  ..., -0.0680,  0.1490,  0.0852],\n",
      "         [ 0.1438, -0.1231,  0.1812,  ..., -0.0373,  0.0555,  0.0980],\n",
      "         [ 0.0623, -0.0382,  0.1030,  ..., -0.1345,  0.0560, -0.1198],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]],\n",
      "\n",
      "        [[ 0.0996, -0.0462,  0.1015,  ..., -0.1224,  0.0961, -0.0981],\n",
      "         [-0.0185,  0.0290,  0.1320,  ..., -0.1789,  0.2930, -0.1576],\n",
      "         [-0.0720, -0.0759,  0.0234,  ..., -0.0954,  0.3564, -0.1362],\n",
      "         ...,\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245],\n",
      "         [ 0.1417,  0.0567,  0.1048,  ..., -0.0781,  0.1095, -0.0245]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0481, -0.0758, -0.0702,  ..., -0.1483, -0.0029, -0.1473],\n",
      "         [ 0.0444, -0.0353, -0.1515,  ..., -0.1992, -0.0885, -0.1736],\n",
      "         [ 0.1161,  0.0211, -0.1450,  ..., -0.1193, -0.0614, -0.3233],\n",
      "         ...,\n",
      "         [ 0.0993,  0.0638,  0.1353,  ..., -0.0793,  0.2187,  0.1136],\n",
      "         [ 0.1860,  0.0099,  0.0367,  ...,  0.0178,  0.1624,  0.0884],\n",
      "         [-0.0145,  0.0211, -0.0561,  ..., -0.1411, -0.0006, -0.0561]],\n",
      "\n",
      "        [[ 0.1845,  0.1313,  0.0656,  ..., -0.1275,  0.0881, -0.0721],\n",
      "         [ 0.0166,  0.0382,  0.0937,  ..., -0.1658,  0.2324, -0.1230],\n",
      "         [ 0.0339,  0.1103,  0.0611,  ..., -0.2380,  0.2107, -0.1518],\n",
      "         ...,\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255]],\n",
      "\n",
      "        [[-0.1052,  0.0237, -0.0650,  ..., -0.1849,  0.1051, -0.1961],\n",
      "         [ 0.0261, -0.1041,  0.0890,  ..., -0.0911,  0.0587, -0.0745],\n",
      "         [-0.0005, -0.0646,  0.0368,  ..., -0.1688,  0.0347, -0.1861],\n",
      "         ...,\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0544,  0.0004,  0.0919,  ..., -0.0757,  0.1296,  0.0095],\n",
      "         [ 0.1380,  0.0531,  0.2866,  ..., -0.0238,  0.2169,  0.0334],\n",
      "         [ 0.0383, -0.0391,  0.1481,  ..., -0.1141,  0.1295, -0.1513],\n",
      "         ...,\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255]],\n",
      "\n",
      "        [[ 0.0481, -0.0758, -0.0702,  ..., -0.1483, -0.0029, -0.1473],\n",
      "         [ 0.0794, -0.1348, -0.1838,  ..., -0.1689, -0.0854, -0.0411],\n",
      "         [ 0.1594, -0.0892, -0.1380,  ..., -0.1071,  0.0869, -0.0815],\n",
      "         ...,\n",
      "         [ 0.0417,  0.0383,  0.0072,  ..., -0.1377,  0.2105,  0.0255],\n",
      "         [-0.1155, -0.0359, -0.1489,  ..., -0.2198,  0.0182, -0.0958],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255]],\n",
      "\n",
      "        [[-0.1052,  0.0237, -0.0650,  ..., -0.1849,  0.1051, -0.1961],\n",
      "         [ 0.0261, -0.1041,  0.0890,  ..., -0.0911,  0.0587, -0.0745],\n",
      "         [-0.0005, -0.0646,  0.0368,  ..., -0.1688,  0.0347, -0.1861],\n",
      "         ...,\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255],\n",
      "         [ 0.1427,  0.0557,  0.1038,  ..., -0.0791,  0.1085, -0.0255]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 14 of 1000; error is 2.3229446411132812"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 5.4391e-02,  3.7600e-04,  9.1879e-02,  ..., -7.5653e-02,\n",
      "           1.2963e-01,  9.5066e-03],\n",
      "         [ 6.1779e-02, -1.3293e-01,  1.8807e-01,  ..., -4.4079e-02,\n",
      "           7.1898e-02,  4.6665e-02],\n",
      "         [ 2.7412e-02, -5.7628e-02,  1.0297e-01,  ..., -1.4382e-01,\n",
      "           5.9479e-02, -1.3429e-01],\n",
      "         ...,\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02]],\n",
      "\n",
      "        [[ 8.6041e-02, -5.6845e-02,  1.2363e-02,  ..., -1.6382e-01,\n",
      "           1.0753e-01, -1.3191e-02],\n",
      "         [-3.7191e-02,  1.7909e-04,  7.9963e-02,  ..., -2.0401e-01,\n",
      "           2.8886e-01, -1.4505e-01],\n",
      "         [-1.6861e-02, -1.0391e-02,  1.0973e-02,  ..., -1.5665e-01,\n",
      "           2.1707e-01, -1.7154e-01],\n",
      "         ...,\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02]],\n",
      "\n",
      "        [[ 1.8450e-01,  1.3128e-01,  6.5618e-02,  ..., -1.2746e-01,\n",
      "           8.8083e-02, -7.2055e-02],\n",
      "         [ 1.6603e-02,  3.8234e-02,  9.3729e-02,  ..., -1.6585e-01,\n",
      "           2.3241e-01, -1.2299e-01],\n",
      "         [-3.9376e-02, -7.5118e-02, -1.1050e-02,  ..., -7.8639e-02,\n",
      "           3.4564e-01, -1.3149e-01],\n",
      "         ...,\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.1480e-02, -3.8532e-02,  7.4790e-02,  ..., -1.1197e-01,\n",
      "           1.5218e-01,  1.6097e-02],\n",
      "         [ 1.4561e-01,  4.1619e-02,  6.2106e-02,  ..., -9.6147e-03,\n",
      "           1.6009e-01,  2.9810e-02],\n",
      "         [ 1.2995e-01, -8.1930e-02,  1.1498e-01,  ...,  5.5528e-02,\n",
      "           8.9807e-02, -1.2448e-02],\n",
      "         ...,\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02]],\n",
      "\n",
      "        [[ 1.5409e-01,  5.5892e-02,  9.0255e-02,  ...,  9.8144e-03,\n",
      "           1.0786e-01,  2.3552e-03],\n",
      "         [ 1.8876e-01,  1.3837e-01,  6.6657e-02,  ...,  1.6652e-02,\n",
      "           1.3662e-01,  3.8915e-02],\n",
      "         [ 1.5204e-01, -2.3982e-02, -9.3289e-02,  ..., -1.0824e-02,\n",
      "           7.7267e-02,  6.0455e-02],\n",
      "         ...,\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02]],\n",
      "\n",
      "        [[ 1.1580e-01,  6.0768e-02,  5.8807e-02,  ..., -3.5397e-02,\n",
      "           1.3876e-01,  1.6742e-03],\n",
      "         [ 1.1733e-01, -9.6300e-02,  1.3024e-01,  ..., -1.7076e-04,\n",
      "           1.2167e-01,  6.2863e-02],\n",
      "         [ 4.0177e-02, -5.7263e-02,  7.7833e-02,  ..., -1.2817e-01,\n",
      "           6.5086e-02, -1.2812e-01],\n",
      "         ...,\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02],\n",
      "         [ 1.4272e-01,  5.5740e-02,  1.0383e-01,  ..., -7.9106e-02,\n",
      "           1.0846e-01, -2.5463e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0447, -0.0799, -0.0772,  ..., -0.1502, -0.0078, -0.1504],\n",
      "         [ 0.0353, -0.0417, -0.1660,  ..., -0.2057, -0.0965, -0.1819],\n",
      "         [ 0.1080,  0.0150, -0.1616,  ..., -0.1250, -0.0736, -0.3333],\n",
      "         ...,\n",
      "         [ 0.0994,  0.0653,  0.1338,  ..., -0.0802,  0.2156,  0.1154],\n",
      "         [ 0.1843,  0.0096,  0.0340,  ...,  0.0156,  0.1606,  0.0874],\n",
      "         [-0.0347,  0.0124, -0.0776,  ..., -0.1544, -0.0107, -0.0664]],\n",
      "\n",
      "        [[ 0.1844,  0.1292,  0.0641,  ..., -0.1284,  0.0873, -0.0727],\n",
      "         [ 0.0136,  0.0336,  0.0877,  ..., -0.1690,  0.2277, -0.1288],\n",
      "         [ 0.0309,  0.1070,  0.0552,  ..., -0.2404,  0.2054, -0.1570],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        [[-0.1110,  0.0184, -0.0743,  ..., -0.1899,  0.1012, -0.2021],\n",
      "         [ 0.0274, -0.1050,  0.0864,  ..., -0.0922,  0.0571, -0.0757],\n",
      "         [-0.0042, -0.0698,  0.0307,  ..., -0.1696,  0.0291, -0.1920],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0545, -0.0013,  0.0898,  ..., -0.0778,  0.1280,  0.0073],\n",
      "         [ 0.1390,  0.0526,  0.2856,  ..., -0.0246,  0.2157,  0.0323],\n",
      "         [ 0.0335, -0.0434,  0.1430,  ..., -0.1150,  0.1250, -0.1559],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        [[ 0.0447, -0.0799, -0.0772,  ..., -0.1502, -0.0078, -0.1504],\n",
      "         [ 0.0734, -0.1405, -0.1965,  ..., -0.1737, -0.0925, -0.0469],\n",
      "         [ 0.1555, -0.0929, -0.1481,  ..., -0.1098,  0.0806, -0.0858],\n",
      "         ...,\n",
      "         [ 0.0350,  0.0344, -0.0067,  ..., -0.1424,  0.2063,  0.0193],\n",
      "         [-0.1408, -0.0503, -0.1770,  ..., -0.2355,  0.0048, -0.1075],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        [[-0.1110,  0.0184, -0.0743,  ..., -0.1899,  0.1012, -0.2021],\n",
      "         [ 0.0274, -0.1050,  0.0864,  ..., -0.0922,  0.0571, -0.0757],\n",
      "         [-0.0042, -0.0698,  0.0307,  ..., -0.1696,  0.0291, -0.1920],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1515,  0.1213,  0.1610,  ..., -0.1413,  0.1775,  0.0320],\n",
      "         [ 0.1951,  0.1020,  0.1054,  ..., -0.0307,  0.1676,  0.0413],\n",
      "         [ 0.1674, -0.0430,  0.1386,  ...,  0.0490,  0.0887, -0.0101],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        [[-0.0267, -0.0220,  0.0583,  ..., -0.0766,  0.1117, -0.0345],\n",
      "         [ 0.0511, -0.1309,  0.1617,  ..., -0.0321,  0.0732,  0.0170],\n",
      "         [ 0.0102, -0.0680,  0.1151,  ..., -0.0949,  0.0744,  0.0597],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        [[ 0.1092, -0.0442, -0.1013,  ..., -0.1489, -0.0139,  0.0350],\n",
      "         [ 0.0972, -0.1426,  0.0585,  ..., -0.0658,  0.0152,  0.0390],\n",
      "         [ 0.1825, -0.0644, -0.0969,  ..., -0.1171, -0.1865,  0.0241],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1288,  0.0537,  0.1425,  ..., -0.0521,  0.0541, -0.0204],\n",
      "         [ 0.1057, -0.0860,  0.1886,  ..., -0.0439,  0.0621, -0.0177],\n",
      "         [ 0.0242, -0.0577,  0.1021,  ..., -0.1507,  0.0415, -0.1714],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        [[ 0.0672,  0.0530,  0.0940,  ..., -0.1095,  0.0369, -0.0831],\n",
      "         [ 0.1589,  0.0849,  0.0675,  ..., -0.0204,  0.1039, -0.0146],\n",
      "         [ 0.0879,  0.1077,  0.0659,  ..., -0.0994,  0.1828, -0.1496],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]],\n",
      "\n",
      "        [[ 0.1151,  0.0593,  0.0550,  ..., -0.0374,  0.1362, -0.0007],\n",
      "         [ 0.1204, -0.0954,  0.1305,  ..., -0.0008,  0.1211,  0.0611],\n",
      "         [ 0.0374, -0.0614,  0.0734,  ..., -0.1285,  0.0607, -0.1332],\n",
      "         ...,\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265],\n",
      "         [ 0.1437,  0.0547,  0.1028,  ..., -0.0801,  0.1075, -0.0265]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0410, -0.0844, -0.0845,  ..., -0.1522, -0.0130, -0.1538],\n",
      "         [ 0.0256, -0.0490, -0.1815,  ..., -0.2127, -0.1053, -0.1908],\n",
      "         [ 0.0991,  0.0080, -0.1794,  ..., -0.1311, -0.0867, -0.3443],\n",
      "         ...,\n",
      "         [ 0.0993,  0.0667,  0.1322,  ..., -0.0812,  0.2124,  0.1171],\n",
      "         [ 0.1826,  0.0090,  0.0309,  ...,  0.0134,  0.1585,  0.0863],\n",
      "         [-0.0558,  0.0024, -0.1006,  ..., -0.1680, -0.0217, -0.0776]],\n",
      "\n",
      "        [[ 0.1844,  0.1273,  0.0626,  ..., -0.1294,  0.0866, -0.0734],\n",
      "         [ 0.0105,  0.0288,  0.0815,  ..., -0.1724,  0.2229, -0.1347],\n",
      "         [ 0.0276,  0.1035,  0.0490,  ..., -0.2430,  0.1998, -0.1626],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        [[-0.1172,  0.0126, -0.0840,  ..., -0.1951,  0.0968, -0.2084],\n",
      "         [ 0.0288, -0.1061,  0.0837,  ..., -0.0933,  0.0553, -0.0770],\n",
      "         [-0.0082, -0.0754,  0.0242,  ..., -0.1707,  0.0229, -0.1984],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0546, -0.0029,  0.0876,  ..., -0.0800,  0.1263,  0.0050],\n",
      "         [ 0.1401,  0.0521,  0.2847,  ..., -0.0252,  0.2145,  0.0313],\n",
      "         [ 0.0286, -0.0480,  0.1376,  ..., -0.1160,  0.1202, -0.1609],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        [[ 0.0410, -0.0844, -0.0845,  ..., -0.1522, -0.0130, -0.1538],\n",
      "         [ 0.0669, -0.1468, -0.2098,  ..., -0.1788, -0.1000, -0.0532],\n",
      "         [ 0.1513, -0.0971, -0.1589,  ..., -0.1127,  0.0738, -0.0903],\n",
      "         ...,\n",
      "         [ 0.0275,  0.0299, -0.0214,  ..., -0.1473,  0.2014,  0.0127],\n",
      "         [-0.1674, -0.0662, -0.2073,  ..., -0.2518, -0.0102, -0.1207],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        [[-0.1172,  0.0126, -0.0840,  ..., -0.1951,  0.0968, -0.2084],\n",
      "         [ 0.0288, -0.1061,  0.0837,  ..., -0.0933,  0.0553, -0.0770],\n",
      "         [-0.0082, -0.0754,  0.0242,  ..., -0.1707,  0.0229, -0.1984],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1095,  0.0880,  0.0354,  ..., -0.1962, -0.0117, -0.1384],\n",
      "         [ 0.0044,  0.0420,  0.0498,  ..., -0.1941,  0.1820, -0.2235],\n",
      "         [-0.0979, -0.0876, -0.0509,  ..., -0.1249,  0.2831, -0.1969],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        [[ 0.1553,  0.0433,  0.0830,  ..., -0.0370,  0.1374, -0.0316],\n",
      "         [ 0.1329, -0.0967,  0.1648,  ..., -0.0205,  0.0772,  0.0114],\n",
      "         [ 0.0328, -0.0564,  0.0813,  ..., -0.1362,  0.0494, -0.1648],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        [[ 0.0958, -0.0527,  0.0907,  ..., -0.1303,  0.0833, -0.1047],\n",
      "         [-0.0302,  0.0173,  0.1106,  ..., -0.1897,  0.2750, -0.1785],\n",
      "         [-0.1148, -0.0984, -0.0168,  ..., -0.1194,  0.3232, -0.1684],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1655,  0.1770,  0.1724,  ..., -0.0840,  0.0424,  0.0150],\n",
      "         [ 0.2482,  0.0307,  0.0712,  ..., -0.0955, -0.0819, -0.0490],\n",
      "         [ 0.2740,  0.2052,  0.0472,  ..., -0.0619, -0.0011,  0.0387],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        [[ 0.1333,  0.0721,  0.0809,  ..., -0.1014,  0.1055, -0.0890],\n",
      "         [ 0.1132, -0.1041,  0.1722,  ..., -0.0756,  0.0677,  0.0411],\n",
      "         [ 0.0247, -0.0626,  0.0790,  ..., -0.1563,  0.0472, -0.1539],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]],\n",
      "\n",
      "        [[ 0.1143,  0.0576,  0.0511,  ..., -0.0395,  0.1336, -0.0033],\n",
      "         [ 0.1235, -0.0945,  0.1307,  ..., -0.0015,  0.1204,  0.0592],\n",
      "         [ 0.0345, -0.0660,  0.0686,  ..., -0.1291,  0.0560, -0.1386],\n",
      "         ...,\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275],\n",
      "         [ 0.1447,  0.0537,  0.1018,  ..., -0.0811,  0.1065, -0.0275]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 16 of 1000; error is 2.2905476093292236"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 3.6935e-02, -8.9291e-02, -9.2264e-02,  ..., -1.5452e-01,\n",
      "          -1.8519e-02, -1.5753e-01],\n",
      "         [ 1.5075e-02, -5.7132e-02, -1.9795e-01,  ..., -2.2015e-01,\n",
      "          -1.1495e-01, -2.0061e-01],\n",
      "         [ 8.9381e-02,  1.0569e-04, -1.9848e-01,  ..., -1.3754e-01,\n",
      "          -1.0106e-01, -3.5624e-01],\n",
      "         ...,\n",
      "         [ 9.8998e-02,  6.7860e-02,  1.3036e-01,  ..., -8.2296e-02,\n",
      "           2.0921e-01,  1.1865e-01],\n",
      "         [ 1.8071e-01,  8.1660e-03,  2.7506e-02,  ...,  1.1176e-02,\n",
      "           1.5635e-01,  8.5135e-02],\n",
      "         [-7.7729e-02, -8.8890e-03, -1.2531e-01,  ..., -1.8216e-01,\n",
      "          -3.3859e-02, -8.9988e-02]],\n",
      "\n",
      "        [[ 1.8430e-01,  1.2534e-01,  6.1069e-02,  ..., -1.3039e-01,\n",
      "           8.5813e-02, -7.4160e-02],\n",
      "         [ 7.2360e-03,  2.3925e-02,  7.5151e-02,  ..., -1.7587e-01,\n",
      "           2.1792e-01, -1.4067e-01],\n",
      "         [ 2.3876e-02,  9.9736e-02,  4.2389e-02,  ..., -2.4585e-01,\n",
      "           1.9409e-01, -1.6846e-01],\n",
      "         ...,\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02]],\n",
      "\n",
      "        [[-1.2383e-01,  6.3201e-03, -9.4120e-02,  ..., -2.0055e-01,\n",
      "           9.2055e-02, -2.1523e-01],\n",
      "         [ 3.0121e-02, -1.0732e-01,  8.0855e-02,  ..., -9.4525e-02,\n",
      "           5.3445e-02, -7.8494e-02],\n",
      "         [-1.2607e-02, -8.1604e-02,  1.7149e-02,  ..., -1.7203e-01,\n",
      "           1.6333e-02, -2.0526e-01],\n",
      "         ...,\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.4816e-02, -4.6337e-03,  8.5327e-02,  ..., -8.2198e-02,\n",
      "           1.2444e-01,  2.6033e-03],\n",
      "         [ 1.4139e-01,  5.1720e-02,  2.8382e-01,  ..., -2.5761e-02,\n",
      "           2.1324e-01,  3.0190e-02],\n",
      "         [ 2.3614e-02, -5.2938e-02,  1.3185e-01,  ..., -1.1706e-01,\n",
      "           1.1496e-01, -1.6632e-01],\n",
      "         ...,\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02]],\n",
      "\n",
      "        [[ 3.6935e-02, -8.9291e-02, -9.2264e-02,  ..., -1.5452e-01,\n",
      "          -1.8519e-02, -1.5753e-01],\n",
      "         [ 5.9987e-02, -1.5357e-01, -2.2391e-01,  ..., -1.8415e-01,\n",
      "          -1.0809e-01, -6.0098e-02],\n",
      "         [ 1.4659e-01, -1.0179e-01, -1.7040e-01,  ..., -1.1581e-01,\n",
      "           6.6522e-02, -9.5341e-02],\n",
      "         ...,\n",
      "         [ 1.9104e-02,  2.4786e-02, -3.7240e-02,  ..., -1.5256e-01,\n",
      "           1.9600e-01,  5.4910e-03],\n",
      "         [-1.9533e-01, -8.3845e-02, -2.3995e-01,  ..., -2.6881e-01,\n",
      "          -2.6774e-02, -1.3557e-01],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02]],\n",
      "\n",
      "        [[-1.2383e-01,  6.3201e-03, -9.4120e-02,  ..., -2.0055e-01,\n",
      "           9.2055e-02, -2.1523e-01],\n",
      "         [ 3.0121e-02, -1.0732e-01,  8.0855e-02,  ..., -9.4525e-02,\n",
      "           5.3445e-02, -7.8494e-02],\n",
      "         [-1.2607e-02, -8.1604e-02,  1.7149e-02,  ..., -1.7203e-01,\n",
      "           1.6333e-02, -2.0526e-01],\n",
      "         ...,\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02],\n",
      "         [ 1.4572e-01,  5.2746e-02,  1.0085e-01,  ..., -8.2097e-02,\n",
      "           1.0547e-01, -2.8462e-02]]], device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1843,  0.1253,  0.0611,  ..., -0.1304,  0.0858, -0.0742],\n",
      "         [ 0.0072,  0.0239,  0.0752,  ..., -0.1759,  0.2179, -0.1407],\n",
      "         [-0.0798, -0.0989, -0.0496,  ..., -0.1034,  0.3138, -0.1646],\n",
      "         ...,\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285]],\n",
      "\n",
      "        [[ 0.1134,  0.0558,  0.0469,  ..., -0.0416,  0.1307, -0.0060],\n",
      "         [-0.0290,  0.0436,  0.0745,  ..., -0.1109,  0.2660, -0.1062],\n",
      "         [ 0.1126,  0.0123, -0.1162,  ..., -0.0775,  0.0815, -0.0702],\n",
      "         ...,\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285]],\n",
      "\n",
      "        [[ 0.0941, -0.0088,  0.1281,  ..., -0.0825,  0.0175, -0.0960],\n",
      "         [ 0.0894, -0.1582,  0.1654,  ..., -0.0394, -0.0052, -0.0373],\n",
      "         [ 0.1439, -0.0806,  0.1376,  ...,  0.0038,  0.0356, -0.0091],\n",
      "         ...,\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2152,  0.0318,  0.1045,  ..., -0.0660,  0.1495,  0.0931],\n",
      "         [ 0.1618, -0.1123,  0.1922,  ..., -0.0351,  0.0579,  0.0984],\n",
      "         [ 0.0563, -0.0512,  0.0911,  ..., -0.1343,  0.0394, -0.1382],\n",
      "         ...,\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285]],\n",
      "\n",
      "        [[ 0.1149,  0.0477,  0.0678,  ..., -0.0760,  0.0099, -0.0696],\n",
      "         [ 0.1283, -0.0604,  0.1430,  ..., -0.0355,  0.0444, -0.0030],\n",
      "         [ 0.0367, -0.0380,  0.0663,  ..., -0.1475,  0.0311, -0.1771],\n",
      "         ...,\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285]],\n",
      "\n",
      "        [[ 0.0944, -0.0553,  0.0866,  ..., -0.1330,  0.0789, -0.1071],\n",
      "         [-0.0347,  0.0129,  0.1028,  ..., -0.1937,  0.2686, -0.1858],\n",
      "         [-0.1306, -0.1073, -0.0320,  ..., -0.1287,  0.3105, -0.1808],\n",
      "         ...,\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285],\n",
      "         [ 0.1457,  0.0527,  0.1008,  ..., -0.0821,  0.1055, -0.0285]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 3.2509e-02, -9.4578e-02, -1.0036e-01,  ..., -1.5707e-01,\n",
      "          -2.4428e-02, -1.6165e-01],\n",
      "         [ 3.8251e-03, -6.6132e-02, -2.1556e-01,  ..., -2.2820e-01,\n",
      "          -1.2558e-01, -2.1132e-01],\n",
      "         [ 7.8684e-02, -8.6947e-03, -2.1905e-01,  ..., -1.4457e-01,\n",
      "          -1.1665e-01, -3.6932e-01],\n",
      "         ...,\n",
      "         [ 9.8402e-02,  6.8706e-02,  1.2815e-01,  ..., -8.3432e-02,\n",
      "           2.0591e-01,  1.1999e-01],\n",
      "         [ 1.7868e-01,  6.9583e-03,  2.3765e-02,  ...,  8.9113e-03,\n",
      "           1.5398e-01,  8.3891e-02],\n",
      "         [-1.0063e-01, -2.1634e-02, -1.5180e-01,  ..., -1.9685e-01,\n",
      "          -4.7195e-02, -1.0355e-01]],\n",
      "\n",
      "        [[ 1.8425e-01,  1.2351e-01,  5.9548e-02,  ..., -1.3143e-01,\n",
      "           8.5069e-02, -7.4948e-02],\n",
      "         [ 3.7898e-03,  1.8990e-02,  6.8654e-02,  ..., -1.7954e-01,\n",
      "           2.1292e-01, -1.4679e-01],\n",
      "         [ 1.9799e-02,  9.5697e-02,  3.5416e-02,  ..., -2.4905e-01,\n",
      "           1.8812e-01, -1.7471e-01],\n",
      "         ...,\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02]],\n",
      "\n",
      "        [[-1.3081e-01, -4.3517e-04, -1.0475e-01,  ..., -2.0630e-01,\n",
      "           8.6853e-02, -2.2251e-01],\n",
      "         [ 3.1428e-02, -1.0869e-01,  7.7791e-02,  ..., -9.5820e-02,\n",
      "           5.1407e-02, -8.0103e-02],\n",
      "         [-1.7331e-02, -8.8252e-02,  9.5146e-03,  ..., -1.7369e-01,\n",
      "           9.2169e-03, -2.1268e-01],\n",
      "         ...,\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.5003e-02, -6.3446e-03,  8.2924e-02,  ..., -8.4480e-02,\n",
      "           1.2250e-01,  1.1405e-04],\n",
      "         [ 1.4275e-01,  5.1415e-02,  2.8296e-01,  ..., -2.6284e-02,\n",
      "           2.1202e-01,  2.9101e-02],\n",
      "         [ 1.8411e-02, -5.8161e-02,  1.2568e-01,  ..., -1.1838e-01,\n",
      "           1.0932e-01, -1.7220e-01],\n",
      "         ...,\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02]],\n",
      "\n",
      "        [[ 3.2509e-02, -9.4578e-02, -1.0036e-01,  ..., -1.5707e-01,\n",
      "          -2.4428e-02, -1.6165e-01],\n",
      "         [ 5.2535e-02, -1.6096e-01, -2.3874e-01,  ..., -1.8987e-01,\n",
      "          -1.1679e-01, -6.7593e-02],\n",
      "         [ 1.4144e-01, -1.0703e-01, -1.8273e-01,  ..., -1.1918e-01,\n",
      "           5.8642e-02, -1.0086e-01],\n",
      "         ...,\n",
      "         [ 9.6111e-03,  1.8954e-02, -5.4241e-02,  ..., -1.5822e-01,\n",
      "           1.8983e-01, -2.3091e-03],\n",
      "         [-2.2477e-01, -1.0329e-01, -2.7510e-01,  ..., -2.8668e-01,\n",
      "          -4.5190e-02, -1.5231e-01],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02]],\n",
      "\n",
      "        [[-1.3081e-01, -4.3517e-04, -1.0475e-01,  ..., -2.0630e-01,\n",
      "           8.6853e-02, -2.2251e-01],\n",
      "         [ 3.1428e-02, -1.0869e-01,  7.7791e-02,  ..., -9.5820e-02,\n",
      "           5.1407e-02, -8.0103e-02],\n",
      "         [-1.7331e-02, -8.8252e-02,  9.5146e-03,  ..., -1.7369e-01,\n",
      "           9.2169e-03, -2.1268e-01],\n",
      "         ...,\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02],\n",
      "         [ 1.4672e-01,  5.1749e-02,  9.9859e-02,  ..., -8.3093e-02,\n",
      "           1.0448e-01, -2.9461e-02]]], device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.2097,  0.0558,  0.1845,  ..., -0.0188,  0.1944,  0.0763],\n",
      "         [ 0.0364,  0.0357,  0.1565,  ..., -0.1195,  0.3014, -0.1064],\n",
      "         [ 0.0390, -0.0217,  0.0910,  ..., -0.0060,  0.1623, -0.1724],\n",
      "         ...,\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295]],\n",
      "\n",
      "        [[ 0.1842,  0.1235,  0.0595,  ..., -0.1314,  0.0851, -0.0749],\n",
      "         [ 0.0038,  0.0190,  0.0687,  ..., -0.1795,  0.2129, -0.1468],\n",
      "         [ 0.1446, -0.0665,  0.0373,  ...,  0.0274,  0.2379, -0.0974],\n",
      "         ...,\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295]],\n",
      "\n",
      "        [[ 0.1396,  0.1209,  0.2272,  ..., -0.0597,  0.2195, -0.0511],\n",
      "         [-0.0009,  0.0482,  0.1406,  ..., -0.1247,  0.3002, -0.1541],\n",
      "         [-0.1284, -0.1005, -0.0147,  ..., -0.0976,  0.3533, -0.1467],\n",
      "         ...,\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1042, -0.0552, -0.1199,  ..., -0.1582, -0.0279,  0.0259],\n",
      "         [ 0.1051, -0.1414,  0.0563,  ..., -0.0690,  0.0116,  0.0332],\n",
      "         [ 0.1873, -0.0765, -0.1119,  ..., -0.1215, -0.2012,  0.0130],\n",
      "         ...,\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295]],\n",
      "\n",
      "        [[ 0.1308,  0.0505,  0.1395,  ..., -0.0532,  0.0476, -0.0319],\n",
      "         [-0.0138,  0.0129,  0.1206,  ..., -0.1453,  0.2177, -0.1876],\n",
      "         [-0.1232, -0.1155, -0.0336,  ..., -0.1030,  0.3063, -0.1776],\n",
      "         ...,\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295]],\n",
      "\n",
      "        [[ 0.0930, -0.0582,  0.0822,  ..., -0.1357,  0.0745, -0.1095],\n",
      "         [-0.0395,  0.0082,  0.0947,  ..., -0.1980,  0.2620, -0.1933],\n",
      "         [-0.1471, -0.1168, -0.0483,  ..., -0.1387,  0.2970, -0.1942],\n",
      "         ...,\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295],\n",
      "         [ 0.1467,  0.0517,  0.0999,  ..., -0.0831,  0.1045, -0.0295]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 2.7722e-02, -1.0024e-01, -1.0888e-01,  ..., -1.5990e-01,\n",
      "          -3.0727e-02, -1.6619e-01],\n",
      "         [-8.2373e-03, -7.5996e-02, -2.3435e-01,  ..., -2.3686e-01,\n",
      "          -1.3721e-01, -2.2303e-01],\n",
      "         [ 6.6951e-02, -1.8465e-02, -2.4120e-01,  ..., -1.5224e-01,\n",
      "          -1.3363e-01, -3.8363e-01],\n",
      "         ...,\n",
      "         [ 9.7465e-02,  6.9187e-02,  1.2550e-01,  ..., -8.4646e-02,\n",
      "           2.0250e-01,  1.2112e-01],\n",
      "         [ 1.7642e-01,  5.3378e-03,  1.9603e-02,  ...,  6.5965e-03,\n",
      "           1.5139e-01,  8.2565e-02],\n",
      "         [-1.2455e-01, -3.5852e-02, -1.8016e-01,  ..., -2.1216e-01,\n",
      "          -6.1802e-02, -1.1843e-01]],\n",
      "\n",
      "        [[ 1.8421e-01,  1.2178e-01,  5.8025e-02,  ..., -1.3250e-01,\n",
      "           8.4331e-02, -7.5788e-02],\n",
      "         [ 1.6458e-04,  1.3991e-02,  6.1999e-02,  ..., -1.8342e-01,\n",
      "           2.0786e-01, -1.5304e-01],\n",
      "         [ 1.5318e-02,  9.1380e-02,  2.8041e-02,  ..., -2.5261e-01,\n",
      "           1.8193e-01, -1.8130e-01],\n",
      "         ...,\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02]],\n",
      "\n",
      "        [[-1.3821e-01, -7.6874e-03, -1.1591e-01,  ..., -2.1237e-01,\n",
      "           8.1180e-02, -2.3029e-01],\n",
      "         [ 3.2661e-02, -1.1019e-01,  7.4541e-02,  ..., -9.7225e-02,\n",
      "           4.9232e-02, -8.1819e-02],\n",
      "         [-2.2446e-02, -9.5387e-02,  1.2650e-03,  ..., -1.7573e-01,\n",
      "           1.5769e-03, -2.2062e-01],\n",
      "         ...,\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.5199e-02, -8.0817e-03,  8.0408e-02,  ..., -8.6820e-02,\n",
      "           1.2043e-01, -2.4735e-03],\n",
      "         [ 1.4420e-01,  5.1174e-02,  2.8210e-01,  ..., -2.6771e-02,\n",
      "           2.1080e-01,  2.7982e-02],\n",
      "         [ 1.3033e-02, -6.3704e-02,  1.1908e-01,  ..., -1.1993e-01,\n",
      "           1.0325e-01, -1.7851e-01],\n",
      "         ...,\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02]],\n",
      "\n",
      "        [[ 2.7722e-02, -1.0024e-01, -1.0888e-01,  ..., -1.5990e-01,\n",
      "          -3.0727e-02, -1.6619e-01],\n",
      "         [ 4.4555e-02, -1.6891e-01, -2.5438e-01,  ..., -1.9600e-01,\n",
      "          -1.2614e-01, -7.5771e-02],\n",
      "         [ 1.3578e-01, -1.1285e-01, -1.9592e-01,  ..., -1.2288e-01,\n",
      "           5.0150e-02, -1.0695e-01],\n",
      "         ...,\n",
      "         [-1.0983e-03,  1.2320e-02, -7.2628e-02,  ..., -1.6438e-01,\n",
      "           1.8284e-01, -1.0876e-02],\n",
      "         [-2.5584e-01, -1.2464e-01, -3.1298e-01,  ..., -3.0550e-01,\n",
      "          -6.5614e-02, -1.7113e-01],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02]],\n",
      "\n",
      "        [[-1.3821e-01, -7.6874e-03, -1.1591e-01,  ..., -2.1237e-01,\n",
      "           8.1180e-02, -2.3029e-01],\n",
      "         [ 3.2661e-02, -1.1019e-01,  7.4541e-02,  ..., -9.7225e-02,\n",
      "           4.9232e-02, -8.1819e-02],\n",
      "         [-2.2446e-02, -9.5387e-02,  1.2650e-03,  ..., -1.7573e-01,\n",
      "           1.5769e-03, -2.2062e-01],\n",
      "         ...,\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02],\n",
      "         [ 1.4773e-01,  5.0753e-02,  9.8871e-02,  ..., -8.4089e-02,\n",
      "           1.0348e-01, -3.0460e-02]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 19 of 1000; error is 2.2324211597442627"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.0894, -0.0281,  0.0055,  ..., -0.0870,  0.1478, -0.0553],\n",
      "         [-0.0727, -0.0203,  0.0376,  ..., -0.1646,  0.2466, -0.1788],\n",
      "         [-0.1792, -0.1255, -0.0844,  ..., -0.1365,  0.2729, -0.2025],\n",
      "         ...,\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305]],\n",
      "\n",
      "        [[ 0.0552, -0.0081,  0.0804,  ..., -0.0868,  0.1204, -0.0025],\n",
      "         [-0.0482, -0.0192,  0.0870,  ..., -0.1370,  0.2365, -0.1351],\n",
      "         [-0.1511, -0.1229, -0.0562,  ..., -0.1133,  0.2943, -0.1799],\n",
      "         ...,\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305]],\n",
      "\n",
      "        [[ 0.1842,  0.1218,  0.0580,  ..., -0.1325,  0.0843, -0.0758],\n",
      "         [ 0.1463, -0.0721,  0.1515,  ..., -0.0724,  0.0706,  0.0101],\n",
      "         [ 0.0340, -0.0736,  0.0532,  ..., -0.1640,  0.0244, -0.1723],\n",
      "         ...,\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1189,  0.0440,  0.0698,  ..., -0.0776,  0.0064, -0.0688],\n",
      "         [ 0.1365, -0.0574,  0.1496,  ..., -0.0354,  0.0466, -0.0021],\n",
      "         [ 0.0316, -0.0480,  0.0587,  ..., -0.1495,  0.0206, -0.1885],\n",
      "         ...,\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305]],\n",
      "\n",
      "        [[ 0.0630, -0.0293,  0.0925,  ..., -0.1853,  0.1414, -0.0373],\n",
      "         [ 0.0785, -0.1279,  0.1576,  ..., -0.1047,  0.0813, -0.0127],\n",
      "         [ 0.1208, -0.0419,  0.0913,  ..., -0.0727,  0.0521, -0.0903],\n",
      "         ...,\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305]],\n",
      "\n",
      "        [[ 0.1597,  0.0364,  0.0768,  ..., -0.0402,  0.1266, -0.0357],\n",
      "         [ 0.1448, -0.0949,  0.1687,  ..., -0.0212,  0.0745,  0.0092],\n",
      "         [ 0.0240, -0.0719,  0.0663,  ..., -0.1392,  0.0320, -0.1837],\n",
      "         ...,\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305],\n",
      "         [ 0.1477,  0.0508,  0.0989,  ..., -0.0841,  0.1035, -0.0305]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0226, -0.1063, -0.1178,  ..., -0.1631, -0.0374, -0.1712],\n",
      "         [-0.0212, -0.0867, -0.2544,  ..., -0.2462, -0.1499, -0.2358],\n",
      "         [ 0.0541, -0.0293, -0.2651,  ..., -0.1607, -0.1521, -0.3992],\n",
      "         ...,\n",
      "         [ 0.0961,  0.0692,  0.1223,  ..., -0.0860,  0.1989,  0.1220],\n",
      "         [ 0.1739,  0.0032,  0.0149,  ...,  0.0042,  0.1486,  0.0812],\n",
      "         [-0.1496, -0.0516, -0.2105,  ..., -0.2282, -0.0778, -0.1347]],\n",
      "\n",
      "        [[ 0.1842,  0.1202,  0.0565,  ..., -0.1336,  0.0836, -0.0767],\n",
      "         [-0.0037,  0.0089,  0.0552,  ..., -0.1875,  0.2027, -0.1594],\n",
      "         [ 0.0104,  0.0868,  0.0202,  ..., -0.2566,  0.1755, -0.1883],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        [[-0.1461, -0.0154, -0.1276,  ..., -0.2188,  0.0750, -0.2386],\n",
      "         [ 0.0338, -0.1118,  0.0711,  ..., -0.0988,  0.0469, -0.0836],\n",
      "         [-0.0280, -0.1030, -0.0077,  ..., -0.1782, -0.0066, -0.2291],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0554, -0.0098,  0.0778,  ..., -0.0892,  0.1182, -0.0052],\n",
      "         [ 0.1457,  0.0510,  0.2812,  ..., -0.0272,  0.2096,  0.0268],\n",
      "         [ 0.0075, -0.0696,  0.1120,  ..., -0.1218,  0.0967, -0.1853],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        [[ 0.0226, -0.1063, -0.1178,  ..., -0.1631, -0.0374, -0.1712],\n",
      "         [ 0.0360, -0.1774, -0.2709,  ..., -0.2026, -0.1362, -0.0847],\n",
      "         [ 0.1295, -0.1193, -0.2101,  ..., -0.1270,  0.0410, -0.1137],\n",
      "         ...,\n",
      "         [-0.0132,  0.0048, -0.0926,  ..., -0.1711,  0.1749, -0.0203],\n",
      "         [-0.2887, -0.1480, -0.3538,  ..., -0.3254, -0.0882, -0.1922],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        [[-0.1461, -0.0154, -0.1276,  ..., -0.2188,  0.0750, -0.2386],\n",
      "         [ 0.0338, -0.1118,  0.0711,  ..., -0.0988,  0.0469, -0.0836],\n",
      "         [-0.0280, -0.1030, -0.0077,  ..., -0.1782, -0.0066, -0.2291],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1820,  0.0548,  0.0684,  ..., -0.1594, -0.0052, -0.0520],\n",
      "         [ 0.2109,  0.0962,  0.0638,  ..., -0.0365,  0.0934,  0.0264],\n",
      "         [ 0.1461,  0.0082,  0.0353,  ..., -0.1092,  0.0681, -0.0390],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        [[ 0.0568,  0.0234, -0.0436,  ..., -0.1381,  0.0465, -0.1223],\n",
      "         [ 0.1455,  0.0969, -0.0249,  ..., -0.0395,  0.1132, -0.0388],\n",
      "         [ 0.1065,  0.1328, -0.0043,  ..., -0.0213,  0.1510, -0.0571],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        [[ 0.1671,  0.1766,  0.1643,  ..., -0.0904,  0.0377,  0.0124],\n",
      "         [ 0.2411,  0.0215,  0.0505,  ..., -0.1036, -0.0934, -0.0607],\n",
      "         [ 0.2732,  0.2008,  0.0390,  ..., -0.0666, -0.0078,  0.0308],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1210,  0.0424,  0.0709,  ..., -0.0783,  0.0048, -0.0683],\n",
      "         [ 0.1408, -0.0556,  0.1532,  ..., -0.0353,  0.0482, -0.0015],\n",
      "         [ 0.0288, -0.0533,  0.0545,  ..., -0.1508,  0.0151, -0.1946],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        [[ 0.1293, -0.0095,  0.1526,  ..., -0.0869,  0.0480, -0.0451],\n",
      "         [ 0.1913,  0.0750,  0.0794,  ..., -0.0101,  0.1034, -0.0162],\n",
      "         [ 0.0186, -0.0078, -0.0290,  ..., -0.1744,  0.0219, -0.1458],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]],\n",
      "\n",
      "        [[ 0.1100,  0.0494,  0.0336,  ..., -0.0485,  0.1212, -0.0150],\n",
      "         [ 0.1363, -0.0905,  0.1316,  ..., -0.0047,  0.1180,  0.0513],\n",
      "         [ 0.0206, -0.0875,  0.0452,  ..., -0.1339,  0.0333, -0.1651],\n",
      "         ...,\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315],\n",
      "         [ 0.1487,  0.0498,  0.0979,  ..., -0.0851,  0.1025, -0.0315]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0170, -0.1126, -0.1273,  ..., -0.1666, -0.0446, -0.1766],\n",
      "         [-0.0350, -0.0983, -0.2758,  ..., -0.2563, -0.1637, -0.2498],\n",
      "         [ 0.0400, -0.0412, -0.2908,  ..., -0.1699, -0.1723, -0.4163],\n",
      "         ...,\n",
      "         [ 0.0942,  0.0687,  0.1184,  ..., -0.0875,  0.1952,  0.1225],\n",
      "         [ 0.1709,  0.0005,  0.0096,  ...,  0.0017,  0.1455,  0.0796],\n",
      "         [-0.1758, -0.0690, -0.2429,  ..., -0.2450, -0.0952, -0.1525]],\n",
      "\n",
      "        [[ 0.1842,  0.1187,  0.0549,  ..., -0.1348,  0.0829, -0.0777],\n",
      "         [-0.0077,  0.0038,  0.0482,  ..., -0.1918,  0.1975, -0.1659],\n",
      "         [ 0.0050,  0.0819,  0.0120,  ..., -0.2609,  0.1688, -0.1956],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        [[-0.1544, -0.0237, -0.1399,  ..., -0.2256,  0.0683, -0.2475],\n",
      "         [ 0.0347, -0.1136,  0.0674,  ..., -0.1005,  0.0445, -0.0856],\n",
      "         [-0.0340, -0.1111, -0.0173,  ..., -0.1812, -0.0153, -0.2382],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0555, -0.0117,  0.0750,  ..., -0.0917,  0.1159, -0.0080],\n",
      "         [ 0.1473,  0.0508,  0.2804,  ..., -0.0277,  0.2083,  0.0256],\n",
      "         [ 0.0017, -0.0757,  0.1044,  ..., -0.1239,  0.0898, -0.1925],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        [[ 0.0170, -0.1126, -0.1273,  ..., -0.1666, -0.0446, -0.1766],\n",
      "         [ 0.0269, -0.1865, -0.2883,  ..., -0.2097, -0.1469, -0.0944],\n",
      "         [ 0.1226, -0.1264, -0.2253,  ..., -0.1316,  0.0312, -0.1211],\n",
      "         ...,\n",
      "         [-0.0269, -0.0039, -0.1145,  ..., -0.1786,  0.1659, -0.0309],\n",
      "         [-0.3234, -0.1736, -0.3978,  ..., -0.3464, -0.1133, -0.2159],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        [[-0.1544, -0.0237, -0.1399,  ..., -0.2256,  0.0683, -0.2475],\n",
      "         [ 0.0347, -0.1136,  0.0674,  ..., -0.1005,  0.0445, -0.0856],\n",
      "         [-0.0340, -0.1111, -0.0173,  ..., -0.1812, -0.0153, -0.2382],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1254,  0.0376,  0.0492,  ..., -0.0160,  0.0830, -0.0359],\n",
      "         [ 0.1803,  0.1267,  0.0455,  ...,  0.0033,  0.1131,  0.0135],\n",
      "         [ 0.1481, -0.0528, -0.1449,  ..., -0.0398,  0.0319,  0.0112],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        [[ 0.0442,  0.0057,  0.0659,  ..., -0.1752,  0.1790,  0.0078],\n",
      "         [ 0.1248,  0.0345,  0.0475,  ..., -0.1229,  0.2372,  0.0508],\n",
      "         [ 0.2304,  0.0948,  0.1691,  ...,  0.0055,  0.2131,  0.0262],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        [[ 0.0555, -0.0117,  0.0750,  ..., -0.0917,  0.1159, -0.0080],\n",
      "         [-0.0562, -0.0293,  0.0726,  ..., -0.1459,  0.2252, -0.1490],\n",
      "         [-0.1857, -0.1444, -0.0900,  ..., -0.1358,  0.2659, -0.2094],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0855,  0.0092, -0.0519,  ..., -0.1842,  0.1109,  0.0012],\n",
      "         [-0.0541,  0.0031,  0.0128,  ..., -0.2068,  0.2371, -0.1623],\n",
      "         [-0.1850, -0.1378, -0.1225,  ..., -0.1447,  0.2648, -0.2226],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        [[ 0.0980, -0.0201,  0.1217,  ..., -0.0880,  0.0086, -0.1031],\n",
      "         [ 0.1036, -0.1555,  0.1707,  ..., -0.0413, -0.0062, -0.0438],\n",
      "         [ 0.1658, -0.0728,  0.1448,  ...,  0.0080,  0.0384, -0.0083],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]],\n",
      "\n",
      "        [[-0.1544, -0.0237, -0.1399,  ..., -0.2256,  0.0683, -0.2475],\n",
      "         [ 0.0347, -0.1136,  0.0674,  ..., -0.1005,  0.0445, -0.0856],\n",
      "         [-0.0340, -0.1111, -0.0173,  ..., -0.1812, -0.0153, -0.2382],\n",
      "         ...,\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325],\n",
      "         [ 0.1497,  0.0488,  0.0969,  ..., -0.0861,  0.1015, -0.0325]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 22 of 1000; error is 2.1591637134552527"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0111, -0.1193, -0.1372,  ..., -0.1704, -0.0522, -0.1826],\n",
      "         [-0.0498, -0.1108, -0.2985,  ..., -0.2672, -0.1786, -0.2649],\n",
      "         [ 0.0246, -0.0543, -0.3185,  ..., -0.1802, -0.1942, -0.4348],\n",
      "         ...,\n",
      "         [ 0.0916,  0.0675,  0.1136,  ..., -0.0892,  0.1911,  0.1226],\n",
      "         [ 0.1675, -0.0028,  0.0034,  ..., -0.0010,  0.1421,  0.0780],\n",
      "         [-0.2034, -0.0881, -0.2776,  ..., -0.2628, -0.1141, -0.1720]],\n",
      "\n",
      "        [[ 0.1841,  0.1173,  0.0534,  ..., -0.1360,  0.0821, -0.0787],\n",
      "         [-0.0120, -0.0014,  0.0410,  ..., -0.1964,  0.1923, -0.1726],\n",
      "         [-0.0008,  0.0766,  0.0033,  ..., -0.2658,  0.1618, -0.2033],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        [[-0.1632, -0.0324, -0.1529,  ..., -0.2329,  0.0611, -0.2569],\n",
      "         [ 0.0355, -0.1155,  0.0634,  ..., -0.1024,  0.0419, -0.0877],\n",
      "         [-0.0405, -0.1196, -0.0278,  ..., -0.1846, -0.0246, -0.2479],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0556, -0.0135,  0.0721,  ..., -0.0943,  0.1135, -0.0109],\n",
      "         [ 0.1489,  0.0507,  0.2794,  ..., -0.0282,  0.2070,  0.0243],\n",
      "         [-0.0042, -0.0821,  0.0963,  ..., -0.1264,  0.0824, -0.2003],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        [[ 0.0111, -0.1193, -0.1372,  ..., -0.1704, -0.0522, -0.1826],\n",
      "         [ 0.0172, -0.1962, -0.3067,  ..., -0.2173, -0.1584, -0.1049],\n",
      "         [ 0.1151, -0.1342, -0.2416,  ..., -0.1367,  0.0205, -0.1293],\n",
      "         ...,\n",
      "         [-0.0424, -0.0138, -0.1386,  ..., -0.1869,  0.1556, -0.0427],\n",
      "         [-0.3602, -0.2015, -0.4452,  ..., -0.3687, -0.1410, -0.2422],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        [[-0.1632, -0.0324, -0.1529,  ..., -0.2329,  0.0611, -0.2569],\n",
      "         [ 0.0355, -0.1155,  0.0634,  ..., -0.1024,  0.0419, -0.0877],\n",
      "         [-0.0405, -0.1196, -0.0278,  ..., -0.1846, -0.0246, -0.2479],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1071,  0.0441,  0.0238,  ..., -0.0535,  0.1139, -0.0219],\n",
      "         [ 0.1426, -0.0883,  0.1323,  ..., -0.0065,  0.1171,  0.0474],\n",
      "         [ 0.0122, -0.1002,  0.0308,  ..., -0.1383,  0.0196, -0.1810],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        [[ 0.1071,  0.0441,  0.0238,  ..., -0.0535,  0.1139, -0.0219],\n",
      "         [-0.0565,  0.0177,  0.0302,  ..., -0.1337,  0.2324, -0.1463],\n",
      "         [-0.0226,  0.0104, -0.0370,  ..., -0.1585,  0.1489, -0.0844],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        [[ 0.0862, -0.0725,  0.0611,  ..., -0.1476,  0.0553, -0.1202],\n",
      "         [-0.0627, -0.0137,  0.0581,  ..., -0.2183,  0.2335, -0.2257],\n",
      "         [-0.2219, -0.1628, -0.1241,  ..., -0.1870,  0.2332, -0.2582],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1777,  0.0487,  0.0609,  ..., -0.1639, -0.0121, -0.0582],\n",
      "         [ 0.2100,  0.0927,  0.0607,  ..., -0.0380,  0.0883,  0.0225],\n",
      "         [ 0.2993,  0.0243,  0.1030,  ...,  0.0004, -0.1233,  0.0621],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        [[ 0.1251,  0.0394,  0.0733,  ..., -0.0794,  0.0019, -0.0672],\n",
      "         [-0.0180,  0.0123,  0.0627,  ..., -0.1544,  0.1649, -0.2127],\n",
      "         [-0.1105, -0.0267,  0.0332,  ..., -0.1975,  0.2268, -0.2964],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]],\n",
      "\n",
      "        [[ 0.1638,  0.0299,  0.0702,  ..., -0.0434,  0.1161, -0.0396],\n",
      "         [-0.0392, -0.0084,  0.0568,  ..., -0.1592,  0.2329, -0.1971],\n",
      "         [ 0.0910, -0.0945,  0.1450,  ..., -0.0505,  0.1072, -0.0660],\n",
      "         ...,\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335],\n",
      "         [ 0.1507,  0.0478,  0.0959,  ..., -0.0871,  0.1005, -0.0335]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[ 0.0048, -0.1263, -0.1477,  ..., -0.1747, -0.0603, -0.1891],\n",
      "         [-0.0656, -0.1241, -0.3228,  ..., -0.2789, -0.1947, -0.2813],\n",
      "         [ 0.0077, -0.0687, -0.3484,  ..., -0.1915, -0.2180, -0.4549],\n",
      "         ...,\n",
      "         [ 0.0880,  0.0655,  0.1077,  ..., -0.0913,  0.1867,  0.1222],\n",
      "         [ 0.1634, -0.0071, -0.0038,  ..., -0.0039,  0.1381,  0.0761],\n",
      "         [-0.2326, -0.1090, -0.3148,  ..., -0.2816, -0.1348, -0.1932]],\n",
      "\n",
      "        [[ 0.1841,  0.1160,  0.0517,  ..., -0.1373,  0.0814, -0.0799],\n",
      "         [-0.0165, -0.0067,  0.0336,  ..., -0.2012,  0.1869, -0.1796],\n",
      "         [-0.0073,  0.0710, -0.0058,  ..., -0.2711,  0.1546, -0.2115],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        [[-0.1725, -0.0416, -0.1665,  ..., -0.2406,  0.0533, -0.2670],\n",
      "         [ 0.0359, -0.1176,  0.0592,  ..., -0.1045,  0.0392, -0.0899],\n",
      "         [-0.0476, -0.1286, -0.0391,  ..., -0.1887, -0.0345, -0.2582],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0557, -0.0154,  0.0690,  ..., -0.0970,  0.1109, -0.0139],\n",
      "         [ 0.1505,  0.0505,  0.2785,  ..., -0.0286,  0.2057,  0.0230],\n",
      "         [-0.0104, -0.0888,  0.0876,  ..., -0.1293,  0.0745, -0.2086],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        [[ 0.0048, -0.1263, -0.1477,  ..., -0.1747, -0.0603, -0.1891],\n",
      "         [ 0.0069, -0.2065, -0.3262,  ..., -0.2256, -0.1706, -0.1163],\n",
      "         [ 0.1067, -0.1428, -0.2591,  ..., -0.1425,  0.0091, -0.1383],\n",
      "         ...,\n",
      "         [-0.0600, -0.0252, -0.1654,  ..., -0.1961,  0.1438, -0.0560],\n",
      "         [-0.3993, -0.2320, -0.4964,  ..., -0.3925, -0.1717, -0.2717],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        [[-0.1725, -0.0416, -0.1665,  ..., -0.2406,  0.0533, -0.2670],\n",
      "         [ 0.0359, -0.1176,  0.0592,  ..., -0.1045,  0.0392, -0.0899],\n",
      "         [-0.0476, -0.1286, -0.0391,  ..., -0.1887, -0.0345, -0.2582],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1397, -0.0014,  0.0404,  ..., -0.0787,  0.1712,  0.0088],\n",
      "         [ 0.1008, -0.0468, -0.0158,  ..., -0.0155,  0.0406, -0.1422],\n",
      "         [ 0.0717,  0.0790,  0.0365,  ..., -0.1210, -0.0413, -0.1078],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        [[ 0.2153,  0.0535,  0.1877,  ..., -0.0247,  0.1983,  0.0801],\n",
      "         [ 0.1439, -0.0702,  0.2027,  ..., -0.0430,  0.1131,  0.0172],\n",
      "         [ 0.0169, -0.0823,  0.0673,  ..., -0.1456,  0.0295, -0.2070],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        [[ 0.1055,  0.0412,  0.0186,  ..., -0.0562,  0.1100, -0.0255],\n",
      "         [-0.0632,  0.0117,  0.0202,  ..., -0.1392,  0.2249, -0.1551],\n",
      "         [ 0.0336, -0.0623, -0.2286,  ..., -0.1297,  0.0158, -0.1392],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0843, -0.0768,  0.0549,  ..., -0.1508,  0.0501, -0.1232],\n",
      "         [-0.0695, -0.0201,  0.0477,  ..., -0.2243,  0.2257, -0.2345],\n",
      "         [-0.2429, -0.1764, -0.1460,  ..., -0.2014,  0.2145, -0.2770],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        [[ 0.1841,  0.1160,  0.0517,  ..., -0.1373,  0.0814, -0.0799],\n",
      "         [ 0.1581, -0.0655,  0.1580,  ..., -0.0724,  0.0775,  0.0088],\n",
      "         [ 0.0209, -0.0966,  0.0297,  ..., -0.1713,  0.0004, -0.2020],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]],\n",
      "\n",
      "        [[ 0.0843, -0.0768,  0.0549,  ..., -0.1508,  0.0501, -0.1232],\n",
      "         [-0.0589, -0.1705, -0.1765,  ..., -0.2057, -0.1057, -0.2289],\n",
      "         [ 0.0183, -0.0780, -0.2784,  ..., -0.1750, -0.1227, -0.1462],\n",
      "         ...,\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344],\n",
      "         [ 0.1517,  0.0468,  0.0949,  ..., -0.0881,  0.0995, -0.0344]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[-0.0019, -0.1336, -0.1587,  ..., -0.1793, -0.0688, -0.1961],\n",
      "         [-0.0825, -0.1383, -0.3485,  ..., -0.2915, -0.2120, -0.2990],\n",
      "         [-0.0108, -0.0845, -0.3804,  ..., -0.2041, -0.2438, -0.4766],\n",
      "         ...,\n",
      "         [ 0.0833,  0.0623,  0.1001,  ..., -0.0939,  0.1817,  0.1212],\n",
      "         [ 0.1584, -0.0124, -0.0125,  ..., -0.0073,  0.1336,  0.0739],\n",
      "         [-0.2635, -0.1321, -0.3547,  ..., -0.3016, -0.1573, -0.2163]],\n",
      "\n",
      "        [[ 0.1841,  0.1148,  0.0501,  ..., -0.1386,  0.0807, -0.0812],\n",
      "         [-0.0213, -0.0122,  0.0259,  ..., -0.2064,  0.1814, -0.1867],\n",
      "         [-0.0142,  0.0649, -0.0155,  ..., -0.2770,  0.1469, -0.2201],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        [[-0.1824, -0.0512, -0.1807,  ..., -0.2488,  0.0449, -0.2777],\n",
      "         [ 0.0361, -0.1199,  0.0546,  ..., -0.1069,  0.0364, -0.0924],\n",
      "         [-0.0552, -0.1382, -0.0515,  ..., -0.1934, -0.0450, -0.2693],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0557, -0.0174,  0.0659,  ..., -0.0998,  0.1082, -0.0170],\n",
      "         [ 0.1520,  0.0504,  0.2775,  ..., -0.0291,  0.2043,  0.0216],\n",
      "         [-0.0168, -0.0958,  0.0782,  ..., -0.1326,  0.0662, -0.2174],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        [[-0.0019, -0.1336, -0.1587,  ..., -0.1793, -0.0688, -0.1961],\n",
      "         [-0.0040, -0.2173, -0.3466,  ..., -0.2345, -0.1837, -0.1285],\n",
      "         [ 0.0975, -0.1523, -0.2780,  ..., -0.1489, -0.0033, -0.1482],\n",
      "         ...,\n",
      "         [-0.0800, -0.0383, -0.1952,  ..., -0.2065,  0.1302, -0.0712],\n",
      "         [-0.4408, -0.2652, -0.5516,  ..., -0.4178, -0.2056, -0.3044],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        [[-0.1824, -0.0512, -0.1807,  ..., -0.2488,  0.0449, -0.2777],\n",
      "         [ 0.0361, -0.1199,  0.0546,  ..., -0.1069,  0.0364, -0.0924],\n",
      "         [-0.0552, -0.1382, -0.0515,  ..., -0.1934, -0.0450, -0.2693],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 24 of 1000; error is 2.1003589630126953"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.0933,  0.0554, -0.0139,  ..., -0.2128,  0.0131, -0.1764],\n",
      "         [ 0.1057, -0.1001,  0.1148,  ..., -0.1037,  0.0550, -0.0326],\n",
      "         [-0.0263, -0.1263, -0.0120,  ..., -0.1936, -0.0303, -0.2343],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        [[ 0.1841,  0.1148,  0.0501,  ..., -0.1386,  0.0807, -0.0812],\n",
      "         [ 0.1610, -0.0634,  0.1598,  ..., -0.0724,  0.0796,  0.0087],\n",
      "         [ 0.0172, -0.1027,  0.0228,  ..., -0.1739, -0.0063, -0.2104],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        [[ 0.1011, -0.0153, -0.0727,  ..., -0.1983, -0.0184, -0.0775],\n",
      "         [ 0.1809,  0.0786, -0.0371,  ..., -0.0811,  0.0529, -0.0406],\n",
      "         [ 0.2642,  0.2231, -0.0204,  ..., -0.0294,  0.0492, -0.0366],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1293,  0.0368,  0.0760,  ..., -0.0800, -0.0004, -0.0659],\n",
      "         [ 0.1580, -0.0459,  0.1692,  ..., -0.0341,  0.0577,  0.0031],\n",
      "         [ 0.0163, -0.0758,  0.0352,  ..., -0.1590, -0.0086, -0.2219],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        [[ 0.1307,  0.0637,  0.0727,  ..., -0.1171,  0.0948, -0.0895],\n",
      "         [ 0.1373, -0.0940,  0.1837,  ..., -0.0806,  0.0669,  0.0374],\n",
      "         [-0.0056, -0.1086,  0.0339,  ..., -0.1723, -0.0026, -0.2119],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]],\n",
      "\n",
      "        [[ 0.0177,  0.0820,  0.0848,  ..., -0.1691,  0.1346, -0.0475],\n",
      "         [-0.0751,  0.0488,  0.0525,  ..., -0.2133,  0.2518, -0.1509],\n",
      "         [-0.2435, -0.1576, -0.1619,  ..., -0.2018,  0.2152, -0.2611],\n",
      "         ...,\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354],\n",
      "         [ 0.1527,  0.0458,  0.0940,  ..., -0.0891,  0.0985, -0.0354]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[-0.0090, -0.1412, -0.1703,  ..., -0.1844, -0.0778, -0.2036],\n",
      "         [-0.1004, -0.1534, -0.3759,  ..., -0.3051, -0.2305, -0.3180],\n",
      "         [-0.0309, -0.1018, -0.4148,  ..., -0.2179, -0.2716, -0.5001],\n",
      "         ...,\n",
      "         [ 0.0768,  0.0577,  0.0905,  ..., -0.0972,  0.1761,  0.1195],\n",
      "         [ 0.1521, -0.0192, -0.0232,  ..., -0.0111,  0.1282,  0.0713],\n",
      "         [-0.2965, -0.1575, -0.3980,  ..., -0.3230, -0.1821, -0.2415]],\n",
      "\n",
      "        [[ 0.1840,  0.1137,  0.0484,  ..., -0.1400,  0.0799, -0.0826],\n",
      "         [-0.0265, -0.0177,  0.0180,  ..., -0.2118,  0.1757, -0.1942],\n",
      "         [-0.0218,  0.0584, -0.0255,  ..., -0.2833,  0.1389, -0.2291],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        [[-0.1927, -0.0613, -0.1956,  ..., -0.2575,  0.0359, -0.2892],\n",
      "         [ 0.0361, -0.1224,  0.0495,  ..., -0.1095,  0.0333, -0.0951],\n",
      "         [-0.0634, -0.1482, -0.0649,  ..., -0.1987, -0.0563, -0.2810],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0556, -0.0194,  0.0626,  ..., -0.1027,  0.1055, -0.0203],\n",
      "         [ 0.1536,  0.0501,  0.2765,  ..., -0.0297,  0.2029,  0.0201],\n",
      "         [-0.0234, -0.1031,  0.0682,  ..., -0.1363,  0.0575, -0.2268],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        [[-0.0090, -0.1412, -0.1703,  ..., -0.1844, -0.0778, -0.2036],\n",
      "         [-0.0156, -0.2288, -0.3681,  ..., -0.2441, -0.1976, -0.1417],\n",
      "         [ 0.0874, -0.1626, -0.2982,  ..., -0.1562, -0.0166, -0.1590],\n",
      "         ...,\n",
      "         [-0.1029, -0.0536, -0.2287,  ..., -0.2182,  0.1145, -0.0886],\n",
      "         [-0.4851, -0.3015, -0.6115,  ..., -0.4449, -0.2432, -0.3408],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        [[-0.1927, -0.0613, -0.1956,  ..., -0.2575,  0.0359, -0.2892],\n",
      "         [ 0.0361, -0.1224,  0.0495,  ..., -0.1095,  0.0333, -0.0951],\n",
      "         [-0.0634, -0.1482, -0.0649,  ..., -0.1987, -0.0563, -0.2810],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.1332,  0.0410,  0.1321,  ..., -0.0560,  0.0324, -0.0583],\n",
      "         [ 0.1410, -0.0622,  0.2082,  ..., -0.0403,  0.0711, -0.0378],\n",
      "         [-0.0060, -0.1063,  0.0490,  ..., -0.1651, -0.0193, -0.2495],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        [[ 0.0458,  0.0280,  0.0530,  ..., -0.1347,  0.0033, -0.1197],\n",
      "         [ 0.1558,  0.0616,  0.0475,  ..., -0.0284,  0.0735, -0.0409],\n",
      "         [ 0.0306,  0.0825, -0.0102,  ..., -0.1361,  0.1118, -0.2091],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        [[-0.0093,  0.0269,  0.0262,  ..., -0.0574,  0.0290,  0.0033],\n",
      "         [-0.0683, -0.1545, -0.2152,  ..., -0.1751, -0.1237, -0.1961],\n",
      "         [ 0.0309, -0.0660, -0.0829,  ..., -0.1316, -0.0133, -0.0835],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1025,  0.0195,  0.0168,  ..., -0.0357,  0.0593, -0.0676],\n",
      "         [ 0.1698,  0.1154,  0.0259,  ..., -0.0083,  0.0941, -0.0099],\n",
      "         [ 0.1361, -0.0754, -0.1844,  ..., -0.0617,  0.0019, -0.0222],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        [[ 0.0317, -0.0714,  0.0445,  ..., -0.2132,  0.1088, -0.0622],\n",
      "         [ 0.0839, -0.1309,  0.1488,  ..., -0.1166,  0.0715, -0.0215],\n",
      "         [ 0.1115, -0.0707,  0.0523,  ..., -0.0794,  0.0250, -0.1149],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]],\n",
      "\n",
      "        [[ 0.0556, -0.0194,  0.0626,  ..., -0.1027,  0.1055, -0.0203],\n",
      "         [-0.0761, -0.0518,  0.0402,  ..., -0.1675,  0.2003, -0.1795],\n",
      "         [-0.2648, -0.1965, -0.1698,  ..., -0.1913,  0.1966, -0.2807],\n",
      "         ...,\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364],\n",
      "         [ 0.1537,  0.0448,  0.0930,  ..., -0.0901,  0.0975, -0.0364]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[-0.0164, -0.1492, -0.1825,  ..., -0.1899, -0.0874, -0.2117],\n",
      "         [-0.1194, -0.1694, -0.4049,  ..., -0.3196, -0.2502, -0.3383],\n",
      "         [-0.0528, -0.1207, -0.4516,  ..., -0.2331, -0.3016, -0.5253],\n",
      "         ...,\n",
      "         [ 0.0679,  0.0512,  0.0781,  ..., -0.1016,  0.1694,  0.1167],\n",
      "         [ 0.1441, -0.0279, -0.0365,  ..., -0.0158,  0.1216,  0.0681],\n",
      "         [-0.3322, -0.1858, -0.4452,  ..., -0.3461, -0.2093, -0.2691]],\n",
      "\n",
      "        [[ 0.1838,  0.1127,  0.0467,  ..., -0.1415,  0.0791, -0.0841],\n",
      "         [-0.0320, -0.0235,  0.0097,  ..., -0.2175,  0.1698, -0.2020],\n",
      "         [-0.0299,  0.0515, -0.0361,  ..., -0.2902,  0.1305, -0.2385],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        [[-0.2035, -0.0719, -0.2113,  ..., -0.2668,  0.0262, -0.3013],\n",
      "         [ 0.0357, -0.1251,  0.0440,  ..., -0.1123,  0.0300, -0.0980],\n",
      "         [-0.0723, -0.1589, -0.0794,  ..., -0.2046, -0.0682, -0.2936],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0555, -0.0216,  0.0592,  ..., -0.1057,  0.1026, -0.0236],\n",
      "         [ 0.1551,  0.0498,  0.2755,  ..., -0.0303,  0.2014,  0.0185],\n",
      "         [-0.0303, -0.1106,  0.0575,  ..., -0.1405,  0.0482, -0.2368],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        [[-0.0164, -0.1492, -0.1825,  ..., -0.1899, -0.0874, -0.2117],\n",
      "         [-0.0277, -0.2410, -0.3907,  ..., -0.2543, -0.2124, -0.1556],\n",
      "         [ 0.0764, -0.1738, -0.3198,  ..., -0.1642, -0.0310, -0.1707],\n",
      "         ...,\n",
      "         [-0.1291, -0.0714, -0.2666,  ..., -0.2315,  0.0962, -0.1088],\n",
      "         [-0.5323, -0.3412, -0.6764,  ..., -0.4740, -0.2850, -0.3813],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        [[-0.2035, -0.0719, -0.2113,  ..., -0.2668,  0.0262, -0.3013],\n",
      "         [ 0.0357, -0.1251,  0.0440,  ..., -0.1123,  0.0300, -0.0980],\n",
      "         [-0.0723, -0.1589, -0.0794,  ..., -0.2046, -0.0682, -0.2936],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 0.0041, -0.1279, -0.0176,  ..., -0.1021,  0.0513, -0.0566],\n",
      "         [-0.1202, -0.1339, -0.0370,  ..., -0.1844,  0.1337, -0.2547],\n",
      "         [-0.1034, -0.2016, -0.0646,  ..., -0.0413,  0.1165, -0.1995],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        [[ 0.2401, -0.0270,  0.0304,  ..., -0.0660,  0.0941, -0.0703],\n",
      "         [ 0.1663, -0.1212,  0.1689,  ..., -0.0287,  0.0810,  0.0308],\n",
      "         [-0.0024, -0.1350,  0.0072,  ..., -0.1615, -0.0209, -0.2295],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        [[ 0.0797, -0.0529, -0.0171,  ..., -0.1082,  0.1240, -0.0771],\n",
      "         [ 0.1342, -0.1487,  0.1297,  ..., -0.0221,  0.0825, -0.0023],\n",
      "         [-0.0237, -0.1577, -0.0186,  ..., -0.1697, -0.0382, -0.2536],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1103,  0.0892,  0.1791,  ..., -0.0793,  0.1882, -0.0837],\n",
      "         [-0.0596, -0.0021,  0.0605,  ..., -0.1645,  0.2421, -0.2256],\n",
      "         [-0.2942, -0.2008, -0.1802,  ..., -0.2053,  0.2139, -0.2888],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        [[ 0.0860, -0.1616, -0.0787,  ..., -0.1342, -0.0568,  0.0399],\n",
      "         [ 0.1511,  0.0039, -0.0350,  ..., -0.0559,  0.0222, -0.0009],\n",
      "         [ 0.0654,  0.0033,  0.0017,  ..., -0.2126,  0.1068, -0.0244],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]],\n",
      "\n",
      "        [[ 0.1333,  0.0345,  0.0790,  ..., -0.0804, -0.0021, -0.0644],\n",
      "         [-0.0328, -0.0083,  0.0417,  ..., -0.1705,  0.1427, -0.2361],\n",
      "         [-0.1437, -0.0574, -0.0103,  ..., -0.2245,  0.1884, -0.3400],\n",
      "         ...,\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374],\n",
      "         [ 0.1547,  0.0438,  0.0920,  ..., -0.0910,  0.0965, -0.0374]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 26. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 2.0340569019317627"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[-0.0242, -0.1574, -0.1952,  ..., -0.1959, -0.0974, -0.2202],\n",
      "         [-0.1395, -0.1865, -0.4355,  ..., -0.3351, -0.2713, -0.3599],\n",
      "         [-0.0764, -0.1414, -0.4907,  ..., -0.2499, -0.3338, -0.5524],\n",
      "         ...,\n",
      "         [ 0.0557,  0.0420,  0.0617,  ..., -0.1075,  0.1613,  0.1125],\n",
      "         [ 0.1334, -0.0393, -0.0536,  ..., -0.0218,  0.1132,  0.0640],\n",
      "         [-0.3712, -0.2177, -0.4972,  ..., -0.3713, -0.2396, -0.2994]],\n",
      "\n",
      "        [[ 0.1837,  0.1116,  0.0450,  ..., -0.1430,  0.0782, -0.0856],\n",
      "         [-0.0379, -0.0294,  0.0011,  ..., -0.2236,  0.1636, -0.2101],\n",
      "         [-0.0387,  0.0440, -0.0472,  ..., -0.2976,  0.1216, -0.2484],\n",
      "         ...,\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384]],\n",
      "\n",
      "        [[-0.2149, -0.0829, -0.2277,  ..., -0.2767,  0.0158, -0.3142],\n",
      "         [ 0.0349, -0.1281,  0.0379,  ..., -0.1153,  0.0263, -0.1012],\n",
      "         [-0.0818, -0.1702, -0.0952,  ..., -0.2112, -0.0810, -0.3070],\n",
      "         ...,\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0552, -0.0238,  0.0557,  ..., -0.1088,  0.0997, -0.0270],\n",
      "         [ 0.1566,  0.0494,  0.2744,  ..., -0.0309,  0.1999,  0.0168],\n",
      "         [-0.0375, -0.1185,  0.0461,  ..., -0.1451,  0.0384, -0.2475],\n",
      "         ...,\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384]],\n",
      "\n",
      "        [[-0.0242, -0.1574, -0.1952,  ..., -0.1959, -0.0974, -0.2202],\n",
      "         [-0.0404, -0.2539, -0.4142,  ..., -0.2652, -0.2280, -0.1705],\n",
      "         [ 0.0643, -0.1861, -0.3430,  ..., -0.1732, -0.0465, -0.1833],\n",
      "         ...,\n",
      "         [-0.1595, -0.0922, -0.3101,  ..., -0.2467,  0.0747, -0.1325],\n",
      "         [-0.5830, -0.3847, -0.7472,  ..., -0.5052, -0.3316, -0.4264],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384]],\n",
      "\n",
      "        [[-0.2149, -0.0829, -0.2277,  ..., -0.2767,  0.0158, -0.3142],\n",
      "         [ 0.0349, -0.1281,  0.0379,  ..., -0.1153,  0.0263, -0.1012],\n",
      "         [-0.0818, -0.1702, -0.0952,  ..., -0.2112, -0.0810, -0.3070],\n",
      "         ...,\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384],\n",
      "         [ 0.1557,  0.0428,  0.0911,  ..., -0.0920,  0.0956, -0.0384]]],\n",
      "       device='cuda:0')\n",
      "batch1\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([5845, 50])\n",
      "torch.Size([108, 117, 50])\n",
      "tensor([[[ 7.4874e-02, -9.7523e-02,  2.6019e-02,  ..., -1.6516e-01,\n",
      "           2.6820e-02, -1.3704e-01],\n",
      "         [ 1.3550e-01, -1.3640e-01,  1.8589e-01,  ..., -8.1010e-02,\n",
      "           6.0742e-02, -2.1989e-02],\n",
      "         [-2.9486e-02, -1.5747e-01, -8.1130e-03,  ..., -1.9182e-01,\n",
      "          -4.4577e-02, -2.6624e-01],\n",
      "         ...,\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02]],\n",
      "\n",
      "        [[ 1.8367e-01,  1.1159e-01,  4.4958e-02,  ..., -1.4304e-01,\n",
      "           7.8220e-02, -8.5644e-02],\n",
      "         [ 1.6932e-01, -5.6234e-02,  1.6534e-01,  ..., -7.2360e-02,\n",
      "           8.6754e-02,  8.9079e-03],\n",
      "         [ 4.7893e-03, -1.2194e-01, -6.1458e-04,  ..., -1.8397e-01,\n",
      "          -2.7845e-02, -2.3798e-01],\n",
      "         ...,\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02]],\n",
      "\n",
      "        [[ 1.8367e-01,  1.1159e-01,  4.4958e-02,  ..., -1.4304e-01,\n",
      "           7.8220e-02, -8.5644e-02],\n",
      "         [-3.7877e-02, -2.9355e-02,  1.1292e-03,  ..., -2.2357e-01,\n",
      "           1.6359e-01, -2.1009e-01],\n",
      "         [-2.5950e-01, -2.1991e-01, -2.3654e-01,  ..., -2.3427e-01,\n",
      "           1.5140e-01, -3.3489e-01],\n",
      "         ...,\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0236e-01, -3.5497e-02,  1.1281e-01,  ..., -9.6557e-02,\n",
      "          -2.6607e-03, -1.1401e-01],\n",
      "         [ 1.2467e-01, -1.4442e-01,  1.8244e-01,  ..., -4.3981e-02,\n",
      "           7.6674e-04, -4.8411e-02],\n",
      "         [ 1.9891e-01, -5.4981e-02,  1.6327e-01,  ...,  1.5353e-02,\n",
      "           5.1563e-02,  2.7016e-04],\n",
      "         ...,\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02]],\n",
      "\n",
      "        [[ 4.2208e-02,  3.1579e-02, -8.1558e-02,  ..., -2.6205e-01,\n",
      "          -8.8785e-02, -2.1620e-01],\n",
      "         [ 9.8430e-02, -7.9846e-02,  1.2324e-01,  ..., -1.1855e-01,\n",
      "          -1.6494e-03, -5.3999e-02],\n",
      "         [-3.5261e-02, -1.2948e-01, -3.3500e-02,  ..., -2.0889e-01,\n",
      "          -8.8251e-02, -2.8106e-01],\n",
      "         ...,\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02]],\n",
      "\n",
      "        [[ 4.2208e-02,  3.1579e-02, -8.1558e-02,  ..., -2.6205e-01,\n",
      "          -8.8785e-02, -2.1620e-01],\n",
      "         [ 9.8430e-02, -7.9846e-02,  1.2324e-01,  ..., -1.1855e-01,\n",
      "          -1.6494e-03, -5.3999e-02],\n",
      "         [-5.3692e-02, -4.7046e-02, -5.8576e-02,  ..., -8.6782e-02,\n",
      "           2.8076e-02, -1.4300e-01],\n",
      "         ...,\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02],\n",
      "         [ 1.5574e-01,  4.2822e-02,  9.1067e-02,  ..., -9.2030e-02,\n",
      "           9.5567e-02, -3.8415e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "batch_preds\n",
      "torch.Size([108, 117])\n",
      "torch.Size([108, 12, 117])\n",
      "here2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "here21\n",
      "torch.Size([696, 50])\n",
      "torch.Size([12, 87, 50])\n",
      "tensor([[[-0.0323, -0.1661, -0.2085,  ..., -0.2023, -0.1078, -0.2292],\n",
      "         [-0.1606, -0.2046, -0.4676,  ..., -0.3516, -0.2936, -0.3828],\n",
      "         [-0.1018, -0.1638, -0.5323,  ..., -0.2682, -0.3683, -0.5813],\n",
      "         ...,\n",
      "         [ 0.0387,  0.0292,  0.0396,  ..., -0.1156,  0.1510,  0.1061],\n",
      "         [ 0.1188, -0.0546, -0.0763,  ..., -0.0296,  0.1021,  0.0586],\n",
      "         [-0.4148, -0.2542, -0.5557,  ..., -0.3992, -0.2739, -0.3331]],\n",
      "\n",
      "        [[ 0.1835,  0.1105,  0.0432,  ..., -0.1446,  0.0772, -0.0873],\n",
      "         [-0.0442, -0.0355, -0.0079,  ..., -0.2300,  0.1571, -0.2186],\n",
      "         [-0.0481,  0.0361, -0.0588,  ..., -0.3056,  0.1123, -0.2586],\n",
      "         ...,\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394]],\n",
      "\n",
      "        [[-0.2267, -0.0944, -0.2448,  ..., -0.2871,  0.0046, -0.3277],\n",
      "         [ 0.0338, -0.1315,  0.0313,  ..., -0.1185,  0.0223, -0.1048],\n",
      "         [-0.0920, -0.1822, -0.1123,  ..., -0.2186, -0.0947, -0.3213],\n",
      "         ...,\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0549, -0.0262,  0.0521,  ..., -0.1121,  0.0967, -0.0305],\n",
      "         [ 0.1579,  0.0489,  0.2732,  ..., -0.0317,  0.1984,  0.0150],\n",
      "         [-0.0450, -0.1267,  0.0341,  ..., -0.1502,  0.0282, -0.2587],\n",
      "         ...,\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394]],\n",
      "\n",
      "        [[-0.0323, -0.1661, -0.2085,  ..., -0.2023, -0.1078, -0.2292],\n",
      "         [-0.0536, -0.2674, -0.4386,  ..., -0.2768, -0.2445, -0.1861],\n",
      "         [ 0.0513, -0.1995, -0.3678,  ..., -0.1831, -0.0632, -0.1971],\n",
      "         ...,\n",
      "         [-0.1949, -0.1170, -0.3608,  ..., -0.2643,  0.0491, -0.1608],\n",
      "         [-0.6375, -0.4325, -0.8247,  ..., -0.5388, -0.3838, -0.4769],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394]],\n",
      "\n",
      "        [[-0.2267, -0.0944, -0.2448,  ..., -0.2871,  0.0046, -0.3277],\n",
      "         [ 0.0338, -0.1315,  0.0313,  ..., -0.1185,  0.0223, -0.1048],\n",
      "         [-0.0920, -0.1822, -0.1123,  ..., -0.2186, -0.0947, -0.3213],\n",
      "         ...,\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394],\n",
      "         [ 0.1567,  0.0418,  0.0901,  ..., -0.0930,  0.0946, -0.0394]]],\n",
      "       device='cuda:0')\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b9e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ceee1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class TorchCRFSequenceLabeler_1(TorchRNNClassifier):\n",
    "\n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "        print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "        print(\"here02\")\n",
    "        model = TorchSequenceLabeler_forCRF_1( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_)\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            #class2index = dict(zip(self.classes_, range(2,2+self.n_classes_)))\n",
    "            #class2index[STOP_TAG]=0    # add start and stop tags (note: stop needs to be 0 as that is default for padding in collate_fn)\n",
    "            #class2index[START_TAG]=1 \n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        # The base class does the heavy lifting:\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "        # Use `softmax`; the model doesn't do this because the loss\n",
    "        # function does it internally.\n",
    "        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        flat_preds = [x for seq in preds for x in seq]\n",
    "        flat_y = [x for seq in y for x in seq]\n",
    "        return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "    \n",
    "    def nClasses(self):\n",
    "        return len(self.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a7e7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequenceLabeler_forCRF_1(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim):\n",
    "        print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # this is the forward method of self.model\n",
    "        print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "        # logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        # During training, we need to swap the dimensions of logits\n",
    "        # to accommodate `nn.CrossEntropyLoss`:\n",
    "        if self.training:\n",
    "            return logits.transpose(1, 2) # transpose dimensions 1 and 2 w/ each other (3d array) # outputs (108,12,117) or (1,5,11)\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2f07cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d05d074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class TorchCRFSequenceLabeler_2(TorchRNNClassifier):\n",
    "\n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "        print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "        self.crf = CRF(self.n_classes_,batch_first=True)\n",
    "        print(\"here02\")\n",
    "        model = TorchSequenceLabeler_forCRF_2( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_,\n",
    "            crf=self.crf)\n",
    "        print(\"here002\")\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            #class2index = dict(zip(self.classes_, range(2,2+self.n_classes_)))\n",
    "            #class2index[STOP_TAG]=0    # add start and stop tags (note: stop needs to be 0 as that is default for padding in collate_fn)\n",
    "            #class2index[START_TAG]=1 \n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        # The base class does the heavy lifting:\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "        # Use `softmax`; the model doesn't do this because the loss\n",
    "        # function does it internally.\n",
    "        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X): # for CRF-RNN X are logits from RNN\n",
    "       # probs = self.predict_proba(X)\n",
    "       # return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]        \n",
    "        mask=self.create_mask(seq_lengths) # creates mask matrix (1s are obs used in CRF; 0s are discarded)  \n",
    "        print(\"pred\")\n",
    "        print(X.shape)\n",
    "        print(mask.shape)\n",
    "       # tag_seq = self.crf.decode(X,mask=mask) # note: X is (nExs,maxTokLen) and here input must be (nExs,maxTokLen,nDistinctTags)\n",
    "        # [[self.classes_[i] for i in seq] for seq in tag_seq]\n",
    "        return 0\n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "       # preds = self.predict(X)\n",
    "       # flat_preds = [x for seq in preds for x in seq]\n",
    "       # flat_y = [x for seq in y for x in seq]\n",
    "       # return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        mask=self.create_mask(seq_lengths) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "        return self.crf(logits, y, mask=mask)\n",
    "        \n",
    "    \n",
    "    def nClasses(self):\n",
    "        return len(self.classes_)\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f4495aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequenceLabeler_forCRF_2(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim, crf):\n",
    "        print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "        self.crf = crf\n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # this is the forward method of self.model\n",
    "        print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "       # print(logits.shape)\n",
    "        # logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        # During training, we need to swap the dimensions of logits\n",
    "        # to accommodate `nn.CrossEntropyLoss`:\n",
    "       # if self.training:\n",
    "       #     return logits.transpose(1, 2) # transpose dimensions 1 and 2 w/ each other (3d array) # outputs (108,12,117) or (1,5,11)\n",
    "       # else:\n",
    "       #     return logits\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        mask = (self.create_mask(seq_lengths)).to(device, non_blocking=True)\n",
    "        print(\"fwd\")\n",
    "        print(logits.shape)\n",
    "        print(mask.shape)\n",
    "        #tag_seqs = self.crf.decode(logits, mask=mask) # most likely tag sequences\n",
    "        return self.crf.forward(logits, mask=mask)\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f6014b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following converts words to indices and pads sequences\n",
    "seq_mod1 = TorchCRFSequenceLabeler_1(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ca555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = seq_mod1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2e64b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(seq_length):\n",
    "    maxLen=max(seq_length)\n",
    "    auxLen=len(seq_length)\n",
    "    auxOne = torch.ones(maxLen)\n",
    "    auxZero = torch.zeros(maxLen)\n",
    "    auxOne_l=[1]*maxLen\n",
    "    auxZero_l=[0]*maxLen\n",
    "    auxMatrix=[]\n",
    "    for i in range(auxLen):\n",
    "        auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "        auxMatrix.append(auxRow)\n",
    "    return torch.tensor(auxMatrix,dtype=torch.uint8)\n",
    "#check:\n",
    "#out=create_mask(seq_length)\n",
    "#print(out)\n",
    "#print(torch.sum(out,dim=1))\n",
    "\n",
    "#Note: before was doing this but can't use this as not guaranteed that label[STOP]=0\n",
    "    # a=torch.full_like(y,0,dtype=torch.uint8)\n",
    "    # b=torch.full_like(y,1,dtype=torch.uint8)\n",
    "    # mask=torch.where(y==0,a,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "adf0ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here2\n",
      "here21\n",
      "torch.Size([120, 117, 12])\n",
      "tensor(16929.6426, grad_fn=<NegBackward>)\n",
      "[[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]]\n"
     ]
    }
   ],
   "source": [
    "########## SAMPLE CRF ON REAL DATA\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dataset = seq_mod1.build_dataset(X_train, y_train) \n",
    "dataloader = seq_mod1._build_dataloader(dataset, shuffle=False) \n",
    "graph = seq_mod1.build_graph()\n",
    "num_tags = seq_mod1.nClasses()\n",
    "model_CRF = CRF(num_tags,batch_first=True)\n",
    "\n",
    "optimizer = optim.SGD(model_CRF.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for batch_num, batch in enumerate(dataloader, start=1):\n",
    "    x=batch[0]   \n",
    "    seq_length=batch[1]\n",
    "    y=batch[2] \n",
    "    logits = graph.forward(x,seq_length).transpose(1,2)\n",
    "    print(logits.shape)\n",
    "    \n",
    "    # CRF piece:\n",
    "    mask=create_mask(seq_length) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "    loss=-model_CRF(logits, y, mask=mask) # in this way minimizing loss means max log likelihood (i.e. we're converting to NLL)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(model_CRF.decode(logits,mask=mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ec4c4565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchSequenceLabeler_forCRF_1(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "da6622e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following converts words to indices and pads sequences\n",
    "seq_mod2 = TorchCRFSequenceLabeler_2(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7d499970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here00\n",
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here002\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "here21\n",
      "fwd\n",
      "torch.Size([108, 117, 12])\n",
      "torch.Size([108, 117])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\DeepLearning-JN\\cs224u\\base_data\\Vasco\\torch_model_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"here-model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0mbatch_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m                \u001b[1;31m# print(\"batch_preds\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                 \u001b[1;31m#print(len(batch_preds))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2540/1238705126.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, seq_lengths)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#tag_seqs = self.crf.decode(logits, mask=mask) # most likely tag sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tags'"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4abe9320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here002\n",
      "here2\n",
      "here21\n",
      "fwd\n",
      "torch.Size([120, 117, 12])\n",
      "torch.Size([120, 117])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2540/100546928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[1;31m#  print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;31m#  print(seq_length.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m    \u001b[1;31m# print(logits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_mod2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2540/1238705126.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, seq_lengths)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#tag_seqs = self.crf.decode(logits, mask=mask) # most likely tag sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tags'"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "################### I AM WORKING HERE #######################\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dataset = seq_mod2.build_dataset(X_train, y_train) \n",
    "dataloader = seq_mod2._build_dataloader(dataset, shuffle=False) \n",
    "graph = seq_mod2.build_graph()\n",
    "num_tags = seq_mod2.nClasses()\n",
    "#model_CRF = CRF(num_tags,batch_first=True)\n",
    "\n",
    "#optimizer = optim.SGD(model_CRF.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for batch_num, batch in enumerate(dataloader, start=1):\n",
    "    x=batch[0]   \n",
    "    seq_length=batch[1]\n",
    "    y=batch[2] \n",
    "  #  print(x.shape)\n",
    "  #  print(seq_length.shape)\n",
    "    logits = graph.forward(x,seq_length)\n",
    "   # print(logits)\n",
    "    print(seq_mod2.predict(x))\n",
    "    \n",
    "    # CRF piece:\n",
    "   # mask=create_mask(seq_length) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "   # loss=-model_CRF(logits, y, mask=mask) # in this way minimizing loss means max log likelihood (i.e. we're converting to NLL)\n",
    "   # print(loss)\n",
    "   # loss.backward()\n",
    "   # optimizer.step()\n",
    "   # print(model_CRF.decode(logits,mask=mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6930ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here002\n",
      "TorchSequenceLabeler_forCRF_2(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      "  (crf): CRF(num_tags=12)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "graph = seq_mod2.build_graph()\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbabc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "1331ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "a88d9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "\n",
    "class TorchCRFSequenceLabeler_3(TorchRNNClassifier):\n",
    "\n",
    "    def __init__(self,             \n",
    "            vocab,\n",
    "            hidden_dim=50,\n",
    "            embedding=None,\n",
    "            use_embedding=True,\n",
    "            embed_dim=50,\n",
    "            rnn_cell_class=nn.LSTM,\n",
    "            bidirectional=False,\n",
    "            freeze_embedding=False,\n",
    "            classifier_activation=nn.ReLU(),\n",
    "            **base_kwargs):   \n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = embedding\n",
    "        self.use_embedding = use_embedding\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn_cell_class = rnn_cell_class\n",
    "        self.bidirectional = bidirectional\n",
    "        self.freeze_embedding = freeze_embedding\n",
    "        self.classifier_activation = classifier_activation\n",
    "        super().__init__(vocab,**base_kwargs)\n",
    "        self.params += [\n",
    "            'hidden_dim',\n",
    "            'embed_dim',\n",
    "            'embedding',\n",
    "            'use_embedding',\n",
    "            'rnn_cell_class',\n",
    "            'bidirectional',\n",
    "            'freeze_embedding',\n",
    "            'classifier_activation']\n",
    "        self.loss = lambda x:x\n",
    "        if self.bidirectional:\n",
    "            self.classifier_dim = self.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.hidden_dim\n",
    "       # self.classifier_layer = nn.Linear(\n",
    "       #     self.classifier_dim, self.n_classes_)\n",
    "\n",
    "       \n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "       # print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "      #  print(\"here02\")\n",
    "        model = TorchSequenceLabeler_forCRF_3( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_)\n",
    "      #  print(\"here002\")\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        self.rnn = rnn\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            #class2index = dict(zip(self.classes_, range(2,2+self.n_classes_)))\n",
    "            #class2index[STOP_TAG]=0    # add start and stop tags (note: stop needs to be 0 as that is default for padding in collate_fn)\n",
    "            #class2index[START_TAG]=1 \n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        # The base class does the heavy lifting:\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "        # Use `softmax`; the model doesn't do this because the loss\n",
    "        # function does it internally.\n",
    "        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X): # for CRF-RNN X are logits from RNN\n",
    "       # probs = self.predict_proba(X)\n",
    "       # return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]        \n",
    "        mask=self.create_mask(seq_lengths) # creates mask matrix (1s are obs used in CRF; 0s are discarded)  \n",
    "        print(\"pred\")\n",
    "        print(X.shape)\n",
    "        print(mask.shape)\n",
    "       # tag_seq = self.crf.decode(X,mask=mask) # note: X is (nExs,maxTokLen) and here input must be (nExs,maxTokLen,nDistinctTags)\n",
    "        # [[self.classes_[i] for i in seq] for seq in tag_seq]\n",
    "        return 0\n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "       # preds = self.predict(X)\n",
    "       # flat_preds = [x for seq in preds for x in seq]\n",
    "       # flat_y = [x for seq in y for x in seq]\n",
    "       # return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        mask=self.create_mask(seq_lengths).to(device, non_blocking=True) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "        outputs, state = self.rnn(X, torch.tensor(seq_lengths)) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        fcl = nn.Linear(self.classifier_dim, self.n_classes_).to(device, non_blocking=True)\n",
    "        logits = fcl(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "        return self.crf(logits, y, mask=mask)\n",
    "        \n",
    "    \n",
    "    def nClasses(self):\n",
    "        return len(self.classes_)\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)  \n",
    "\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        \"\"\"\n",
    "        Generic optimization method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *args: list of objects\n",
    "            We assume that the final element of args give the labels\n",
    "            and all the preceding elements give the system inputs.\n",
    "            For regular supervised learning, this is like (X, y), but\n",
    "            we allow for models that might use multiple data structures\n",
    "            for their inputs.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        model: nn.Module or subclass thereof\n",
    "            Set by `build_graph`. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`.\n",
    "\n",
    "        optimizer: torch.optimizer.Optimizer\n",
    "            Set by `build_optimizer`. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`.\n",
    "\n",
    "        errors: list of float\n",
    "            List of errors. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`. Thus, where\n",
    "            `max_iter=5`, if we call `fit` twice with `warm_start=True`,\n",
    "            then `errors` will end up with 10 floats in it.\n",
    "\n",
    "        validation_scores: list\n",
    "            List of scores. This is filled only if `early_stopping=True`.\n",
    "            If `warm_start=True`, then this is initialized only by the\n",
    "            first call to `fit`. Thus, where `max_iter=5`, if we call\n",
    "            `fit` twice with `warm_start=True`, then `validation_scores`\n",
    "            will end up with 10 floats in it.\n",
    "\n",
    "        no_improvement_count: int\n",
    "            Used to control early stopping and convergence. These values\n",
    "            are controlled by `_update_no_improvement_count_early_stopping`\n",
    "            or `_update_no_improvement_count_errors`.  If `warm_start=True`,\n",
    "            then this is initialized only by the first call to `fit`. Thus,\n",
    "            in that situation, the values could accumulate across calls to\n",
    "            `fit`.\n",
    "\n",
    "        best_error: float\n",
    "           Used to control convergence. Smaller is assumed to be better.\n",
    "           If `warm_start=True`, then this is initialized only by the first\n",
    "           call to `fit`. It will be reset by\n",
    "           `_update_no_improvement_count_errors` depending on how the\n",
    "           optimization is proceeding.\n",
    "\n",
    "        best_score: float\n",
    "           Used to control early stopping. If `warm_start=True`, then this\n",
    "           is initialized only by the first call to `fit`. It will be reset\n",
    "           by `_update_no_improvement_count_early_stopping` depending on how\n",
    "           the optimization is proceeding. Important: we currently assume\n",
    "           that larger scores are better. As a result, we will not get the\n",
    "           correct results for, e.g., a scoring function based in\n",
    "           `mean_squared_error`. See `self.score` for additional details.\n",
    "\n",
    "        best_parameters: dict\n",
    "            This is a PyTorch state dict. It is used if and only if\n",
    "            `early_stopping=True`. In that case, it is updated whenever\n",
    "            `best_score` is improved numerically. If the early stopping\n",
    "            criteria are met, then `self.model` is reset to contain these\n",
    "            parameters before `fit` exits.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "      #  print(\"here00\")\n",
    "        if self.early_stopping:\n",
    "            args, dev = self._build_validation_split(\n",
    "                *args, validation_fraction=self.validation_fraction)\n",
    "\n",
    "        # Dataset:\n",
    "        dataset = self.build_dataset(*args)\n",
    "        dataloader = self._build_dataloader(dataset, shuffle=True)\n",
    "\n",
    "        # Graph:\n",
    "        if not self.warm_start or not hasattr(self, \"model\"):\n",
    "            self.model = self.build_graph()\n",
    "            # This device move has to happen before the optimizer is built:\n",
    "            # https://pytorch.org/docs/master/optim.html#constructing-it\n",
    "            self.model.to(self.device)\n",
    "            self.optimizer = self.build_optimizer()\n",
    "            self.errors = []\n",
    "            self.validation_scores = []\n",
    "            self.no_improvement_count = 0\n",
    "            self.best_error = np.inf\n",
    "            self.best_score = -np.inf\n",
    "            self.best_parameters = None\n",
    "\n",
    "        # Make sure the model is where we want it:\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for iteration in range(1, self.max_iter+1):\n",
    "\n",
    "            epoch_error = 0.0\n",
    "\n",
    "            for batch_num, batch in enumerate(dataloader, start=1):\n",
    "               # print(\"batch\"+str(batch_num)) \n",
    "\n",
    "               # print(batch)\n",
    "                batch = [x.to(self.device, non_blocking=True) for x in batch]\n",
    "\n",
    "                X_batch = batch[: -1] # list w/ 2 els: 1st el is tensor (108xmaxLen) w/ tokens for each example in batch; 2nd el is (108x1) with lengths of each example\n",
    "                y_batch = batch[-1] # list with each element of this batch (108 el in list) with tensor (maxLen x 1) labels converted to ints and w/ len = maxLen of all example sequences # print(y_batch[0].shape)\n",
    "               # print(X_batch[1].shape)\n",
    "               # print(y_batch[0])\n",
    "               \n",
    "                batch_preds = self.model(*X_batch) # produces logits outputs of lstm\n",
    "               # print(\"batch_preds\")\n",
    "                self.crf = CRF(self.n_classes_,batch_first=True).to(self.device, non_blocking=True)\n",
    "\n",
    "               # print(\"here-model2\")\n",
    "                mask = (self.create_mask(X_batch[1])).to(self.device, non_blocking=True)\n",
    "                #err = self.loss(batch_preds, y_batch) # batch_preds = (108,12,117); y_batch = (108,117)\n",
    "                err = -self.crf(batch_preds,y_batch,mask=mask)\n",
    "\n",
    "                if self.gradient_accumulation_steps > 1 and \\\n",
    "                  self.loss.reduction == \"mean\":\n",
    "                    err /= self.gradient_accumulation_steps\n",
    "\n",
    "                err.backward()\n",
    "\n",
    "                epoch_error += err.item()\n",
    "\n",
    "                if batch_num % self.gradient_accumulation_steps == 0 or \\\n",
    "                  batch_num == len(dataloader):\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.max_grad_norm)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            # Stopping criteria:\n",
    "\n",
    "            if self.early_stopping:\n",
    "                #self._update_no_improvement_count_early_stopping(*dev)\n",
    "                self._update_no_improvement_count_early_stopping(X_batch[0],y_batch)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Validation score did \"\n",
    "                        \"not improve by tol={} for more than {} epochs. \"\n",
    "                        \"Final error is {}\".format(iteration, self.tol,\n",
    "                            self.n_iter_no_change, epoch_error),\n",
    "                        verbose=self.display_progress)\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                self._update_no_improvement_count_errors(epoch_error)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Training loss did \"\n",
    "                        \"not improve more than tol={}. Final error \"\n",
    "                        \"is {}.\".format(iteration, self.tol, epoch_error),\n",
    "                        verbose=self.display_progress)\n",
    "                    break\n",
    "\n",
    "            utils.progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.max_iter, epoch_error),\n",
    "                verbose=self.display_progress)\n",
    "\n",
    "        if self.early_stopping:\n",
    "            self.model.load_state_dict(self.best_parameters)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "5adb54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequenceLabeler_forCRF_3(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim):\n",
    "       # print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # this is the forward method of self.model\n",
    "       # print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "       # print(logits.shape)\n",
    "        # logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        return logits\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "01032427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following converts words to indices and pads sequences\n",
    "seq_mod3 = TorchCRFSequenceLabeler_3(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "5889ef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here002\n",
      "TorchSequenceLabeler_forCRF_3(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "dataset = seq_mod3.build_dataset(X_train, y_train) \n",
    "dataloader = seq_mod3._build_dataloader(dataset, shuffle=False) \n",
    "graph = seq_mod3.build_graph()\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "3f8004a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 21. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 11815.50390625"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddf736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e345cb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5731, grad_fn=<SumBackward0>)\n",
      "tensor([[[-0.4519, -0.1661]]])\n",
      "tensor([[1]])\n",
      "Parameter containing:\n",
      "tensor([[-0.0941,  0.0600],\n",
      "        [-0.0206,  0.0509]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0515, -0.0441], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0194,  0.0469], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "########## SAMPLE CRF ON TOY DATA\n",
    "# this is where I matched my first model score (i.e. log likelihood of crf)\n",
    "torch.manual_seed(1)\n",
    "seq_length = 1  # maximum sequence length in a batch\n",
    "batch_size = 1  # number of samples in the batch\n",
    "num_tags=2\n",
    "model = CRF(num_tags,batch_first=True)\n",
    "#emissions = torch.randn(batch_size, seq_length, num_tags)\n",
    "#tags = torch.tensor([[0, 2, 3], [1, 4, 1]], dtype=torch.long)  # (batch_size, seq_length)\n",
    "emissions = torch.randn(1, seq_length, num_tags)\n",
    "\n",
    "tags = torch.tensor([[1]], dtype=torch.long)  \n",
    "print(model(emissions, tags))\n",
    "print(emissions)\n",
    "print(tags)\n",
    "print(model.transitions)\n",
    "print(model.start_transitions)\n",
    "print(model.end_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "4a9eb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "tensor(-8.3568, grad_fn=<SumBackward0>)\n",
      "[[3, 4, 3], [4, 1]]\n",
      "tensor(-2.7487, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "########## SAMPLE CRF ON TOY DATA\n",
    "# add mask vector so as to not to consider padding part of sequences\n",
    "# e.g. want to mask last zero of 2nd obs\n",
    "torch.manual_seed(1)\n",
    "seq_length = 3  # maximum sequence length in a batch\n",
    "batch_size = 2  # number of samples in the batch\n",
    "num_tags=5\n",
    "model = CRF(num_tags,batch_first=True)\n",
    "emissions = torch.randn(batch_size, seq_length, num_tags)\n",
    "tags = torch.tensor([[1, 2, 3], [1, 4, 0]], dtype=torch.long)  # (batch_size, seq_length)\n",
    "mask = torch.tensor([[1, 1, 1], [1, 1, 0]], dtype=torch.uint8) # i.e. mask 3rd token of 2nd example in batch\n",
    "print(emissions.shape)\n",
    "print(tags.shape)\n",
    "print(mask.shape)\n",
    "# 1.\n",
    "print(model(emissions, tags, mask=mask)) # model log likelihood\n",
    "# 2.\n",
    "print(model.decode(emissions,mask=mask)) # most likely tag sequences\n",
    "# 3. inference:\n",
    "tags_test = torch.tensor([[1, 2, 3]], dtype=torch.long)  # (batch_size, seq_length)\n",
    "print(model.forward(emissions[0].unsqueeze(dim=0),tags_test)) # returns log likelihood of test tag sequence (larger no. means more likely)\n",
    "# note: need to use torch array w/ same no. of examples as tags_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d5c17e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "# note: this isn't exactly correct as training examples are shuffled (esp. max len of the smaller 12 ex batch is != 92)\n",
    "auxMax=0\n",
    "x_max_idx=108\n",
    "for i in range(0,min(len(X_train),x_max_idx)):\n",
    "    if len(X_train[i])>auxMax:\n",
    "        auxMax=len(X_train[i])\n",
    "print(auxMax)\n",
    "auxMax2=0\n",
    "x_min_idx=109\n",
    "for i in range(max(0,x_min_idx),len(X_train)):\n",
    "    if len(X_train[i])>auxMax2:\n",
    "        auxMax2=len(X_train[i])\n",
    "print(auxMax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cfe2a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O']\n",
      "['KAEUFER', 'KAEUFER', 'O', 'O', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "y_pred = seq_mod.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2960fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=seq_mod.classes_\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d615f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold all our data - NOTE: this means we don't care about per sentence results. \n",
    "# i.e. each classification is worth same regardless of sentence in which it occurs\n",
    "y_test_unfold = [y for element in y_test for y in element]\n",
    "y_pred_unfold = [y for element in y_pred for y in element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "99efc2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(y_test_unfold[:10])\n",
    "print(y_pred_unfold[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5de61d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_test and y_pred into binary formats\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f4747c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                  O      0.808     0.888     0.846       643\n",
      "            KAEUFER      0.070     0.278     0.112        18\n",
      "DATUM_VERBUECHERUNG      0.000     0.000     0.000        25\n",
      "      DATUM_VERTRAG      1.000     0.037     0.071        27\n",
      "         VERKAEUFER      0.333     0.042     0.074        24\n",
      "   TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "        GESAMTPREIS      0.000     0.000     0.000        11\n",
      "            FLAECHE      0.000     0.000     0.000        15\n",
      "           IMMO_TYP      0.000     0.000     0.000        19\n",
      "            QMPREIS      0.000     0.000     0.000        10\n",
      "                ORT      0.000     0.000     0.000        26\n",
      "            STRASSE      0.118     0.125     0.121        16\n",
      "\n",
      "           accuracy                          0.691       839\n",
      "          macro avg      0.194     0.114     0.102       839\n",
      "       weighted avg      0.664     0.691     0.657       839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_test_unfold, y_pred_unfold, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9457a",
   "metadata": {},
   "source": [
    "Now try with leading \"B-\" and \"I-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6f7276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## ONLY RUN IF WE WANT TO ADD LEADING \"B-\" / \"I-\" TO CLASS LABEL\n",
    "# now use above code and loop through all items of annot list:\n",
    "# addLeading=1 for \"Yes\" (i.e. add leading \"B-\",\"I-\" to annot); 0 for \"No\" (i.e. add labels to annot simply as they are)\n",
    "addLeading = 1\n",
    "\n",
    "if addLeading == 1:\n",
    "    for j in range(0,len(annot)):\n",
    "        a = annot[j]\n",
    "        # select list of dict of tokens w/ annnotations and add column w/ no. of words to each dict:\n",
    "        b = a['spans']\n",
    "        # add noWords to b dict. note: b is list of dicts w/ annotations; tokens not on this list don't have annotations\n",
    "        if b!=[]: #i.e. only try to add annotations to tokens if there are annotations to begin with\n",
    "            #print(b)\n",
    "            for i in range(0,len(annot[j]['tokens'])):\n",
    "                    # now break-up label into 1st occurrence (leading \"B-\") and subsequent occurrences (leading \"I-\") (only for non \"O\"'s)\n",
    "                    if annot[j]['tokens'][i]['label'] != \"O\":\n",
    "                        if i==0:\n",
    "                            annot[j]['tokens'][i]['label'] = \"B-\" + annot[j]['tokens'][i]['label']\n",
    "                        else: \n",
    "                            if annot[j]['tokens'][i]['label'] == annot[j]['tokens'][i-1]['label'][2:]: # need to remove the leading \"B-\" that we had already been added to c[i-1]\n",
    "                                annot[j]['tokens'][i]['label'] = \"I-\" + annot[j]['tokens'][i]['label']\n",
    "                            else:\n",
    "                                annot[j]['tokens'][i]['label'] = \"B-\" + annot[j]['tokens'][i]['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6bd88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get data into format that TorchRNN expects:\n",
    "X=[] \n",
    "y=[]\n",
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]['tokens']\n",
    "    auxX = []\n",
    "    auxy = []\n",
    "    if annot[j]['spans']!=[]: # are there annot for this example?\n",
    "        for i in range(0,len(a)):\n",
    "            #token_element = (a[i]['text'],a[i]['label'])\n",
    "            auxX.append(a[i]['text'])\n",
    "            auxy.append(a[i]['label'])\n",
    "        X.append(auxX)\n",
    "        y.append(auxy)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X[:120], X[120:], y[:120], y[120:]\n",
    "vocab = sorted({w for seq in X_train for w in seq}) + [\"$UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "353255ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DORNBIRN', 'In', 'der', 'Schulgasse', 'in', 'Dornbirn', 'hat', 'eine', '71,93', 'Quadratmeter', 'groe', 'Wohnung', 'fr', 'einen', 'Quadratmeterpreis', 'von', '5533,71', 'Euro', 'den', 'Besitzer', 'gewechselt', '.', 'Dieser', 'beinhaltet', 'auch', 'einen', 'Pkw-Abstellplatz', '.', 'Kufer', 'der', 'Wohnung', 'mit', '9,86', 'Quadratmetern', 'Terrasse', 'ist', 'die', 'ValLiLean', 'Beteiligungs-', 'und', 'Immobilienverwaltungs', 'GmbH', 'Beim', 'Verkufer', 'handelt', 'es', 'sich', 'um', 'die', 'Karrenblick', 'Projekt', 'GmbH', ' ', 'Der', 'Kaufpreis', 'liegt', 'bei', '398.040', 'Euro', '.', 'Unterzeichnet', 'wurde', 'der', 'Kaufvertrag', 'am', '18.', 'September', '.', 'Die', 'Verbcherung', 'datiert', 'mit', 'Oktober', '2020', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "071c6a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 35. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 2.249159574508667"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.78 s\n",
      "['B-ORT', 'O', 'O', 'B-STRASSE', 'I-STRASSE', 'O', 'B-ORT', 'O', 'O', 'B-FLAECHE', 'O', 'O', 'B-IMMO_TYP', 'O', 'O', 'O', 'O', 'B-QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'B-GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATUM_VERTRAG', 'I-DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'B-DATUM_VERBUECHERUNG', 'I-DATUM_VERBUECHERUNG', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod.fit(X_train, y_train)\n",
    "y_pred = seq_mod.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92a8a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=seq_mod.classes_\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b338e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold all our data - NOTE: this means we don't care about per sentence results. \n",
    "# i.e. each classification is worth same regardless of sentence in which it occurs\n",
    "y_test_unfold = [y for element in y_test for y in element]\n",
    "y_pred_unfold = [y for element in y_pred for y in element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "78515133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                    O      0.773     0.988     0.867       643\n",
      "B-DATUM_VERBUECHERUNG      0.000     0.000     0.000        13\n",
      "I-DATUM_VERBUECHERUNG      0.000     0.000     0.000        12\n",
      "      B-DATUM_VERTRAG      0.000     0.000     0.000        13\n",
      "      I-DATUM_VERTRAG      0.000     0.000     0.000        14\n",
      "            B-FLAECHE      0.000     0.000     0.000        15\n",
      "            I-FLAECHE      0.000     0.000     0.000         0\n",
      "        B-GESAMTPREIS      0.000     0.000     0.000        11\n",
      "        I-GESAMTPREIS      0.000     0.000     0.000         0\n",
      "           B-IMMO_TYP      0.000     0.000     0.000        19\n",
      "           I-IMMO_TYP      0.000     0.000     0.000         0\n",
      "            B-KAEUFER      0.000     0.000     0.000        10\n",
      "            I-KAEUFER      0.000     0.000     0.000         8\n",
      "                B-ORT      0.300     0.115     0.167        26\n",
      "            B-QMPREIS      0.000     0.000     0.000        10\n",
      "            I-QMPREIS      0.000     0.000     0.000         0\n",
      "            B-STRASSE      0.000     0.000     0.000        12\n",
      "            I-STRASSE      0.000     0.000     0.000         4\n",
      "   B-TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "         B-VERKAEUFER      0.000     0.000     0.000        13\n",
      "         I-VERKAEUFER      0.000     0.000     0.000        11\n",
      "\n",
      "            micro avg      0.760     0.760     0.760       839\n",
      "            macro avg      0.051     0.053     0.049       839\n",
      "         weighted avg      0.601     0.760     0.670       839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasco\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_test_unfold, y_pred_unfold, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680fd70",
   "metadata": {},
   "source": [
    "Remove \"B-\" and \"I-\" (in case they are present in labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a2de8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]\n",
    "    b = a['spans']\n",
    "    if b!=[]: #i.e. only try to add annotations to tokens if there are annotations to begin with\n",
    "        for i in range(0,len(annot[j]['tokens'])):\n",
    "                if annot[j]['tokens'][i]['label'] != \"O\":\n",
    "                    if annot[j]['tokens'][i]['label'][:2]==\"B-\" or annot[j]['tokens'][i]['label'][:2]==\"I-\":\n",
    "                        annot[j]['tokens'][i]['label']=annot[j]['tokens'][i]['label'][2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f22b42",
   "metadata": {},
   "source": [
    "Try bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "147649d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mod = TorchRNNSequenceLabeler(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001,\n",
    "    bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "63ca2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8413a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 18. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 2.1157665252685547"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8d5884c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                  O      0.760     0.893     0.821       643\n",
      "            KAEUFER      0.000     0.000     0.000        18\n",
      "DATUM_VERBUECHERUNG      0.000     0.000     0.000        25\n",
      "      DATUM_VERTRAG      0.000     0.000     0.000        27\n",
      "         VERKAEUFER      0.000     0.000     0.000        24\n",
      "   TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "        GESAMTPREIS      0.000     0.000     0.000        11\n",
      "            FLAECHE      0.000     0.000     0.000        15\n",
      "           IMMO_TYP      0.000     0.000     0.000        19\n",
      "            QMPREIS      0.000     0.000     0.000        10\n",
      "                ORT      0.000     0.000     0.000        26\n",
      "            STRASSE      0.000     0.000     0.000        16\n",
      "\n",
      "           accuracy                          0.684       839\n",
      "          macro avg      0.063     0.074     0.068       839\n",
      "       weighted avg      0.583     0.684     0.629       839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = seq_mod.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(y_pred[0])\n",
    "\n",
    "labels=seq_mod.classes_\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "# unfold all our data - NOTE: this means we don't care about per sentence results. \n",
    "# i.e. each classification is worth same regardless of sentence in which it occurs\n",
    "y_test_unfold = [y for element in y_test for y in element]\n",
    "y_pred_unfold = [y for element in y_pred for y in element]\n",
    "\n",
    "print(classification_report(\n",
    "    y_test_unfold, y_pred_unfold, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d592b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
